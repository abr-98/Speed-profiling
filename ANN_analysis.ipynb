{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Train on 4263 samples, validate on 1066 samples\n",
      "Epoch 1/30\n",
      "4263/4263 [==============================] - 2s 358us/step - loss: 0.9471 - acc: 0.5951 - val_loss: 1.1534 - val_acc: 0.5478\n",
      "Epoch 2/30\n",
      "4263/4263 [==============================] - 0s 96us/step - loss: 0.8657 - acc: 0.6125 - val_loss: 1.1267 - val_acc: 0.5675\n",
      "Epoch 3/30\n",
      "4263/4263 [==============================] - 0s 97us/step - loss: 0.8323 - acc: 0.6261 - val_loss: 1.0643 - val_acc: 0.5385\n",
      "Epoch 4/30\n",
      "4263/4263 [==============================] - 0s 83us/step - loss: 0.8179 - acc: 0.6343 - val_loss: 1.1001 - val_acc: 0.5563\n",
      "Epoch 5/30\n",
      "4263/4263 [==============================] - 0s 91us/step - loss: 0.8071 - acc: 0.6519 - val_loss: 1.2199 - val_acc: 0.5178\n",
      "Epoch 6/30\n",
      "4263/4263 [==============================] - 0s 87us/step - loss: 0.7938 - acc: 0.6617 - val_loss: 1.0281 - val_acc: 0.5403\n",
      "Epoch 7/30\n",
      "4263/4263 [==============================] - 0s 97us/step - loss: 0.7838 - acc: 0.6683 - val_loss: 1.1387 - val_acc: 0.5450\n",
      "Epoch 8/30\n",
      "4263/4263 [==============================] - 0s 101us/step - loss: 0.7731 - acc: 0.6739 - val_loss: 1.1588 - val_acc: 0.5263\n",
      "Epoch 9/30\n",
      "4263/4263 [==============================] - 0s 102us/step - loss: 0.7677 - acc: 0.6725 - val_loss: 1.0995 - val_acc: 0.5216\n",
      "5329/5329 [==============================] - 0s 42us/step\n",
      "\n",
      "acc: 65.27%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df['Class'].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "y_d_2=pd.get_dummies(y_d)\n",
    "n_cols=X_d.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_d, y_d_2, epochs=30, validation_split=0.2, callbacks=[early_stopping_monitor])\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_d, y_d_2)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_d,y_d,test_size=0.3,random_state=42)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Train on 3730 samples, validate on 1599 samples\n",
      "Epoch 1/50\n",
      "3730/3730 [==============================] - 2s 442us/step - loss: 0.9372 - acc: 0.6029 - val_loss: 1.1954 - val_acc: 0.4684\n",
      "Epoch 2/50\n",
      "3730/3730 [==============================] - 1s 150us/step - loss: 0.8324 - acc: 0.6534 - val_loss: 1.1753 - val_acc: 0.4903\n",
      "Epoch 3/50\n",
      "3730/3730 [==============================] - 0s 125us/step - loss: 0.8007 - acc: 0.6676 - val_loss: 1.2586 - val_acc: 0.4440\n",
      "Epoch 4/50\n",
      "3730/3730 [==============================] - 0s 114us/step - loss: 0.7968 - acc: 0.6635 - val_loss: 1.1266 - val_acc: 0.4909\n",
      "Epoch 5/50\n",
      "3730/3730 [==============================] - 1s 149us/step - loss: 0.7832 - acc: 0.6622 - val_loss: 1.2021 - val_acc: 0.4828\n",
      "Epoch 6/50\n",
      "3730/3730 [==============================] - 0s 118us/step - loss: 0.7652 - acc: 0.6729 - val_loss: 1.1093 - val_acc: 0.4972\n",
      "Epoch 7/50\n",
      "3730/3730 [==============================] - 0s 119us/step - loss: 0.7574 - acc: 0.6777 - val_loss: 1.0958 - val_acc: 0.5291\n",
      "Epoch 8/50\n",
      "3730/3730 [==============================] - 0s 126us/step - loss: 0.7590 - acc: 0.6826 - val_loss: 1.2138 - val_acc: 0.4615\n",
      "Epoch 9/50\n",
      "3730/3730 [==============================] - 0s 123us/step - loss: 0.7471 - acc: 0.6879 - val_loss: 1.1857 - val_acc: 0.4922\n",
      "Epoch 10/50\n",
      "3730/3730 [==============================] - 0s 114us/step - loss: 0.7538 - acc: 0.6879 - val_loss: 1.1335 - val_acc: 0.5191\n",
      "5329/5329 [==============================] - 0s 45us/step\n",
      "\n",
      "acc: 64.42%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df['Class'].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "y_d_2=pd.get_dummies(y_d)\n",
    "n_cols=X_d.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_d, y_d_2, epochs=50, validation_split=0.3, callbacks=[early_stopping_monitor])\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_d, y_d_2)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_d,y_d,test_size=0.3,random_state=42)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/160\n",
      "3730/3730 [==============================] - 2s 504us/step - loss: 1.0611 - acc: 0.5515\n",
      "Epoch 2/160\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.9330 - acc: 0.5941\n",
      "Epoch 3/160\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.9045 - acc: 0.5920\n",
      "Epoch 4/160\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.8889 - acc: 0.6005\n",
      "Epoch 5/160\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.8757 - acc: 0.6088\n",
      "Epoch 6/160\n",
      "3730/3730 [==============================] - 0s 74us/step - loss: 0.8676 - acc: 0.6279\n",
      "Epoch 7/160\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.8468 - acc: 0.6397\n",
      "Epoch 8/160\n",
      "3730/3730 [==============================] - 0s 72us/step - loss: 0.8363 - acc: 0.6391\n",
      "Epoch 9/160\n",
      "3730/3730 [==============================] - 0s 74us/step - loss: 0.8379 - acc: 0.6354\n",
      "Epoch 10/160\n",
      "3730/3730 [==============================] - 0s 72us/step - loss: 0.8313 - acc: 0.6434\n",
      "Epoch 11/160\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.8246 - acc: 0.6359\n",
      "Epoch 12/160\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.8287 - acc: 0.6413\n",
      "Epoch 13/160\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.8240 - acc: 0.6424\n",
      "Epoch 14/160\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.8172 - acc: 0.6429\n",
      "Epoch 15/160\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.8142 - acc: 0.6405\n",
      "Epoch 16/160\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.8132 - acc: 0.6469\n",
      "Epoch 17/160\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.8140 - acc: 0.6426\n",
      "Epoch 18/160\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.8109 - acc: 0.6515\n",
      "Epoch 19/160\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7999 - acc: 0.6496\n",
      "Epoch 20/160\n",
      "3730/3730 [==============================] - 0s 73us/step - loss: 0.7987 - acc: 0.6507\n",
      "Epoch 21/160\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7970 - acc: 0.6528\n",
      "Epoch 22/160\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.7899 - acc: 0.6547\n",
      "Epoch 23/160\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.7964 - acc: 0.6568\n",
      "Epoch 24/160\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.7904 - acc: 0.6568\n",
      "Epoch 25/160\n",
      "3730/3730 [==============================] - 0s 74us/step - loss: 0.7862 - acc: 0.6579\n",
      "Epoch 26/160\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7937 - acc: 0.6547\n",
      "Epoch 27/160\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7826 - acc: 0.6625\n",
      "Epoch 28/160\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.7845 - acc: 0.6705\n",
      "Epoch 29/160\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7769 - acc: 0.6614\n",
      "Epoch 30/160\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.7823 - acc: 0.6643\n",
      "Epoch 31/160\n",
      "3730/3730 [==============================] - 0s 89us/step - loss: 0.7814 - acc: 0.6622\n",
      "Epoch 32/160\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7823 - acc: 0.6601\n",
      "Epoch 33/160\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.7875 - acc: 0.6531\n",
      "Epoch 34/160\n",
      "3730/3730 [==============================] - 0s 74us/step - loss: 0.7707 - acc: 0.6646\n",
      "Epoch 35/160\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7736 - acc: 0.6630\n",
      "Epoch 36/160\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.7754 - acc: 0.6649\n",
      "Epoch 37/160\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.7693 - acc: 0.6643\n",
      "Epoch 38/160\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.7742 - acc: 0.6660\n",
      "Epoch 39/160\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.7746 - acc: 0.6638\n",
      "Epoch 40/160\n",
      "3730/3730 [==============================] - 0s 73us/step - loss: 0.7695 - acc: 0.6694\n",
      "Epoch 41/160\n",
      "3730/3730 [==============================] - 0s 74us/step - loss: 0.7683 - acc: 0.6678\n",
      "Epoch 42/160\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.7646 - acc: 0.6697\n",
      "Epoch 43/160\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.7685 - acc: 0.6673\n",
      "Epoch 44/160\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.7678 - acc: 0.6662\n",
      "Epoch 45/160\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7648 - acc: 0.6638\n",
      "Epoch 46/160\n",
      "3730/3730 [==============================] - 0s 74us/step - loss: 0.7574 - acc: 0.6692\n",
      "Epoch 47/160\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.7585 - acc: 0.6724\n",
      "Epoch 48/160\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7623 - acc: 0.6651\n",
      "Epoch 49/160\n",
      "3730/3730 [==============================] - 0s 94us/step - loss: 0.7572 - acc: 0.6705\n",
      "Epoch 50/160\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.7542 - acc: 0.6780\n",
      "Epoch 51/160\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.7641 - acc: 0.6576\n",
      "Epoch 52/160\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7625 - acc: 0.6649\n",
      "Epoch 53/160\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7549 - acc: 0.6673\n",
      "Epoch 54/160\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7567 - acc: 0.6777\n",
      "Epoch 55/160\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.7530 - acc: 0.6753\n",
      "Epoch 56/160\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.7521 - acc: 0.6721\n",
      "Epoch 57/160\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7502 - acc: 0.6780\n",
      "Epoch 58/160\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.7476 - acc: 0.6871\n",
      "Epoch 59/160\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.7493 - acc: 0.6753\n",
      "Epoch 60/160\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7493 - acc: 0.6721\n",
      "Epoch 61/160\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7489 - acc: 0.6794\n",
      "Epoch 62/160\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.7474 - acc: 0.6799\n",
      "Epoch 63/160\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.7488 - acc: 0.6796\n",
      "Epoch 64/160\n",
      "3730/3730 [==============================] - 0s 91us/step - loss: 0.7452 - acc: 0.6836\n",
      "Epoch 65/160\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.7470 - acc: 0.6794\n",
      "Epoch 66/160\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.7423 - acc: 0.6794\n",
      "Epoch 67/160\n",
      "3730/3730 [==============================] - 0s 87us/step - loss: 0.7437 - acc: 0.6764\n",
      "Epoch 68/160\n",
      "3730/3730 [==============================] - 0s 88us/step - loss: 0.7416 - acc: 0.6796\n",
      "Epoch 69/160\n",
      "3730/3730 [==============================] - 0s 89us/step - loss: 0.7389 - acc: 0.6804\n",
      "Epoch 70/160\n",
      "3730/3730 [==============================] - 0s 89us/step - loss: 0.7368 - acc: 0.6871\n",
      "Epoch 71/160\n",
      "3730/3730 [==============================] - 0s 88us/step - loss: 0.7426 - acc: 0.6740\n",
      "Epoch 72/160\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7424 - acc: 0.6810\n",
      "Epoch 73/160\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.7425 - acc: 0.6815\n",
      "Epoch 74/160\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7320 - acc: 0.6826\n",
      "Epoch 75/160\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7332 - acc: 0.6863\n",
      "Epoch 76/160\n",
      "3730/3730 [==============================] - 0s 92us/step - loss: 0.7322 - acc: 0.6909\n",
      "Epoch 77/160\n",
      "3730/3730 [==============================] - 0s 91us/step - loss: 0.7438 - acc: 0.6753\n",
      "Epoch 78/160\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.7357 - acc: 0.6831\n",
      "Epoch 79/160\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.7329 - acc: 0.6861\n",
      "Epoch 80/160\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7293 - acc: 0.6861\n",
      "Epoch 81/160\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7377 - acc: 0.6732\n",
      "Epoch 82/160\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7281 - acc: 0.6882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/160\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.7362 - acc: 0.6839\n",
      "Epoch 84/160\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7254 - acc: 0.6928\n",
      "Epoch 85/160\n",
      "3730/3730 [==============================] - 0s 74us/step - loss: 0.7231 - acc: 0.6912\n",
      "Epoch 86/160\n",
      "3730/3730 [==============================] - 0s 74us/step - loss: 0.7266 - acc: 0.6871\n",
      "Epoch 87/160\n",
      "3730/3730 [==============================] - 0s 71us/step - loss: 0.7241 - acc: 0.6863\n",
      "Epoch 88/160\n",
      "3730/3730 [==============================] - 0s 73us/step - loss: 0.7243 - acc: 0.6909\n",
      "Epoch 89/160\n",
      "3730/3730 [==============================] - 0s 71us/step - loss: 0.7262 - acc: 0.6863\n",
      "Epoch 90/160\n",
      "3730/3730 [==============================] - 0s 70us/step - loss: 0.7193 - acc: 0.6882\n",
      "Epoch 91/160\n",
      "3730/3730 [==============================] - 0s 72us/step - loss: 0.7241 - acc: 0.6869\n",
      "Epoch 92/160\n",
      "3730/3730 [==============================] - 0s 68us/step - loss: 0.7276 - acc: 0.6877\n",
      "Epoch 93/160\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.7197 - acc: 0.6933\n",
      "Epoch 94/160\n",
      "3730/3730 [==============================] - 0s 72us/step - loss: 0.7209 - acc: 0.6858\n",
      "Epoch 95/160\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.7155 - acc: 0.6901\n",
      "Epoch 96/160\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.7218 - acc: 0.6909\n",
      "Epoch 97/160\n",
      "3730/3730 [==============================] - 0s 73us/step - loss: 0.7193 - acc: 0.6917\n",
      "Epoch 98/160\n",
      "3730/3730 [==============================] - 0s 72us/step - loss: 0.7217 - acc: 0.6882\n",
      "Epoch 99/160\n",
      "3730/3730 [==============================] - 0s 72us/step - loss: 0.7195 - acc: 0.6922\n",
      "Epoch 100/160\n",
      "3730/3730 [==============================] - 0s 74us/step - loss: 0.7157 - acc: 0.6936\n",
      "Epoch 101/160\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.7126 - acc: 0.6928\n",
      "Epoch 102/160\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7081 - acc: 0.6981\n",
      "Epoch 103/160\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.7102 - acc: 0.6871\n",
      "Epoch 104/160\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7066 - acc: 0.6965\n",
      "Epoch 105/160\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.7074 - acc: 0.6965\n",
      "Epoch 106/160\n",
      "3730/3730 [==============================] - 0s 74us/step - loss: 0.7075 - acc: 0.6882\n",
      "Epoch 107/160\n",
      "3730/3730 [==============================] - 0s 74us/step - loss: 0.7102 - acc: 0.6933\n",
      "Epoch 108/160\n",
      "3730/3730 [==============================] - 0s 73us/step - loss: 0.7142 - acc: 0.6866\n",
      "Epoch 109/160\n",
      "3730/3730 [==============================] - 0s 74us/step - loss: 0.7099 - acc: 0.6842\n",
      "Epoch 110/160\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.7146 - acc: 0.6890\n",
      "Epoch 111/160\n",
      "3730/3730 [==============================] - 0s 85us/step - loss: 0.7098 - acc: 0.6936\n",
      "Epoch 112/160\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7046 - acc: 0.6987\n",
      "Epoch 113/160\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.7029 - acc: 0.6960\n",
      "Epoch 114/160\n",
      "3730/3730 [==============================] - 0s 74us/step - loss: 0.7028 - acc: 0.6901\n",
      "Epoch 115/160\n",
      "3730/3730 [==============================] - 0s 74us/step - loss: 0.7034 - acc: 0.6992\n",
      "Epoch 116/160\n",
      "3730/3730 [==============================] - 0s 96us/step - loss: 0.7016 - acc: 0.6941\n",
      "Epoch 117/160\n",
      "3730/3730 [==============================] - 0s 106us/step - loss: 0.7021 - acc: 0.6920\n",
      "Epoch 118/160\n",
      "3730/3730 [==============================] - 0s 109us/step - loss: 0.6972 - acc: 0.7032\n",
      "Epoch 119/160\n",
      "3730/3730 [==============================] - 0s 115us/step - loss: 0.6981 - acc: 0.7019\n",
      "Epoch 120/160\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.6953 - acc: 0.6962\n",
      "Epoch 121/160\n",
      "3730/3730 [==============================] - 0s 96us/step - loss: 0.6977 - acc: 0.6984\n",
      "Epoch 122/160\n",
      "3730/3730 [==============================] - 0s 94us/step - loss: 0.6968 - acc: 0.6997\n",
      "Epoch 123/160\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.6876 - acc: 0.7027\n",
      "Epoch 124/160\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.6970 - acc: 0.6954\n",
      "Epoch 125/160\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.6879 - acc: 0.7038\n",
      "Epoch 126/160\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.6938 - acc: 0.7011\n",
      "Epoch 127/160\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.6961 - acc: 0.7013\n",
      "Epoch 128/160\n",
      "3730/3730 [==============================] - 0s 85us/step - loss: 0.6936 - acc: 0.6957\n",
      "Epoch 129/160\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.6837 - acc: 0.7064\n",
      "Epoch 130/160\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.6840 - acc: 0.7078\n",
      "Epoch 131/160\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.6926 - acc: 0.7040\n",
      "Epoch 132/160\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.6966 - acc: 0.6952\n",
      "Epoch 133/160\n",
      "3730/3730 [==============================] - 0s 74us/step - loss: 0.6869 - acc: 0.7016\n",
      "Epoch 134/160\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.6875 - acc: 0.7064\n",
      "Epoch 135/160\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.6903 - acc: 0.7032\n",
      "Epoch 136/160\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.6820 - acc: 0.7040\n",
      "Epoch 137/160\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.6832 - acc: 0.7078\n",
      "Epoch 138/160\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.6799 - acc: 0.7054\n",
      "Epoch 139/160\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.6761 - acc: 0.7005\n",
      "Epoch 140/160\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.6869 - acc: 0.6973\n",
      "Epoch 141/160\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.6784 - acc: 0.7062\n",
      "Epoch 142/160\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.6744 - acc: 0.7048\n",
      "Epoch 143/160\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.6697 - acc: 0.7094\n",
      "Epoch 144/160\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.6731 - acc: 0.7097\n",
      "Epoch 145/160\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.6702 - acc: 0.7110\n",
      "Epoch 146/160\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.6754 - acc: 0.6989\n",
      "Epoch 147/160\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.6788 - acc: 0.7032\n",
      "Epoch 148/160\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.6667 - acc: 0.7086\n",
      "Epoch 149/160\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.6722 - acc: 0.7102\n",
      "Epoch 150/160\n",
      "3730/3730 [==============================] - 0s 90us/step - loss: 0.6650 - acc: 0.7150\n",
      "Epoch 151/160\n",
      "3730/3730 [==============================] - 0s 118us/step - loss: 0.6699 - acc: 0.7110\n",
      "Epoch 152/160\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.6642 - acc: 0.7110\n",
      "Epoch 153/160\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.6753 - acc: 0.7070\n",
      "Epoch 154/160\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.6681 - acc: 0.7147\n",
      "Epoch 155/160\n",
      "3730/3730 [==============================] - 0s 85us/step - loss: 0.6660 - acc: 0.7123\n",
      "Epoch 156/160\n",
      "3730/3730 [==============================] - 0s 87us/step - loss: 0.6749 - acc: 0.7097\n",
      "Epoch 157/160\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.6652 - acc: 0.7115\n",
      "Epoch 158/160\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.6623 - acc: 0.7147\n",
      "Epoch 159/160\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.6576 - acc: 0.7193\n",
      "Epoch 160/160\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.6525 - acc: 0.7166\n",
      "1599/1599 [==============================] - 1s 377us/step\n",
      "\n",
      "acc: 68.36%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df['Class'].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d,y_d,test_size=0.3,random_state=42)\n",
    "\n",
    "y_train_2=pd.get_dummies(y_train)\n",
    "y_test_2=pd.get_dummies(y_test)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=160, callbacks=[early_stopping_monitor],batch_size=50)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/165\n",
      "3730/3730 [==============================] - 2s 535us/step - loss: 1.0621 - acc: 0.5418\n",
      "Epoch 2/165\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.9430 - acc: 0.5812\n",
      "Epoch 3/165\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.9049 - acc: 0.5925\n",
      "Epoch 4/165\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.8862 - acc: 0.6097\n",
      "Epoch 5/165\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.8611 - acc: 0.6255\n",
      "Epoch 6/165\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.8484 - acc: 0.6300\n",
      "Epoch 7/165\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.8393 - acc: 0.6282\n",
      "Epoch 8/165\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.8318 - acc: 0.6450\n",
      "Epoch 9/165\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.8284 - acc: 0.6389\n",
      "Epoch 10/165\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.8217 - acc: 0.6416\n",
      "Epoch 11/165\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.8156 - acc: 0.6410\n",
      "Epoch 12/165\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.8133 - acc: 0.6472\n",
      "Epoch 13/165\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.8086 - acc: 0.6480\n",
      "Epoch 14/165\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.8120 - acc: 0.6429\n",
      "Epoch 15/165\n",
      "3730/3730 [==============================] - 0s 73us/step - loss: 0.7977 - acc: 0.6491\n",
      "Epoch 16/165\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.8037 - acc: 0.6504\n",
      "Epoch 17/165\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7992 - acc: 0.6501\n",
      "Epoch 18/165\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.7958 - acc: 0.6566\n",
      "Epoch 19/165\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.7914 - acc: 0.6611\n",
      "Epoch 20/165\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.7913 - acc: 0.6499\n",
      "Epoch 21/165\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7929 - acc: 0.6598\n",
      "Epoch 22/165\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7936 - acc: 0.6552\n",
      "Epoch 23/165\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7974 - acc: 0.6542\n",
      "Epoch 24/165\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.7858 - acc: 0.6617\n",
      "Epoch 25/165\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7814 - acc: 0.6619\n",
      "Epoch 26/165\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.7782 - acc: 0.6552\n",
      "Epoch 27/165\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7787 - acc: 0.6630\n",
      "Epoch 28/165\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.7812 - acc: 0.6660\n",
      "Epoch 29/165\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7713 - acc: 0.6665\n",
      "Epoch 30/165\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7681 - acc: 0.6657\n",
      "Epoch 31/165\n",
      "3730/3730 [==============================] - 0s 74us/step - loss: 0.7742 - acc: 0.6662\n",
      "Epoch 32/165\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.7698 - acc: 0.6713\n",
      "Epoch 33/165\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7719 - acc: 0.6646\n",
      "Epoch 34/165\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7739 - acc: 0.6617\n",
      "Epoch 35/165\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.7730 - acc: 0.6649\n",
      "Epoch 36/165\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.7700 - acc: 0.6627\n",
      "Epoch 37/165\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.7713 - acc: 0.6617\n",
      "Epoch 38/165\n",
      "3730/3730 [==============================] - 0s 74us/step - loss: 0.7695 - acc: 0.6579\n",
      "Epoch 39/165\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.7605 - acc: 0.6716\n",
      "Epoch 40/165\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.7650 - acc: 0.6633\n",
      "Epoch 41/165\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.7580 - acc: 0.6737\n",
      "Epoch 42/165\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.7633 - acc: 0.6654\n",
      "Epoch 43/165\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7616 - acc: 0.6767\n",
      "Epoch 44/165\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.7576 - acc: 0.6721\n",
      "Epoch 45/165\n",
      "3730/3730 [==============================] - 0s 73us/step - loss: 0.7635 - acc: 0.6668\n",
      "Epoch 46/165\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.7575 - acc: 0.6718\n",
      "Epoch 47/165\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7636 - acc: 0.6622\n",
      "Epoch 48/165\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7540 - acc: 0.6743\n",
      "Epoch 49/165\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.7567 - acc: 0.6684\n",
      "Epoch 50/165\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7616 - acc: 0.6681\n",
      "Epoch 51/165\n",
      "3730/3730 [==============================] - 0s 87us/step - loss: 0.7546 - acc: 0.6678\n",
      "Epoch 52/165\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.7525 - acc: 0.6745\n",
      "Epoch 53/165\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.7535 - acc: 0.6718\n",
      "Epoch 54/165\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.7545 - acc: 0.6724\n",
      "Epoch 55/165\n",
      "3730/3730 [==============================] - 0s 85us/step - loss: 0.7454 - acc: 0.6751\n",
      "Epoch 56/165\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.7560 - acc: 0.6732\n",
      "Epoch 57/165\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.7537 - acc: 0.6780\n",
      "Epoch 58/165\n",
      "3730/3730 [==============================] - 0s 85us/step - loss: 0.7542 - acc: 0.6735\n",
      "Epoch 59/165\n",
      "3730/3730 [==============================] - 0s 94us/step - loss: 0.7501 - acc: 0.6759\n",
      "Epoch 60/165\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7498 - acc: 0.6716\n",
      "Epoch 61/165\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7565 - acc: 0.6716\n",
      "Epoch 62/165\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7496 - acc: 0.6759\n",
      "Epoch 63/165\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7470 - acc: 0.6783\n",
      "Epoch 64/165\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7518 - acc: 0.6732\n",
      "Epoch 65/165\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7473 - acc: 0.6767\n",
      "Epoch 66/165\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7476 - acc: 0.6732\n",
      "Epoch 67/165\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7491 - acc: 0.6764\n",
      "Epoch 68/165\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.7513 - acc: 0.6788\n",
      "Epoch 69/165\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7499 - acc: 0.6745\n",
      "Epoch 70/165\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7418 - acc: 0.6788\n",
      "Epoch 71/165\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7463 - acc: 0.6753\n",
      "Epoch 72/165\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7446 - acc: 0.6767\n",
      "Epoch 73/165\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7371 - acc: 0.6788\n",
      "Epoch 74/165\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7401 - acc: 0.6761\n",
      "Epoch 75/165\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.7324 - acc: 0.6823\n",
      "Epoch 76/165\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7397 - acc: 0.6796\n",
      "Epoch 77/165\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7338 - acc: 0.6871\n",
      "Epoch 78/165\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.7468 - acc: 0.6802\n",
      "Epoch 79/165\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.7333 - acc: 0.6836\n",
      "Epoch 80/165\n",
      "3730/3730 [==============================] - 0s 89us/step - loss: 0.7329 - acc: 0.6826\n",
      "Epoch 81/165\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.7377 - acc: 0.6866\n",
      "Epoch 82/165\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.7374 - acc: 0.6780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/165\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7393 - acc: 0.6826\n",
      "Epoch 84/165\n",
      "3730/3730 [==============================] - 0s 74us/step - loss: 0.7323 - acc: 0.6794\n",
      "Epoch 85/165\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.7301 - acc: 0.6796\n",
      "Epoch 86/165\n",
      "3730/3730 [==============================] - 0s 74us/step - loss: 0.7307 - acc: 0.6812\n",
      "Epoch 87/165\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7290 - acc: 0.6861\n",
      "Epoch 88/165\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7315 - acc: 0.6826\n",
      "Epoch 89/165\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7251 - acc: 0.6850\n",
      "Epoch 90/165\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7283 - acc: 0.6890\n",
      "Epoch 91/165\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.7274 - acc: 0.6831\n",
      "Epoch 92/165\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7331 - acc: 0.6823\n",
      "Epoch 93/165\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.7217 - acc: 0.6812\n",
      "Epoch 94/165\n",
      "3730/3730 [==============================] - 0s 73us/step - loss: 0.7238 - acc: 0.6871\n",
      "Epoch 95/165\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.7264 - acc: 0.6853\n",
      "Epoch 96/165\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7249 - acc: 0.6845\n",
      "Epoch 97/165\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.7221 - acc: 0.6909\n",
      "Epoch 98/165\n",
      "3730/3730 [==============================] - 0s 89us/step - loss: 0.7224 - acc: 0.6866\n",
      "Epoch 99/165\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7152 - acc: 0.6866\n",
      "Epoch 100/165\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7165 - acc: 0.6882\n",
      "Epoch 101/165\n",
      "3730/3730 [==============================] - 0s 73us/step - loss: 0.7172 - acc: 0.6823\n",
      "Epoch 102/165\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.7180 - acc: 0.6885\n",
      "Epoch 103/165\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.7190 - acc: 0.6898\n",
      "Epoch 104/165\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7175 - acc: 0.6858\n",
      "Epoch 105/165\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.7187 - acc: 0.6898\n",
      "Epoch 106/165\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7121 - acc: 0.6901\n",
      "Epoch 107/165\n",
      "3730/3730 [==============================] - 0s 74us/step - loss: 0.7154 - acc: 0.6882\n",
      "Epoch 108/165\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.7103 - acc: 0.6928\n",
      "Epoch 109/165\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.7103 - acc: 0.6968\n",
      "Epoch 110/165\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7079 - acc: 0.6922\n",
      "Epoch 111/165\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7079 - acc: 0.7003\n",
      "Epoch 112/165\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7094 - acc: 0.6879\n",
      "Epoch 113/165\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7113 - acc: 0.6936\n",
      "Epoch 114/165\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7012 - acc: 0.6944\n",
      "Epoch 115/165\n",
      "3730/3730 [==============================] - 0s 74us/step - loss: 0.7023 - acc: 0.6957\n",
      "Epoch 116/165\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.7105 - acc: 0.6928\n",
      "Epoch 117/165\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7073 - acc: 0.6925\n",
      "Epoch 118/165\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7062 - acc: 0.6925\n",
      "Epoch 119/165\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.7035 - acc: 0.6920\n",
      "Epoch 120/165\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.6941 - acc: 0.6997\n",
      "Epoch 121/165\n",
      "3730/3730 [==============================] - 0s 74us/step - loss: 0.7055 - acc: 0.6933\n",
      "Epoch 122/165\n",
      "3730/3730 [==============================] - 0s 73us/step - loss: 0.7024 - acc: 0.6944\n",
      "Epoch 123/165\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.7010 - acc: 0.6914\n",
      "Epoch 124/165\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.6973 - acc: 0.6957\n",
      "Epoch 125/165\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.6903 - acc: 0.6987\n",
      "Epoch 126/165\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.6963 - acc: 0.7003\n",
      "Epoch 127/165\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.6926 - acc: 0.6995\n",
      "Epoch 128/165\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.6894 - acc: 0.7008\n",
      "Epoch 129/165\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.6925 - acc: 0.6960\n",
      "Epoch 130/165\n",
      "3730/3730 [==============================] - 0s 74us/step - loss: 0.6880 - acc: 0.6941\n",
      "Epoch 131/165\n",
      "3730/3730 [==============================] - 0s 73us/step - loss: 0.6885 - acc: 0.7021\n",
      "Epoch 132/165\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.6905 - acc: 0.7008\n",
      "Epoch 133/165\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.6832 - acc: 0.7011\n",
      "Epoch 134/165\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.6882 - acc: 0.7043\n",
      "Epoch 135/165\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.6913 - acc: 0.6965\n",
      "Epoch 136/165\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.6887 - acc: 0.7032\n",
      "Epoch 137/165\n",
      "3730/3730 [==============================] - 0s 87us/step - loss: 0.6815 - acc: 0.7046\n",
      "Epoch 138/165\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.6868 - acc: 0.7078\n",
      "Epoch 139/165\n",
      "3730/3730 [==============================] - 0s 113us/step - loss: 0.6856 - acc: 0.7011\n",
      "Epoch 140/165\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.6837 - acc: 0.7035\n",
      "Epoch 141/165\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.6792 - acc: 0.7003\n",
      "Epoch 142/165\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.6755 - acc: 0.7086\n",
      "Epoch 143/165\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.6703 - acc: 0.7121\n",
      "Epoch 144/165\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.6723 - acc: 0.7054\n",
      "Epoch 145/165\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.6705 - acc: 0.7078\n",
      "Epoch 146/165\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.6751 - acc: 0.6976\n",
      "Epoch 147/165\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.6728 - acc: 0.7040\n",
      "Epoch 148/165\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.6786 - acc: 0.6960\n",
      "Epoch 149/165\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.6658 - acc: 0.7094\n",
      "Epoch 150/165\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.6704 - acc: 0.7107\n",
      "Epoch 151/165\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.6702 - acc: 0.7038\n",
      "Epoch 152/165\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.6612 - acc: 0.7062\n",
      "Epoch 153/165\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.6697 - acc: 0.7110\n",
      "Epoch 154/165\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.6621 - acc: 0.7110\n",
      "Epoch 155/165\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.6603 - acc: 0.7059\n",
      "Epoch 156/165\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.6674 - acc: 0.7021\n",
      "Epoch 157/165\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.6702 - acc: 0.7054\n",
      "Epoch 158/165\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.6591 - acc: 0.7099\n",
      "Epoch 159/165\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.6637 - acc: 0.7078\n",
      "Epoch 160/165\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.6554 - acc: 0.7123\n",
      "Epoch 161/165\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.6522 - acc: 0.7153\n",
      "Epoch 162/165\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.6525 - acc: 0.7169\n",
      "Epoch 163/165\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.6566 - acc: 0.7147\n",
      "Epoch 164/165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.6536 - acc: 0.7118\n",
      "Epoch 165/165\n",
      "3730/3730 [==============================] - 0s 73us/step - loss: 0.6526 - acc: 0.7099\n",
      "1599/1599 [==============================] - 1s 441us/step\n",
      "\n",
      "acc: 67.17%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df['Class'].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d,y_d,test_size=0.3,random_state=42)\n",
    "\n",
    "y_train_2=pd.get_dummies(y_train)\n",
    "y_test_2=pd.get_dummies(y_test)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=165, callbacks=[early_stopping_monitor],batch_size=50)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/155\n",
      "3730/3730 [==============================] - 2s 594us/step - loss: 1.0679 - acc: 0.5520\n",
      "Epoch 2/155\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.9521 - acc: 0.5895\n",
      "Epoch 3/155\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.8962 - acc: 0.6043\n",
      "Epoch 4/155\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.8823 - acc: 0.6029\n",
      "Epoch 5/155\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.8626 - acc: 0.6279\n",
      "Epoch 6/155\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.8610 - acc: 0.6260\n",
      "Epoch 7/155\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.8482 - acc: 0.6263\n",
      "Epoch 8/155\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.8436 - acc: 0.6290\n",
      "Epoch 9/155\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.8402 - acc: 0.6359\n",
      "Epoch 10/155\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.8340 - acc: 0.6359\n",
      "Epoch 11/155\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.8346 - acc: 0.6357\n",
      "Epoch 12/155\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.8228 - acc: 0.6389\n",
      "Epoch 13/155\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.8360 - acc: 0.6319\n",
      "Epoch 14/155\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.8241 - acc: 0.6383\n",
      "Epoch 15/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.8228 - acc: 0.6464\n",
      "Epoch 16/155\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.8201 - acc: 0.6386\n",
      "Epoch 17/155\n",
      "3730/3730 [==============================] - 1s 195us/step - loss: 0.8082 - acc: 0.6507\n",
      "Epoch 18/155\n",
      "3730/3730 [==============================] - 1s 217us/step - loss: 0.8049 - acc: 0.6512\n",
      "Epoch 19/155\n",
      "3730/3730 [==============================] - 1s 215us/step - loss: 0.8112 - acc: 0.6402\n",
      "Epoch 20/155\n",
      "3730/3730 [==============================] - 0s 95us/step - loss: 0.8017 - acc: 0.6466\n",
      "Epoch 21/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7887 - acc: 0.6590\n",
      "Epoch 22/155\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.7934 - acc: 0.6582\n",
      "Epoch 23/155\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.7999 - acc: 0.6544\n",
      "Epoch 24/155\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7886 - acc: 0.6507\n",
      "Epoch 25/155\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7864 - acc: 0.6622\n",
      "Epoch 26/155\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7825 - acc: 0.6611\n",
      "Epoch 27/155\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7806 - acc: 0.6566\n",
      "Epoch 28/155\n",
      "3730/3730 [==============================] - 0s 91us/step - loss: 0.7851 - acc: 0.6550\n",
      "Epoch 29/155\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.7797 - acc: 0.6566\n",
      "Epoch 30/155\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7814 - acc: 0.6534\n",
      "Epoch 31/155\n",
      "3730/3730 [==============================] - 0s 74us/step - loss: 0.7772 - acc: 0.6627\n",
      "Epoch 32/155\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7749 - acc: 0.6668\n",
      "Epoch 33/155\n",
      "3730/3730 [==============================] - 0s 74us/step - loss: 0.7787 - acc: 0.6574\n",
      "Epoch 34/155\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.7681 - acc: 0.6673\n",
      "Epoch 35/155\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7821 - acc: 0.6654\n",
      "Epoch 36/155\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7797 - acc: 0.6568\n",
      "Epoch 37/155\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7807 - acc: 0.6649\n",
      "Epoch 38/155\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7653 - acc: 0.6643\n",
      "Epoch 39/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7608 - acc: 0.6651\n",
      "Epoch 40/155\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7630 - acc: 0.6646\n",
      "Epoch 41/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7664 - acc: 0.6668\n",
      "Epoch 42/155\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.7566 - acc: 0.6702\n",
      "Epoch 43/155\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.7596 - acc: 0.6705\n",
      "Epoch 44/155\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7677 - acc: 0.6646\n",
      "Epoch 45/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7614 - acc: 0.6684\n",
      "Epoch 46/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7578 - acc: 0.6654\n",
      "Epoch 47/155\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7627 - acc: 0.6676\n",
      "Epoch 48/155\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7540 - acc: 0.6761\n",
      "Epoch 49/155\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7641 - acc: 0.6660\n",
      "Epoch 50/155\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.7590 - acc: 0.6767\n",
      "Epoch 51/155\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7583 - acc: 0.6745\n",
      "Epoch 52/155\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7554 - acc: 0.6727\n",
      "Epoch 53/155\n",
      "3730/3730 [==============================] - 0s 88us/step - loss: 0.7569 - acc: 0.6646\n",
      "Epoch 54/155\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.7545 - acc: 0.6710\n",
      "Epoch 55/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7545 - acc: 0.6708\n",
      "Epoch 56/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7567 - acc: 0.6681\n",
      "Epoch 57/155\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.7523 - acc: 0.6783\n",
      "Epoch 58/155\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.7488 - acc: 0.6718\n",
      "Epoch 59/155\n",
      "3730/3730 [==============================] - 0s 85us/step - loss: 0.7473 - acc: 0.6732\n",
      "Epoch 60/155\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7450 - acc: 0.6772\n",
      "Epoch 61/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7487 - acc: 0.6820\n",
      "Epoch 62/155\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7481 - acc: 0.6759\n",
      "Epoch 63/155\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7520 - acc: 0.6681\n",
      "Epoch 64/155\n",
      "3730/3730 [==============================] - 0s 85us/step - loss: 0.7424 - acc: 0.6756\n",
      "Epoch 65/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7453 - acc: 0.6756\n",
      "Epoch 66/155\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7426 - acc: 0.6748\n",
      "Epoch 67/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7431 - acc: 0.6729\n",
      "Epoch 68/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7432 - acc: 0.6807\n",
      "Epoch 69/155\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.7413 - acc: 0.6735\n",
      "Epoch 70/155\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7421 - acc: 0.6721\n",
      "Epoch 71/155\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.7401 - acc: 0.6828\n",
      "Epoch 72/155\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7478 - acc: 0.6794\n",
      "Epoch 73/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7434 - acc: 0.6732\n",
      "Epoch 74/155\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7377 - acc: 0.6796\n",
      "Epoch 75/155\n",
      "3730/3730 [==============================] - 0s 88us/step - loss: 0.7412 - acc: 0.6732\n",
      "Epoch 76/155\n",
      "3730/3730 [==============================] - 0s 88us/step - loss: 0.7393 - acc: 0.6769\n",
      "Epoch 77/155\n",
      "3730/3730 [==============================] - 0s 89us/step - loss: 0.7355 - acc: 0.6855\n",
      "Epoch 78/155\n",
      "3730/3730 [==============================] - 0s 85us/step - loss: 0.7388 - acc: 0.6767\n",
      "Epoch 79/155\n",
      "3730/3730 [==============================] - 0s 87us/step - loss: 0.7358 - acc: 0.6791\n",
      "Epoch 80/155\n",
      "3730/3730 [==============================] - 0s 87us/step - loss: 0.7291 - acc: 0.6853\n",
      "Epoch 81/155\n",
      "3730/3730 [==============================] - 0s 89us/step - loss: 0.7392 - acc: 0.6769\n",
      "Epoch 82/155\n",
      "3730/3730 [==============================] - 0s 90us/step - loss: 0.7369 - acc: 0.6812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/155\n",
      "3730/3730 [==============================] - 0s 87us/step - loss: 0.7320 - acc: 0.6842\n",
      "Epoch 84/155\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7344 - acc: 0.6823\n",
      "Epoch 85/155\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7265 - acc: 0.6877\n",
      "Epoch 86/155\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.7283 - acc: 0.6877\n",
      "Epoch 87/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7365 - acc: 0.6823\n",
      "Epoch 88/155\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7291 - acc: 0.6831\n",
      "Epoch 89/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7265 - acc: 0.6898\n",
      "Epoch 90/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7211 - acc: 0.6874\n",
      "Epoch 91/155\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7370 - acc: 0.6810\n",
      "Epoch 92/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7223 - acc: 0.6877\n",
      "Epoch 93/155\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.7227 - acc: 0.6853\n",
      "Epoch 94/155\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7202 - acc: 0.6925\n",
      "Epoch 95/155\n",
      "3730/3730 [==============================] - 0s 85us/step - loss: 0.7223 - acc: 0.6885\n",
      "Epoch 96/155\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7264 - acc: 0.6866\n",
      "Epoch 97/155\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7191 - acc: 0.6847\n",
      "Epoch 98/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7210 - acc: 0.6874\n",
      "Epoch 99/155\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7128 - acc: 0.6914\n",
      "Epoch 100/155\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7248 - acc: 0.6842\n",
      "Epoch 101/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7165 - acc: 0.6866\n",
      "Epoch 102/155\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7208 - acc: 0.6874\n",
      "Epoch 103/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7129 - acc: 0.6928\n",
      "Epoch 104/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7138 - acc: 0.6898\n",
      "Epoch 105/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7106 - acc: 0.6917\n",
      "Epoch 106/155\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7170 - acc: 0.6906\n",
      "Epoch 107/155\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7124 - acc: 0.6890\n",
      "Epoch 108/155\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7093 - acc: 0.6912\n",
      "Epoch 109/155\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7167 - acc: 0.6853\n",
      "Epoch 110/155\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7084 - acc: 0.6922\n",
      "Epoch 111/155\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.7110 - acc: 0.6920\n",
      "Epoch 112/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7164 - acc: 0.6895\n",
      "Epoch 113/155\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.7096 - acc: 0.6874\n",
      "Epoch 114/155\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7066 - acc: 0.6946\n",
      "Epoch 115/155\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7038 - acc: 0.6941\n",
      "Epoch 116/155\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7071 - acc: 0.6885\n",
      "Epoch 117/155\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.7035 - acc: 0.6968\n",
      "Epoch 118/155\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7031 - acc: 0.6962\n",
      "Epoch 119/155\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7020 - acc: 0.6944\n",
      "Epoch 120/155\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.7029 - acc: 0.7000\n",
      "Epoch 121/155\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.6975 - acc: 0.6938\n",
      "Epoch 122/155\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7090 - acc: 0.6877\n",
      "Epoch 123/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.6970 - acc: 0.6928\n",
      "Epoch 124/155\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.6984 - acc: 0.6995\n",
      "Epoch 125/155\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.6961 - acc: 0.6971\n",
      "Epoch 126/155\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.6947 - acc: 0.6936\n",
      "Epoch 127/155\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.6921 - acc: 0.6962\n",
      "Epoch 128/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.6879 - acc: 0.7013\n",
      "Epoch 129/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.6954 - acc: 0.7000\n",
      "Epoch 130/155\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.6964 - acc: 0.6979\n",
      "Epoch 131/155\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.6948 - acc: 0.6971\n",
      "Epoch 132/155\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.6883 - acc: 0.7019\n",
      "Epoch 133/155\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.6983 - acc: 0.6960\n",
      "Epoch 134/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.6921 - acc: 0.6987\n",
      "Epoch 135/155\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.6797 - acc: 0.7000\n",
      "Epoch 136/155\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.6792 - acc: 0.7046\n",
      "Epoch 137/155\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.6940 - acc: 0.7019\n",
      "Epoch 138/155\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.6890 - acc: 0.6992\n",
      "Epoch 139/155\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.6809 - acc: 0.7016\n",
      "Epoch 140/155\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.6806 - acc: 0.7032\n",
      "Epoch 141/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.6806 - acc: 0.7029\n",
      "Epoch 142/155\n",
      "3730/3730 [==============================] - 0s 76us/step - loss: 0.6821 - acc: 0.7008\n",
      "Epoch 143/155\n",
      "3730/3730 [==============================] - 0s 75us/step - loss: 0.6739 - acc: 0.7091\n",
      "Epoch 144/155\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.6828 - acc: 0.7048\n",
      "Epoch 145/155\n",
      "3730/3730 [==============================] - 0s 89us/step - loss: 0.6729 - acc: 0.7067\n",
      "Epoch 146/155\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.6729 - acc: 0.7072\n",
      "Epoch 147/155\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.6719 - acc: 0.7097\n",
      "Epoch 148/155\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.6746 - acc: 0.7029\n",
      "Epoch 149/155\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.6801 - acc: 0.7043\n",
      "Epoch 150/155\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.6740 - acc: 0.7070\n",
      "Epoch 151/155\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.6771 - acc: 0.7075\n",
      "Epoch 152/155\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.6772 - acc: 0.7038\n",
      "Epoch 153/155\n",
      "3730/3730 [==============================] - 0s 85us/step - loss: 0.6684 - acc: 0.7075\n",
      "Epoch 154/155\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.6745 - acc: 0.7000\n",
      "Epoch 155/155\n",
      "3730/3730 [==============================] - 0s 94us/step - loss: 0.6659 - acc: 0.7113\n",
      "1599/1599 [==============================] - 1s 539us/step\n",
      "\n",
      "acc: 68.11%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df['Class'].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d,y_d,test_size=0.3,random_state=42)\n",
    "\n",
    "y_train_2=pd.get_dummies(y_train)\n",
    "y_test_2=pd.get_dummies(y_test)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=155, callbacks=[early_stopping_monitor],batch_size=50)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/157\n",
      "3730/3730 [==============================] - 2s 605us/step - loss: 1.0643 - acc: 0.5501\n",
      "Epoch 2/157\n",
      "3730/3730 [==============================] - 0s 85us/step - loss: 0.9468 - acc: 0.5858\n",
      "Epoch 3/157\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.9112 - acc: 0.5925\n",
      "Epoch 4/157\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.8835 - acc: 0.6072\n",
      "Epoch 5/157\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.8553 - acc: 0.6370\n",
      "Epoch 6/157\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.8532 - acc: 0.6228\n",
      "Epoch 7/157\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.8475 - acc: 0.6308\n",
      "Epoch 8/157\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.8353 - acc: 0.6378\n",
      "Epoch 9/157\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.8368 - acc: 0.6303\n",
      "Epoch 10/157\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.8341 - acc: 0.6405\n",
      "Epoch 11/157\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.8256 - acc: 0.6343\n",
      "Epoch 12/157\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.8257 - acc: 0.6426\n",
      "Epoch 13/157\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.8139 - acc: 0.6491\n",
      "Epoch 14/157\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.8100 - acc: 0.6450\n",
      "Epoch 15/157\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.8094 - acc: 0.6469\n",
      "Epoch 16/157\n",
      "3730/3730 [==============================] - 0s 70us/step - loss: 0.8025 - acc: 0.6531\n",
      "Epoch 17/157\n",
      "3730/3730 [==============================] - 0s 70us/step - loss: 0.7975 - acc: 0.6563\n",
      "Epoch 18/157\n",
      "3730/3730 [==============================] - 0s 73us/step - loss: 0.7979 - acc: 0.6544\n",
      "Epoch 19/157\n",
      "3730/3730 [==============================] - 0s 70us/step - loss: 0.7940 - acc: 0.6560\n",
      "Epoch 20/157\n",
      "3730/3730 [==============================] - 0s 68us/step - loss: 0.7888 - acc: 0.6582\n",
      "Epoch 21/157\n",
      "3730/3730 [==============================] - 0s 63us/step - loss: 0.7892 - acc: 0.6579\n",
      "Epoch 22/157\n",
      "3730/3730 [==============================] - 0s 66us/step - loss: 0.7940 - acc: 0.6507\n",
      "Epoch 23/157\n",
      "3730/3730 [==============================] - 0s 87us/step - loss: 0.7888 - acc: 0.6552\n",
      "Epoch 24/157\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7804 - acc: 0.6584\n",
      "Epoch 25/157\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7890 - acc: 0.6507\n",
      "Epoch 26/157\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7835 - acc: 0.6558\n",
      "Epoch 27/157\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7769 - acc: 0.6633\n",
      "Epoch 28/157\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7758 - acc: 0.6582\n",
      "Epoch 29/157\n",
      "3730/3730 [==============================] - 0s 87us/step - loss: 0.7728 - acc: 0.6651\n",
      "Epoch 30/157\n",
      "3730/3730 [==============================] - 0s 89us/step - loss: 0.7838 - acc: 0.6601\n",
      "Epoch 31/157\n",
      "3730/3730 [==============================] - 0s 85us/step - loss: 0.7716 - acc: 0.6700\n",
      "Epoch 32/157\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.7680 - acc: 0.6681\n",
      "Epoch 33/157\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7687 - acc: 0.6635\n",
      "Epoch 34/157\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7695 - acc: 0.6584\n",
      "Epoch 35/157\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.7693 - acc: 0.6582\n",
      "Epoch 36/157\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7682 - acc: 0.6694\n",
      "Epoch 37/157\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7723 - acc: 0.6630\n",
      "Epoch 38/157\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.7746 - acc: 0.6670\n",
      "Epoch 39/157\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7634 - acc: 0.6702\n",
      "Epoch 40/157\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7665 - acc: 0.6710\n",
      "Epoch 41/157\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7666 - acc: 0.6649\n",
      "Epoch 42/157\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.7557 - acc: 0.6767\n",
      "Epoch 43/157\n",
      "3730/3730 [==============================] - 0s 78us/step - loss: 0.7616 - acc: 0.6678\n",
      "Epoch 44/157\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7596 - acc: 0.6660\n",
      "Epoch 45/157\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7544 - acc: 0.6753\n",
      "Epoch 46/157\n",
      "3730/3730 [==============================] - 0s 77us/step - loss: 0.7584 - acc: 0.6702\n",
      "Epoch 47/157\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7607 - acc: 0.6705\n",
      "Epoch 48/157\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7635 - acc: 0.6673\n",
      "Epoch 49/157\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.7562 - acc: 0.6686\n",
      "Epoch 50/157\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.7567 - acc: 0.6676\n",
      "Epoch 51/157\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7522 - acc: 0.6743\n",
      "Epoch 52/157\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7554 - acc: 0.6678\n",
      "Epoch 53/157\n",
      "3730/3730 [==============================] - 0s 88us/step - loss: 0.7632 - acc: 0.6654\n",
      "Epoch 54/157\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.7506 - acc: 0.6775\n",
      "Epoch 55/157\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7564 - acc: 0.6654\n",
      "Epoch 56/157\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7478 - acc: 0.6729\n",
      "Epoch 57/157\n",
      "3730/3730 [==============================] - 0s 88us/step - loss: 0.7505 - acc: 0.6729\n",
      "Epoch 58/157\n",
      "3730/3730 [==============================] - 0s 85us/step - loss: 0.7540 - acc: 0.6721\n",
      "Epoch 59/157\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7536 - acc: 0.6635\n",
      "Epoch 60/157\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.7496 - acc: 0.6748\n",
      "Epoch 61/157\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7501 - acc: 0.6729\n",
      "Epoch 62/157\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.7432 - acc: 0.6812\n",
      "Epoch 63/157\n",
      "3730/3730 [==============================] - 0s 89us/step - loss: 0.7506 - acc: 0.6751\n",
      "Epoch 64/157\n",
      "3730/3730 [==============================] - 0s 85us/step - loss: 0.7517 - acc: 0.6694\n",
      "Epoch 65/157\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.7445 - acc: 0.6759\n",
      "Epoch 66/157\n",
      "3730/3730 [==============================] - 0s 85us/step - loss: 0.7449 - acc: 0.6834\n",
      "Epoch 67/157\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.7448 - acc: 0.6767\n",
      "Epoch 68/157\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.7470 - acc: 0.6748\n",
      "Epoch 69/157\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.7418 - acc: 0.6772\n",
      "Epoch 70/157\n",
      "3730/3730 [==============================] - 0s 82us/step - loss: 0.7398 - acc: 0.6786\n",
      "Epoch 71/157\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.7400 - acc: 0.6777\n",
      "Epoch 72/157\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.7403 - acc: 0.6759\n",
      "Epoch 73/157\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7439 - acc: 0.6751\n",
      "Epoch 74/157\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7401 - acc: 0.6826\n",
      "Epoch 75/157\n",
      "3730/3730 [==============================] - 0s 85us/step - loss: 0.7414 - acc: 0.6716\n",
      "Epoch 76/157\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.7401 - acc: 0.6858\n",
      "Epoch 77/157\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.7374 - acc: 0.6826\n",
      "Epoch 78/157\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.7335 - acc: 0.6780\n",
      "Epoch 79/157\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7334 - acc: 0.6842\n",
      "Epoch 80/157\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.7359 - acc: 0.6786\n",
      "Epoch 81/157\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.7413 - acc: 0.6802\n",
      "Epoch 82/157\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.7343 - acc: 0.6783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/157\n",
      "3730/3730 [==============================] - 0s 81us/step - loss: 0.7351 - acc: 0.6823\n",
      "Epoch 84/157\n",
      "3730/3730 [==============================] - 0s 79us/step - loss: 0.7290 - acc: 0.6850\n",
      "Epoch 85/157\n",
      "3730/3730 [==============================] - 0s 80us/step - loss: 0.7309 - acc: 0.6836\n",
      "Epoch 86/157\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.7270 - acc: 0.6877\n",
      "Epoch 87/157\n",
      "3730/3730 [==============================] - 0s 87us/step - loss: 0.7438 - acc: 0.6724\n",
      "Epoch 88/157\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.7345 - acc: 0.6820\n",
      "Epoch 89/157\n",
      "3730/3730 [==============================] - 0s 88us/step - loss: 0.7258 - acc: 0.6914\n",
      "Epoch 90/157\n",
      "3730/3730 [==============================] - 0s 85us/step - loss: 0.7230 - acc: 0.6866\n",
      "Epoch 91/157\n",
      "3730/3730 [==============================] - 0s 87us/step - loss: 0.7302 - acc: 0.6847\n",
      "Epoch 92/157\n",
      "3730/3730 [==============================] - 0s 87us/step - loss: 0.7300 - acc: 0.6820\n",
      "Epoch 93/157\n",
      "3730/3730 [==============================] - 0s 96us/step - loss: 0.7230 - acc: 0.6839\n",
      "Epoch 94/157\n",
      "3730/3730 [==============================] - 0s 95us/step - loss: 0.7266 - acc: 0.6895\n",
      "Epoch 95/157\n",
      "3730/3730 [==============================] - 0s 92us/step - loss: 0.7279 - acc: 0.6826\n",
      "Epoch 96/157\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.7174 - acc: 0.6890\n",
      "Epoch 97/157\n",
      "3730/3730 [==============================] - 0s 91us/step - loss: 0.7214 - acc: 0.6898\n",
      "Epoch 98/157\n",
      "3730/3730 [==============================] - 0s 96us/step - loss: 0.7311 - acc: 0.6826\n",
      "Epoch 99/157\n",
      "3730/3730 [==============================] - 0s 95us/step - loss: 0.7224 - acc: 0.6906\n",
      "Epoch 100/157\n",
      "3730/3730 [==============================] - 0s 91us/step - loss: 0.7182 - acc: 0.6866\n",
      "Epoch 101/157\n",
      "3730/3730 [==============================] - 0s 94us/step - loss: 0.7189 - acc: 0.6861\n",
      "Epoch 102/157\n",
      "3730/3730 [==============================] - 0s 96us/step - loss: 0.7153 - acc: 0.6960\n",
      "Epoch 103/157\n",
      "3730/3730 [==============================] - 0s 91us/step - loss: 0.7121 - acc: 0.6879\n",
      "Epoch 104/157\n",
      "3730/3730 [==============================] - 0s 97us/step - loss: 0.7202 - acc: 0.6898\n",
      "Epoch 105/157\n",
      "3730/3730 [==============================] - 0s 98us/step - loss: 0.7129 - acc: 0.6917\n",
      "Epoch 106/157\n",
      "3730/3730 [==============================] - 0s 94us/step - loss: 0.7134 - acc: 0.6922\n",
      "Epoch 107/157\n",
      "3730/3730 [==============================] - 0s 92us/step - loss: 0.7109 - acc: 0.6925\n",
      "Epoch 108/157\n",
      "3730/3730 [==============================] - 0s 89us/step - loss: 0.7219 - acc: 0.6887\n",
      "Epoch 109/157\n",
      "3730/3730 [==============================] - 0s 94us/step - loss: 0.7092 - acc: 0.6901\n",
      "Epoch 110/157\n",
      "3730/3730 [==============================] - 0s 94us/step - loss: 0.7087 - acc: 0.6930\n",
      "Epoch 111/157\n",
      "3730/3730 [==============================] - 0s 90us/step - loss: 0.7101 - acc: 0.6887\n",
      "Epoch 112/157\n",
      "3730/3730 [==============================] - 0s 90us/step - loss: 0.7068 - acc: 0.6933\n",
      "Epoch 113/157\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.7032 - acc: 0.6979\n",
      "Epoch 114/157\n",
      "3730/3730 [==============================] - 0s 85us/step - loss: 0.7074 - acc: 0.6930\n",
      "Epoch 115/157\n",
      "3730/3730 [==============================] - 0s 88us/step - loss: 0.7071 - acc: 0.6920\n",
      "Epoch 116/157\n",
      "3730/3730 [==============================] - 0s 88us/step - loss: 0.7079 - acc: 0.6962\n",
      "Epoch 117/157\n",
      "3730/3730 [==============================] - 0s 85us/step - loss: 0.6990 - acc: 0.6989\n",
      "Epoch 118/157\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.6996 - acc: 0.6941\n",
      "Epoch 119/157\n",
      "3730/3730 [==============================] - 0s 88us/step - loss: 0.7023 - acc: 0.6946\n",
      "Epoch 120/157\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.7023 - acc: 0.6960\n",
      "Epoch 121/157\n",
      "3730/3730 [==============================] - 0s 87us/step - loss: 0.7003 - acc: 0.6987\n",
      "Epoch 122/157\n",
      "3730/3730 [==============================] - 0s 92us/step - loss: 0.7076 - acc: 0.6973\n",
      "Epoch 123/157\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.6984 - acc: 0.6877\n",
      "Epoch 124/157\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.6988 - acc: 0.6960\n",
      "Epoch 125/157\n",
      "3730/3730 [==============================] - 0s 87us/step - loss: 0.6962 - acc: 0.7005\n",
      "Epoch 126/157\n",
      "3730/3730 [==============================] - 0s 88us/step - loss: 0.7074 - acc: 0.6957\n",
      "Epoch 127/157\n",
      "3730/3730 [==============================] - 0s 89us/step - loss: 0.6957 - acc: 0.6938\n",
      "Epoch 128/157\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.6899 - acc: 0.7029\n",
      "Epoch 129/157\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.6978 - acc: 0.6936\n",
      "Epoch 130/157\n",
      "3730/3730 [==============================] - 0s 85us/step - loss: 0.6937 - acc: 0.7027\n",
      "Epoch 131/157\n",
      "3730/3730 [==============================] - 0s 85us/step - loss: 0.6952 - acc: 0.7003\n",
      "Epoch 132/157\n",
      "3730/3730 [==============================] - 0s 87us/step - loss: 0.6935 - acc: 0.6973\n",
      "Epoch 133/157\n",
      "3730/3730 [==============================] - 0s 89us/step - loss: 0.6878 - acc: 0.6997\n",
      "Epoch 134/157\n",
      "3730/3730 [==============================] - 0s 84us/step - loss: 0.6930 - acc: 0.7027\n",
      "Epoch 135/157\n",
      "3730/3730 [==============================] - 0s 83us/step - loss: 0.6919 - acc: 0.6979\n",
      "Epoch 136/157\n",
      "3730/3730 [==============================] - 0s 88us/step - loss: 0.6838 - acc: 0.7072\n",
      "Epoch 137/157\n",
      "3730/3730 [==============================] - 0s 94us/step - loss: 0.6854 - acc: 0.7000\n",
      "Epoch 138/157\n",
      "3730/3730 [==============================] - 0s 91us/step - loss: 0.6847 - acc: 0.7005\n",
      "Epoch 139/157\n",
      "3730/3730 [==============================] - 0s 92us/step - loss: 0.6780 - acc: 0.7088\n",
      "Epoch 140/157\n",
      "3730/3730 [==============================] - 0s 93us/step - loss: 0.6882 - acc: 0.7021\n",
      "Epoch 141/157\n",
      "3730/3730 [==============================] - 0s 93us/step - loss: 0.6863 - acc: 0.7019\n",
      "Epoch 142/157\n",
      "3730/3730 [==============================] - 0s 93us/step - loss: 0.6919 - acc: 0.6962\n",
      "Epoch 143/157\n",
      "3730/3730 [==============================] - 0s 99us/step - loss: 0.6690 - acc: 0.7113\n",
      "Epoch 144/157\n",
      "3730/3730 [==============================] - 0s 95us/step - loss: 0.6795 - acc: 0.7038\n",
      "Epoch 145/157\n",
      "3730/3730 [==============================] - 0s 90us/step - loss: 0.6749 - acc: 0.7078\n",
      "Epoch 146/157\n",
      "3730/3730 [==============================] - 0s 96us/step - loss: 0.6806 - acc: 0.7062\n",
      "Epoch 147/157\n",
      "3730/3730 [==============================] - 0s 92us/step - loss: 0.6837 - acc: 0.7005\n",
      "Epoch 148/157\n",
      "3730/3730 [==============================] - 0s 88us/step - loss: 0.6833 - acc: 0.7021\n",
      "Epoch 149/157\n",
      "3730/3730 [==============================] - 0s 93us/step - loss: 0.6789 - acc: 0.7035\n",
      "Epoch 150/157\n",
      "3730/3730 [==============================] - 0s 90us/step - loss: 0.6748 - acc: 0.7062\n",
      "Epoch 151/157\n",
      "3730/3730 [==============================] - 0s 90us/step - loss: 0.6803 - acc: 0.7054\n",
      "Epoch 152/157\n",
      "3730/3730 [==============================] - 0s 90us/step - loss: 0.6736 - acc: 0.7038\n",
      "Epoch 153/157\n",
      "3730/3730 [==============================] - 0s 86us/step - loss: 0.6695 - acc: 0.7107\n",
      "Epoch 154/157\n",
      "3730/3730 [==============================] - 0s 89us/step - loss: 0.6731 - acc: 0.7088\n",
      "Epoch 155/157\n",
      "3730/3730 [==============================] - 0s 88us/step - loss: 0.6693 - acc: 0.7121\n",
      "Epoch 156/157\n",
      "3730/3730 [==============================] - 0s 109us/step - loss: 0.6592 - acc: 0.7102\n",
      "Epoch 157/157\n",
      "3730/3730 [==============================] - 0s 104us/step - loss: 0.6788 - acc: 0.7043\n",
      "1599/1599 [==============================] - 1s 499us/step\n",
      "\n",
      "acc: 67.48%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df['Class'].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d,y_d,test_size=0.3,random_state=42)\n",
    "\n",
    "y_train_2=pd.get_dummies(y_train)\n",
    "y_test_2=pd.get_dummies(y_test)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=157, callbacks=[early_stopping_monitor],batch_size=50)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/160\n",
      "1350/3730 [=========>....................] - ETA: 6s - loss: 1.1149 - acc: 0.5430"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-9462e8b87137>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping_monitor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df['Class'].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d,y_d,test_size=0.3,random_state=42)\n",
    "\n",
    "y_train_2=pd.get_dummies(y_train)\n",
    "y_test_2=pd.get_dummies(y_test)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(650, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=160, callbacks=[early_stopping_monitor],batch_size=50)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "scores_2 = model.evaluate(X_train, y_train_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_2[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Epoch 1/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 1.0155 - acc: 0.5681\n",
      "Epoch 2/160\n",
      "4263/4263 [==============================] - 3s 658us/step - loss: 0.9113 - acc: 0.6017\n",
      "Epoch 3/160\n",
      "4263/4263 [==============================] - 3s 644us/step - loss: 0.8826 - acc: 0.6108\n",
      "Epoch 4/160\n",
      "4263/4263 [==============================] - 3s 678us/step - loss: 0.8766 - acc: 0.6064\n",
      "Epoch 5/160\n",
      "4263/4263 [==============================] - 3s 695us/step - loss: 0.8699 - acc: 0.6244\n",
      "Epoch 6/160\n",
      "4263/4263 [==============================] - 3s 704us/step - loss: 0.8516 - acc: 0.6362\n",
      "Epoch 7/160\n",
      "4263/4263 [==============================] - 3s 747us/step - loss: 0.8414 - acc: 0.6369\n",
      "Epoch 8/160\n",
      "4263/4263 [==============================] - 3s 700us/step - loss: 0.8368 - acc: 0.6331\n",
      "Epoch 9/160\n",
      "4263/4263 [==============================] - 3s 689us/step - loss: 0.8259 - acc: 0.6465\n",
      "Epoch 10/160\n",
      "4263/4263 [==============================] - 3s 663us/step - loss: 0.8305 - acc: 0.6420\n",
      "Epoch 11/160\n",
      "4263/4263 [==============================] - 3s 676us/step - loss: 0.8207 - acc: 0.6453\n",
      "Epoch 12/160\n",
      "4263/4263 [==============================] - 3s 753us/step - loss: 0.8299 - acc: 0.6458\n",
      "Epoch 13/160\n",
      "4263/4263 [==============================] - 4s 858us/step - loss: 0.8323 - acc: 0.6416\n",
      "Epoch 14/160\n",
      "4263/4263 [==============================] - 4s 897us/step - loss: 0.8180 - acc: 0.6517\n",
      "Epoch 15/160\n",
      "4263/4263 [==============================] - 4s 1ms/step - loss: 0.8207 - acc: 0.6493\n",
      "Epoch 16/160\n",
      "4263/4263 [==============================] - 3s 805us/step - loss: 0.8112 - acc: 0.6491\n",
      "Epoch 17/160\n",
      "4263/4263 [==============================] - 4s 832us/step - loss: 0.8094 - acc: 0.6547\n",
      "Epoch 18/160\n",
      "4263/4263 [==============================] - 4s 975us/step - loss: 0.8087 - acc: 0.6507\n",
      "Epoch 19/160\n",
      "4263/4263 [==============================] - 4s 883us/step - loss: 0.8029 - acc: 0.6500\n",
      "Epoch 20/160\n",
      "4263/4263 [==============================] - 3s 807us/step - loss: 0.8077 - acc: 0.6531\n",
      "Epoch 21/160\n",
      "4263/4263 [==============================] - 3s 794us/step - loss: 0.8049 - acc: 0.6538\n",
      "Epoch 22/160\n",
      "4263/4263 [==============================] - 3s 799us/step - loss: 0.7976 - acc: 0.6561\n",
      "Epoch 23/160\n",
      "4263/4263 [==============================] - 3s 798us/step - loss: 0.7927 - acc: 0.6563\n",
      "Epoch 24/160\n",
      "4263/4263 [==============================] - 3s 788us/step - loss: 0.7961 - acc: 0.6566\n",
      "Epoch 25/160\n",
      "4263/4263 [==============================] - 4s 864us/step - loss: 0.7984 - acc: 0.6615\n",
      "Epoch 26/160\n",
      "4263/4263 [==============================] - 4s 944us/step - loss: 0.7786 - acc: 0.6639\n",
      "Epoch 27/160\n",
      "4263/4263 [==============================] - 3s 806us/step - loss: 0.7908 - acc: 0.6519\n",
      "Epoch 28/160\n",
      "4263/4263 [==============================] - 4s 893us/step - loss: 0.7985 - acc: 0.6505\n",
      "Epoch 29/160\n",
      "4263/4263 [==============================] - 4s 890us/step - loss: 0.7785 - acc: 0.6676\n",
      "Epoch 30/160\n",
      "4263/4263 [==============================] - 3s 801us/step - loss: 0.7879 - acc: 0.6599\n",
      "Epoch 31/160\n",
      "4263/4263 [==============================] - 3s 799us/step - loss: 0.7818 - acc: 0.6570\n",
      "Epoch 32/160\n",
      "4263/4263 [==============================] - 4s 860us/step - loss: 0.7846 - acc: 0.6533\n",
      "Epoch 33/160\n",
      "4263/4263 [==============================] - 4s 939us/step - loss: 0.7730 - acc: 0.6596\n",
      "Epoch 34/160\n",
      "4263/4263 [==============================] - 3s 807us/step - loss: 0.7989 - acc: 0.6486\n",
      "Epoch 35/160\n",
      "4263/4263 [==============================] - 3s 786us/step - loss: 0.7749 - acc: 0.6674\n",
      "Epoch 36/160\n",
      "4263/4263 [==============================] - 3s 794us/step - loss: 0.7759 - acc: 0.6662\n",
      "Epoch 37/160\n",
      "4263/4263 [==============================] - 3s 796us/step - loss: 0.7769 - acc: 0.6570\n",
      "Epoch 38/160\n",
      "4263/4263 [==============================] - 3s 820us/step - loss: 0.7614 - acc: 0.6674\n",
      "Epoch 39/160\n",
      "4263/4263 [==============================] - 4s 948us/step - loss: 0.7749 - acc: 0.6641\n",
      "Epoch 40/160\n",
      "4263/4263 [==============================] - 4s 866us/step - loss: 0.7770 - acc: 0.6587\n",
      "Epoch 41/160\n",
      "4263/4263 [==============================] - 3s 805us/step - loss: 0.7691 - acc: 0.6622\n",
      "Epoch 42/160\n",
      "4263/4263 [==============================] - 3s 795us/step - loss: 0.7644 - acc: 0.6629\n",
      "Epoch 43/160\n",
      "4263/4263 [==============================] - 4s 941us/step - loss: 0.7681 - acc: 0.6664\n",
      "Epoch 44/160\n",
      "4263/4263 [==============================] - 4s 862us/step - loss: 0.7627 - acc: 0.6617\n",
      "Epoch 45/160\n",
      "4263/4263 [==============================] - 3s 795us/step - loss: 0.7613 - acc: 0.6667\n",
      "Epoch 46/160\n",
      "4263/4263 [==============================] - 3s 790us/step - loss: 0.7744 - acc: 0.6552\n",
      "Epoch 47/160\n",
      "4263/4263 [==============================] - 3s 791us/step - loss: 0.7769 - acc: 0.6631\n",
      "Epoch 48/160\n",
      "4263/4263 [==============================] - 3s 763us/step - loss: 0.7686 - acc: 0.6629\n",
      "Epoch 49/160\n",
      "4263/4263 [==============================] - 3s 785us/step - loss: 0.7636 - acc: 0.6643\n",
      "Epoch 50/160\n",
      "4263/4263 [==============================] - 3s 806us/step - loss: 0.7614 - acc: 0.6664\n",
      "Epoch 51/160\n",
      "4263/4263 [==============================] - 3s 794us/step - loss: 0.7623 - acc: 0.6692\n",
      "Epoch 52/160\n",
      "4263/4263 [==============================] - 3s 789us/step - loss: 0.7576 - acc: 0.6700\n",
      "Epoch 53/160\n",
      "4263/4263 [==============================] - 3s 794us/step - loss: 0.7572 - acc: 0.6714\n",
      "Epoch 54/160\n",
      "4263/4263 [==============================] - 3s 808us/step - loss: 0.7648 - acc: 0.6622\n",
      "Epoch 55/160\n",
      "4263/4263 [==============================] - 3s 792us/step - loss: 0.7573 - acc: 0.6711\n",
      "Epoch 56/160\n",
      "4263/4263 [==============================] - 3s 799us/step - loss: 0.7622 - acc: 0.6667\n",
      "Epoch 57/160\n",
      "4263/4263 [==============================] - 3s 797us/step - loss: 0.7582 - acc: 0.6627\n",
      "Epoch 58/160\n",
      "4263/4263 [==============================] - 3s 792us/step - loss: 0.7657 - acc: 0.6646\n",
      "Epoch 59/160\n",
      "4263/4263 [==============================] - 3s 803us/step - loss: 0.7641 - acc: 0.6657\n",
      "Epoch 60/160\n",
      "4263/4263 [==============================] - 3s 794us/step - loss: 0.7599 - acc: 0.6704\n",
      "Epoch 61/160\n",
      "4263/4263 [==============================] - 3s 801us/step - loss: 0.7558 - acc: 0.6615\n",
      "Epoch 62/160\n",
      "4263/4263 [==============================] - 3s 798us/step - loss: 0.7565 - acc: 0.6709\n",
      "Epoch 63/160\n",
      "4263/4263 [==============================] - 3s 797us/step - loss: 0.7527 - acc: 0.6763\n",
      "Epoch 64/160\n",
      "4263/4263 [==============================] - 3s 802us/step - loss: 0.7566 - acc: 0.6685\n",
      "Epoch 65/160\n",
      "4263/4263 [==============================] - 3s 781us/step - loss: 0.7527 - acc: 0.6739\n",
      "Epoch 66/160\n",
      "4263/4263 [==============================] - 3s 770us/step - loss: 0.7525 - acc: 0.6746\n",
      "Epoch 67/160\n",
      "4263/4263 [==============================] - 4s 848us/step - loss: 0.7660 - acc: 0.6620\n",
      "Epoch 68/160\n",
      "4263/4263 [==============================] - 3s 793us/step - loss: 0.7557 - acc: 0.6697\n",
      "Epoch 69/160\n",
      "4263/4263 [==============================] - 3s 776us/step - loss: 0.7587 - acc: 0.6746\n",
      "Epoch 70/160\n",
      "4263/4263 [==============================] - 4s 1ms/step - loss: 0.7582 - acc: 0.6639\n",
      "Epoch 71/160\n",
      "4263/4263 [==============================] - 4s 988us/step - loss: 0.7440 - acc: 0.6784\n",
      "Epoch 72/160\n",
      "4263/4263 [==============================] - 3s 815us/step - loss: 0.7491 - acc: 0.6777\n",
      "Epoch 73/160\n",
      "4263/4263 [==============================] - 3s 799us/step - loss: 0.7494 - acc: 0.6728\n",
      "Epoch 74/160\n",
      "4263/4263 [==============================] - 3s 799us/step - loss: 0.7561 - acc: 0.6714\n",
      "Epoch 75/160\n",
      "4263/4263 [==============================] - 3s 788us/step - loss: 0.7503 - acc: 0.6732\n",
      "Epoch 76/160\n",
      "4263/4263 [==============================] - 3s 795us/step - loss: 0.7498 - acc: 0.6779\n",
      "Epoch 77/160\n",
      "4263/4263 [==============================] - 3s 798us/step - loss: 0.7534 - acc: 0.6709\n",
      "Epoch 78/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 3s 752us/step - loss: 0.7471 - acc: 0.6760\n",
      "Epoch 79/160\n",
      "4263/4263 [==============================] - 3s 746us/step - loss: 0.7466 - acc: 0.6789\n",
      "Epoch 80/160\n",
      "4263/4263 [==============================] - 3s 741us/step - loss: 0.7544 - acc: 0.6657\n",
      "Epoch 81/160\n",
      "4263/4263 [==============================] - 3s 751us/step - loss: 0.7408 - acc: 0.6800\n",
      "Epoch 82/160\n",
      "4263/4263 [==============================] - 3s 766us/step - loss: 0.7444 - acc: 0.6702\n",
      "Epoch 83/160\n",
      "4263/4263 [==============================] - 3s 763us/step - loss: 0.7351 - acc: 0.6812\n",
      "Epoch 84/160\n",
      "4263/4263 [==============================] - 3s 781us/step - loss: 0.7455 - acc: 0.6782\n",
      "Epoch 85/160\n",
      "4263/4263 [==============================] - 3s 802us/step - loss: 0.7457 - acc: 0.6775\n",
      "Epoch 86/160\n",
      "4263/4263 [==============================] - 3s 771us/step - loss: 0.7506 - acc: 0.6723\n",
      "Epoch 87/160\n",
      "4263/4263 [==============================] - 3s 782us/step - loss: 0.7399 - acc: 0.6819\n",
      "Epoch 88/160\n",
      "4263/4263 [==============================] - 3s 779us/step - loss: 0.7436 - acc: 0.6756\n",
      "Epoch 89/160\n",
      "4263/4263 [==============================] - 3s 677us/step - loss: 0.7429 - acc: 0.6779\n",
      "Epoch 90/160\n",
      "4263/4263 [==============================] - 3s 611us/step - loss: 0.7386 - acc: 0.6793\n",
      "Epoch 91/160\n",
      "4263/4263 [==============================] - 3s 612us/step - loss: 0.7385 - acc: 0.6800\n",
      "Epoch 92/160\n",
      "4263/4263 [==============================] - 3s 611us/step - loss: 0.7510 - acc: 0.6716\n",
      "Epoch 93/160\n",
      "4263/4263 [==============================] - 3s 611us/step - loss: 0.7363 - acc: 0.6800\n",
      "Epoch 94/160\n",
      "4263/4263 [==============================] - 3s 613us/step - loss: 0.7394 - acc: 0.6807\n",
      "Epoch 95/160\n",
      "4263/4263 [==============================] - 3s 611us/step - loss: 0.7462 - acc: 0.6777\n",
      "Epoch 96/160\n",
      "4263/4263 [==============================] - 3s 604us/step - loss: 0.7344 - acc: 0.6861\n",
      "Epoch 97/160\n",
      "4263/4263 [==============================] - 3s 611us/step - loss: 0.7406 - acc: 0.6768\n",
      "Epoch 98/160\n",
      "4263/4263 [==============================] - 3s 608us/step - loss: 0.7366 - acc: 0.6821\n",
      "Epoch 99/160\n",
      "4263/4263 [==============================] - 3s 607us/step - loss: 0.7328 - acc: 0.6843\n",
      "Epoch 100/160\n",
      "4263/4263 [==============================] - 3s 607us/step - loss: 0.7379 - acc: 0.6831\n",
      "Epoch 101/160\n",
      "4263/4263 [==============================] - 3s 611us/step - loss: 0.7338 - acc: 0.6861\n",
      "Epoch 102/160\n",
      "4263/4263 [==============================] - 3s 606us/step - loss: 0.7283 - acc: 0.6873\n",
      "Epoch 103/160\n",
      "4263/4263 [==============================] - 3s 609us/step - loss: 0.7287 - acc: 0.6873\n",
      "Epoch 104/160\n",
      "4263/4263 [==============================] - 3s 607us/step - loss: 0.7252 - acc: 0.6819\n",
      "Epoch 105/160\n",
      "4263/4263 [==============================] - 3s 609us/step - loss: 0.7297 - acc: 0.6775\n",
      "Epoch 106/160\n",
      "4263/4263 [==============================] - 3s 604us/step - loss: 0.7391 - acc: 0.6810\n",
      "Epoch 107/160\n",
      "4263/4263 [==============================] - 3s 611us/step - loss: 0.7388 - acc: 0.6775\n",
      "Epoch 108/160\n",
      "4263/4263 [==============================] - 3s 603us/step - loss: 0.7282 - acc: 0.6871\n",
      "Epoch 109/160\n",
      "4263/4263 [==============================] - 3s 609us/step - loss: 0.7314 - acc: 0.6866\n",
      "Epoch 110/160\n",
      "4263/4263 [==============================] - 3s 607us/step - loss: 0.7314 - acc: 0.6854\n",
      "Epoch 111/160\n",
      "4263/4263 [==============================] - 3s 611us/step - loss: 0.7270 - acc: 0.6852\n",
      "Epoch 112/160\n",
      "4263/4263 [==============================] - 3s 602us/step - loss: 0.7354 - acc: 0.6826\n",
      "Epoch 113/160\n",
      "4263/4263 [==============================] - 3s 612us/step - loss: 0.7305 - acc: 0.6897\n",
      "Epoch 114/160\n",
      "4263/4263 [==============================] - 3s 604us/step - loss: 0.7367 - acc: 0.6805\n",
      "Epoch 115/160\n",
      "4263/4263 [==============================] - 3s 611us/step - loss: 0.7295 - acc: 0.6885\n",
      "Epoch 116/160\n",
      "4263/4263 [==============================] - 3s 609us/step - loss: 0.7250 - acc: 0.6852\n",
      "Epoch 117/160\n",
      "4263/4263 [==============================] - 3s 609us/step - loss: 0.7204 - acc: 0.6958\n",
      "Epoch 118/160\n",
      "4263/4263 [==============================] - 3s 600us/step - loss: 0.7273 - acc: 0.6847\n",
      "Epoch 119/160\n",
      "4263/4263 [==============================] - 3s 606us/step - loss: 0.7370 - acc: 0.6812\n",
      "Epoch 120/160\n",
      "4263/4263 [==============================] - 3s 603us/step - loss: 0.7302 - acc: 0.6838\n",
      "Epoch 121/160\n",
      "4263/4263 [==============================] - 3s 609us/step - loss: 0.7237 - acc: 0.6843\n",
      "Epoch 122/160\n",
      "4263/4263 [==============================] - 3s 606us/step - loss: 0.7269 - acc: 0.6904\n",
      "Epoch 123/160\n",
      "4263/4263 [==============================] - 3s 607us/step - loss: 0.7188 - acc: 0.6908\n",
      "Epoch 124/160\n",
      "4263/4263 [==============================] - 3s 607us/step - loss: 0.7157 - acc: 0.6943\n",
      "Epoch 125/160\n",
      "4263/4263 [==============================] - 3s 602us/step - loss: 0.7207 - acc: 0.6913\n",
      "Epoch 126/160\n",
      "4263/4263 [==============================] - 3s 626us/step - loss: 0.7197 - acc: 0.6897\n",
      "Epoch 127/160\n",
      "4263/4263 [==============================] - 3s 595us/step - loss: 0.7299 - acc: 0.6920\n",
      "Epoch 128/160\n",
      "4263/4263 [==============================] - 3s 606us/step - loss: 0.7181 - acc: 0.6894\n",
      "Epoch 129/160\n",
      "4263/4263 [==============================] - 3s 605us/step - loss: 0.7165 - acc: 0.6913\n",
      "Epoch 130/160\n",
      "4263/4263 [==============================] - 3s 605us/step - loss: 0.7211 - acc: 0.6894\n",
      "Epoch 131/160\n",
      "4263/4263 [==============================] - 3s 606us/step - loss: 0.7202 - acc: 0.6847\n",
      "Epoch 132/160\n",
      "4263/4263 [==============================] - 3s 609us/step - loss: 0.7117 - acc: 0.6882\n",
      "Epoch 133/160\n",
      "4263/4263 [==============================] - 3s 597us/step - loss: 0.7243 - acc: 0.6864\n",
      "Epoch 134/160\n",
      "4263/4263 [==============================] - 3s 604us/step - loss: 0.7133 - acc: 0.6951\n",
      "Epoch 135/160\n",
      "4263/4263 [==============================] - 3s 600us/step - loss: 0.7138 - acc: 0.6925\n",
      "Epoch 136/160\n",
      "4263/4263 [==============================] - 3s 607us/step - loss: 0.7108 - acc: 0.6922\n",
      "Epoch 137/160\n",
      "4263/4263 [==============================] - 3s 615us/step - loss: 0.7166 - acc: 0.6965\n",
      "Epoch 138/160\n",
      "4263/4263 [==============================] - 3s 620us/step - loss: 0.7155 - acc: 0.6908\n",
      "Epoch 139/160\n",
      "4263/4263 [==============================] - 3s 602us/step - loss: 0.7232 - acc: 0.6824\n",
      "Epoch 140/160\n",
      "4263/4263 [==============================] - 3s 607us/step - loss: 0.7087 - acc: 0.6969\n",
      "Epoch 141/160\n",
      "4263/4263 [==============================] - 3s 600us/step - loss: 0.7222 - acc: 0.6936\n",
      "Epoch 142/160\n",
      "4263/4263 [==============================] - 3s 609us/step - loss: 0.7168 - acc: 0.6897\n",
      "Epoch 143/160\n",
      "4263/4263 [==============================] - 3s 603us/step - loss: 0.7122 - acc: 0.6936\n",
      "Epoch 144/160\n",
      "4263/4263 [==============================] - 3s 607us/step - loss: 0.7125 - acc: 0.6878\n",
      "Epoch 145/160\n",
      "4263/4263 [==============================] - 3s 599us/step - loss: 0.7237 - acc: 0.6885\n",
      "Epoch 146/160\n",
      "4263/4263 [==============================] - 3s 606us/step - loss: 0.7170 - acc: 0.6897\n",
      "Epoch 147/160\n",
      "4263/4263 [==============================] - 3s 601us/step - loss: 0.7076 - acc: 0.6915\n",
      "Epoch 148/160\n",
      "4263/4263 [==============================] - 3s 604us/step - loss: 0.7183 - acc: 0.6854\n",
      "Epoch 149/160\n",
      "4263/4263 [==============================] - 3s 603us/step - loss: 0.7069 - acc: 0.6927\n",
      "Epoch 150/160\n",
      "4263/4263 [==============================] - 3s 651us/step - loss: 0.7060 - acc: 0.6955\n",
      "Epoch 151/160\n",
      "4263/4263 [==============================] - 3s 704us/step - loss: 0.7063 - acc: 0.6920\n",
      "Epoch 152/160\n",
      "4263/4263 [==============================] - 3s 662us/step - loss: 0.7159 - acc: 0.6866\n",
      "Epoch 153/160\n",
      "4263/4263 [==============================] - 3s 660us/step - loss: 0.7133 - acc: 0.6948\n",
      "Epoch 154/160\n",
      "4263/4263 [==============================] - 3s 637us/step - loss: 0.7073 - acc: 0.6906\n",
      "Epoch 155/160\n",
      "4263/4263 [==============================] - 3s 642us/step - loss: 0.7055 - acc: 0.6929\n",
      "Epoch 156/160\n",
      "4263/4263 [==============================] - 3s 636us/step - loss: 0.7016 - acc: 0.6929\n",
      "Epoch 157/160\n",
      "4263/4263 [==============================] - 3s 640us/step - loss: 0.7027 - acc: 0.6951\n",
      "Epoch 158/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 3s 630us/step - loss: 0.7041 - acc: 0.7019\n",
      "Epoch 159/160\n",
      "4263/4263 [==============================] - 3s 628us/step - loss: 0.6997 - acc: 0.7000\n",
      "Epoch 160/160\n",
      "4263/4263 [==============================] - 3s 635us/step - loss: 0.6999 - acc: 0.6983\n",
      "1066/1066 [==============================] - 1s 1ms/step\n",
      "4263/4263 [==============================] - 1s 157us/step\n",
      "\n",
      "acc: 67.07%\n",
      "\n",
      "acc: 70.61%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df['Class'].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d,y_d,test_size=0.2,random_state=42)\n",
    "\n",
    "y_train_2=pd.get_dummies(y_train)\n",
    "y_test_2=pd.get_dummies(y_test)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(650, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=160, callbacks=[early_stopping_monitor],batch_size=50)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "scores_2 = model.evaluate(X_train, y_train_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_2[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/160\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 1.0321 - acc: 0.5487\n",
      "Epoch 2/160\n",
      "4263/4263 [==============================] - 3s 643us/step - loss: 0.9226 - acc: 0.5996\n",
      "Epoch 3/160\n",
      "4263/4263 [==============================] - 3s 638us/step - loss: 0.8805 - acc: 0.6190\n",
      "Epoch 4/160\n",
      "4263/4263 [==============================] - 3s 645us/step - loss: 0.8600 - acc: 0.6244\n",
      "Epoch 5/160\n",
      "4263/4263 [==============================] - 3s 635us/step - loss: 0.8483 - acc: 0.6310\n",
      "Epoch 6/160\n",
      "4263/4263 [==============================] - 3s 648us/step - loss: 0.8548 - acc: 0.6291\n",
      "Epoch 7/160\n",
      "4263/4263 [==============================] - 3s 637us/step - loss: 0.8439 - acc: 0.6390\n",
      "Epoch 8/160\n",
      "4263/4263 [==============================] - 3s 641us/step - loss: 0.8424 - acc: 0.6319\n",
      "Epoch 9/160\n",
      "4263/4263 [==============================] - 3s 637us/step - loss: 0.8427 - acc: 0.6376\n",
      "Epoch 10/160\n",
      "4263/4263 [==============================] - 3s 642us/step - loss: 0.8375 - acc: 0.6383\n",
      "Epoch 11/160\n",
      "4263/4263 [==============================] - 3s 640us/step - loss: 0.8393 - acc: 0.6423\n",
      "Epoch 12/160\n",
      "4263/4263 [==============================] - 3s 636us/step - loss: 0.8256 - acc: 0.6449\n",
      "Epoch 13/160\n",
      "4263/4263 [==============================] - 3s 642us/step - loss: 0.8237 - acc: 0.6411\n",
      "Epoch 14/160\n",
      "4263/4263 [==============================] - 3s 635us/step - loss: 0.8238 - acc: 0.6425\n",
      "Epoch 15/160\n",
      "4263/4263 [==============================] - 3s 647us/step - loss: 0.8218 - acc: 0.6467\n",
      "Epoch 16/160\n",
      "4263/4263 [==============================] - 3s 643us/step - loss: 0.8208 - acc: 0.6453\n",
      "Epoch 17/160\n",
      "4263/4263 [==============================] - 3s 644us/step - loss: 0.8096 - acc: 0.6531\n",
      "Epoch 18/160\n",
      "4263/4263 [==============================] - 3s 633us/step - loss: 0.7991 - acc: 0.6512\n",
      "Epoch 19/160\n",
      "4263/4263 [==============================] - 3s 643us/step - loss: 0.8024 - acc: 0.6552\n",
      "Epoch 20/160\n",
      "4263/4263 [==============================] - 3s 637us/step - loss: 0.7898 - acc: 0.6641\n",
      "Epoch 21/160\n",
      "4263/4263 [==============================] - 3s 640us/step - loss: 0.8079 - acc: 0.6512\n",
      "Epoch 22/160\n",
      "4263/4263 [==============================] - 3s 640us/step - loss: 0.7886 - acc: 0.6547\n",
      "Epoch 23/160\n",
      "4263/4263 [==============================] - 3s 636us/step - loss: 0.7846 - acc: 0.6554\n",
      "Epoch 24/160\n",
      "4263/4263 [==============================] - 3s 640us/step - loss: 0.7874 - acc: 0.6641\n",
      "Epoch 25/160\n",
      "4263/4263 [==============================] - 3s 643us/step - loss: 0.7928 - acc: 0.6608\n",
      "Epoch 26/160\n",
      "4263/4263 [==============================] - 3s 640us/step - loss: 0.7909 - acc: 0.6547\n",
      "Epoch 27/160\n",
      "4263/4263 [==============================] - 3s 633us/step - loss: 0.7820 - acc: 0.6608\n",
      "Epoch 28/160\n",
      "4263/4263 [==============================] - 3s 641us/step - loss: 0.7829 - acc: 0.6592\n",
      "Epoch 29/160\n",
      "4263/4263 [==============================] - 3s 634us/step - loss: 0.7876 - acc: 0.6648\n",
      "Epoch 30/160\n",
      "4263/4263 [==============================] - 3s 644us/step - loss: 0.7776 - acc: 0.6594\n",
      "Epoch 31/160\n",
      "4263/4263 [==============================] - 3s 635us/step - loss: 0.7760 - acc: 0.6531\n",
      "Epoch 32/160\n",
      "4263/4263 [==============================] - 3s 638us/step - loss: 0.7783 - acc: 0.6587\n",
      "Epoch 33/160\n",
      "4263/4263 [==============================] - 3s 638us/step - loss: 0.7872 - acc: 0.6629\n",
      "Epoch 34/160\n",
      "4263/4263 [==============================] - 3s 638us/step - loss: 0.7651 - acc: 0.6683\n",
      "Epoch 35/160\n",
      "4263/4263 [==============================] - 3s 639us/step - loss: 0.7726 - acc: 0.6585\n",
      "Epoch 36/160\n",
      "4263/4263 [==============================] - 3s 637us/step - loss: 0.7776 - acc: 0.6629\n",
      "Epoch 37/160\n",
      "4263/4263 [==============================] - 3s 637us/step - loss: 0.7729 - acc: 0.6617\n",
      "Epoch 38/160\n",
      "4263/4263 [==============================] - 3s 632us/step - loss: 0.7731 - acc: 0.6681\n",
      "Epoch 39/160\n",
      "4263/4263 [==============================] - 3s 638us/step - loss: 0.7745 - acc: 0.6594\n",
      "Epoch 40/160\n",
      "4263/4263 [==============================] - 3s 633us/step - loss: 0.7697 - acc: 0.6634\n",
      "Epoch 41/160\n",
      "4263/4263 [==============================] - 3s 640us/step - loss: 0.7651 - acc: 0.6681\n",
      "Epoch 42/160\n",
      "4263/4263 [==============================] - 3s 635us/step - loss: 0.7667 - acc: 0.6657\n",
      "Epoch 43/160\n",
      "4263/4263 [==============================] - 3s 637us/step - loss: 0.7660 - acc: 0.6690\n",
      "Epoch 44/160\n",
      "4263/4263 [==============================] - 3s 638us/step - loss: 0.7653 - acc: 0.6639\n",
      "Epoch 45/160\n",
      "4263/4263 [==============================] - 3s 635us/step - loss: 0.7722 - acc: 0.6655\n",
      "Epoch 46/160\n",
      "4263/4263 [==============================] - 3s 639us/step - loss: 0.7685 - acc: 0.6608\n",
      "Epoch 47/160\n",
      "4263/4263 [==============================] - 3s 633us/step - loss: 0.7632 - acc: 0.6674\n",
      "Epoch 48/160\n",
      "4263/4263 [==============================] - 3s 638us/step - loss: 0.7647 - acc: 0.6671\n",
      "Epoch 49/160\n",
      "4263/4263 [==============================] - 3s 632us/step - loss: 0.7628 - acc: 0.6683\n",
      "Epoch 50/160\n",
      "4263/4263 [==============================] - 3s 641us/step - loss: 0.7618 - acc: 0.6674\n",
      "Epoch 51/160\n",
      "4263/4263 [==============================] - 3s 634us/step - loss: 0.7626 - acc: 0.6624\n",
      "Epoch 52/160\n",
      "4263/4263 [==============================] - 3s 643us/step - loss: 0.7592 - acc: 0.6725\n",
      "Epoch 53/160\n",
      "4263/4263 [==============================] - 3s 633us/step - loss: 0.7675 - acc: 0.6669\n",
      "Epoch 54/160\n",
      "4263/4263 [==============================] - 3s 635us/step - loss: 0.7533 - acc: 0.6688\n",
      "Epoch 55/160\n",
      "4263/4263 [==============================] - 3s 632us/step - loss: 0.7564 - acc: 0.6716\n",
      "Epoch 56/160\n",
      "4263/4263 [==============================] - 3s 634us/step - loss: 0.7692 - acc: 0.6634\n",
      "Epoch 57/160\n",
      "4263/4263 [==============================] - 3s 636us/step - loss: 0.7733 - acc: 0.6671\n",
      "Epoch 58/160\n",
      "4263/4263 [==============================] - 3s 632us/step - loss: 0.7589 - acc: 0.6723\n",
      "Epoch 59/160\n",
      "4263/4263 [==============================] - 3s 630us/step - loss: 0.7519 - acc: 0.6685\n",
      "Epoch 60/160\n",
      "4263/4263 [==============================] - 3s 612us/step - loss: 0.7590 - acc: 0.6732\n",
      "Epoch 61/160\n",
      "4263/4263 [==============================] - 3s 635us/step - loss: 0.7487 - acc: 0.6711\n",
      "Epoch 62/160\n",
      "4263/4263 [==============================] - 3s 628us/step - loss: 0.7652 - acc: 0.6735\n",
      "Epoch 63/160\n",
      "4263/4263 [==============================] - 3s 633us/step - loss: 0.7612 - acc: 0.6678\n",
      "Epoch 64/160\n",
      "4263/4263 [==============================] - 3s 627us/step - loss: 0.7552 - acc: 0.6704\n",
      "Epoch 65/160\n",
      "4263/4263 [==============================] - 3s 628us/step - loss: 0.7637 - acc: 0.6660\n",
      "Epoch 66/160\n",
      "4263/4263 [==============================] - 3s 633us/step - loss: 0.7580 - acc: 0.6723\n",
      "Epoch 67/160\n",
      "4263/4263 [==============================] - 3s 634us/step - loss: 0.7622 - acc: 0.6669\n",
      "Epoch 68/160\n",
      "4263/4263 [==============================] - 3s 628us/step - loss: 0.7509 - acc: 0.6662\n",
      "Epoch 69/160\n",
      "4263/4263 [==============================] - 3s 634us/step - loss: 0.7566 - acc: 0.6714\n",
      "Epoch 70/160\n",
      "4263/4263 [==============================] - 3s 633us/step - loss: 0.7530 - acc: 0.6700\n",
      "Epoch 71/160\n",
      "4263/4263 [==============================] - 3s 629us/step - loss: 0.7488 - acc: 0.6756\n",
      "Epoch 72/160\n",
      "4263/4263 [==============================] - 3s 656us/step - loss: 0.7567 - acc: 0.6742\n",
      "Epoch 73/160\n",
      "4263/4263 [==============================] - 3s 632us/step - loss: 0.7455 - acc: 0.6760\n",
      "Epoch 74/160\n",
      "4263/4263 [==============================] - 3s 632us/step - loss: 0.7483 - acc: 0.6753\n",
      "Epoch 75/160\n",
      "4263/4263 [==============================] - 3s 627us/step - loss: 0.7466 - acc: 0.6751\n",
      "Epoch 76/160\n",
      "4263/4263 [==============================] - 3s 635us/step - loss: 0.7510 - acc: 0.6798\n",
      "Epoch 77/160\n",
      "4263/4263 [==============================] - 3s 627us/step - loss: 0.7517 - acc: 0.6714\n",
      "Epoch 78/160\n",
      "4263/4263 [==============================] - 3s 631us/step - loss: 0.7479 - acc: 0.6798\n",
      "Epoch 79/160\n",
      "4263/4263 [==============================] - 3s 629us/step - loss: 0.7509 - acc: 0.6784\n",
      "Epoch 80/160\n",
      "4263/4263 [==============================] - 3s 631us/step - loss: 0.7498 - acc: 0.6742\n",
      "Epoch 81/160\n",
      "4263/4263 [==============================] - 3s 628us/step - loss: 0.7553 - acc: 0.6730\n",
      "Epoch 82/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 3s 628us/step - loss: 0.7566 - acc: 0.6728\n",
      "Epoch 83/160\n",
      "4263/4263 [==============================] - 3s 633us/step - loss: 0.7476 - acc: 0.6810\n",
      "Epoch 84/160\n",
      "4263/4263 [==============================] - 3s 625us/step - loss: 0.7438 - acc: 0.6819\n",
      "Epoch 85/160\n",
      "4263/4263 [==============================] - 3s 633us/step - loss: 0.7728 - acc: 0.6601\n",
      "Epoch 86/160\n",
      "4263/4263 [==============================] - 3s 628us/step - loss: 0.7507 - acc: 0.6796\n",
      "Epoch 87/160\n",
      "4263/4263 [==============================] - 3s 631us/step - loss: 0.7467 - acc: 0.6810\n",
      "Epoch 88/160\n",
      "4263/4263 [==============================] - 3s 627us/step - loss: 0.7437 - acc: 0.6735\n",
      "Epoch 89/160\n",
      "4263/4263 [==============================] - 3s 633us/step - loss: 0.7415 - acc: 0.6833\n",
      "Epoch 90/160\n",
      "4263/4263 [==============================] - 3s 626us/step - loss: 0.7373 - acc: 0.6798\n",
      "Epoch 91/160\n",
      "4263/4263 [==============================] - 3s 637us/step - loss: 0.7451 - acc: 0.6805\n",
      "Epoch 92/160\n",
      "4263/4263 [==============================] - 3s 629us/step - loss: 0.7542 - acc: 0.6760\n",
      "Epoch 93/160\n",
      "4263/4263 [==============================] - 3s 632us/step - loss: 0.7441 - acc: 0.6796\n",
      "Epoch 94/160\n",
      "4263/4263 [==============================] - 3s 629us/step - loss: 0.7457 - acc: 0.6735\n",
      "Epoch 95/160\n",
      "4263/4263 [==============================] - 3s 629us/step - loss: 0.7415 - acc: 0.6845\n",
      "Epoch 96/160\n",
      "4263/4263 [==============================] - 3s 630us/step - loss: 0.7450 - acc: 0.6775\n",
      "Epoch 97/160\n",
      "4263/4263 [==============================] - 3s 630us/step - loss: 0.7346 - acc: 0.6791\n",
      "Epoch 98/160\n",
      "4263/4263 [==============================] - 3s 633us/step - loss: 0.7486 - acc: 0.6793\n",
      "Epoch 99/160\n",
      "4263/4263 [==============================] - 3s 629us/step - loss: 0.7507 - acc: 0.6641\n",
      "Epoch 100/160\n",
      "4263/4263 [==============================] - 3s 635us/step - loss: 0.7487 - acc: 0.6739\n",
      "Epoch 101/160\n",
      "4263/4263 [==============================] - 3s 625us/step - loss: 0.7426 - acc: 0.6756\n",
      "Epoch 102/160\n",
      "4263/4263 [==============================] - 3s 635us/step - loss: 0.7411 - acc: 0.6819\n",
      "Epoch 103/160\n",
      "4263/4263 [==============================] - 3s 627us/step - loss: 0.7372 - acc: 0.6765\n",
      "Epoch 104/160\n",
      "4263/4263 [==============================] - 3s 609us/step - loss: 0.7361 - acc: 0.6871\n",
      "Epoch 105/160\n",
      "4263/4263 [==============================] - 3s 635us/step - loss: 0.7328 - acc: 0.6829\n",
      "Epoch 106/160\n",
      "4263/4263 [==============================] - 3s 631us/step - loss: 0.7381 - acc: 0.6768\n",
      "Epoch 107/160\n",
      "4263/4263 [==============================] - 3s 626us/step - loss: 0.7335 - acc: 0.6852\n",
      "Epoch 108/160\n",
      "4263/4263 [==============================] - 3s 629us/step - loss: 0.7340 - acc: 0.6868\n",
      "Epoch 109/160\n",
      "4263/4263 [==============================] - 3s 628us/step - loss: 0.7319 - acc: 0.6824\n",
      "Epoch 110/160\n",
      "4263/4263 [==============================] - 3s 627us/step - loss: 0.7365 - acc: 0.6845\n",
      "Epoch 111/160\n",
      "4263/4263 [==============================] - 3s 670us/step - loss: 0.7277 - acc: 0.6843\n",
      "Epoch 112/160\n",
      "4263/4263 [==============================] - 3s 724us/step - loss: 0.7280 - acc: 0.6840\n",
      "Epoch 113/160\n",
      "4263/4263 [==============================] - 3s 702us/step - loss: 0.7307 - acc: 0.6843\n",
      "Epoch 114/160\n",
      "4263/4263 [==============================] - 3s 662us/step - loss: 0.7318 - acc: 0.6829\n",
      "Epoch 115/160\n",
      "4263/4263 [==============================] - 3s 590us/step - loss: 0.7229 - acc: 0.6854\n",
      "Epoch 116/160\n",
      "4263/4263 [==============================] - 2s 561us/step - loss: 0.7352 - acc: 0.6768\n",
      "Epoch 117/160\n",
      "4263/4263 [==============================] - 2s 559us/step - loss: 0.7290 - acc: 0.6833\n",
      "Epoch 118/160\n",
      "4263/4263 [==============================] - 2s 554us/step - loss: 0.7310 - acc: 0.6845\n",
      "Epoch 119/160\n",
      "4263/4263 [==============================] - 3s 591us/step - loss: 0.7278 - acc: 0.6814\n",
      "Epoch 120/160\n",
      "4263/4263 [==============================] - 3s 636us/step - loss: 0.7354 - acc: 0.6875\n",
      "Epoch 121/160\n",
      "4263/4263 [==============================] - 3s 649us/step - loss: 0.7288 - acc: 0.6894\n",
      "Epoch 122/160\n",
      "4263/4263 [==============================] - 3s 674us/step - loss: 0.7345 - acc: 0.6840\n",
      "Epoch 123/160\n",
      "4263/4263 [==============================] - 3s 674us/step - loss: 0.7333 - acc: 0.6805\n",
      "Epoch 124/160\n",
      "4263/4263 [==============================] - 3s 667us/step - loss: 0.7360 - acc: 0.6838\n",
      "Epoch 125/160\n",
      "4263/4263 [==============================] - 3s 642us/step - loss: 0.7240 - acc: 0.6836\n",
      "Epoch 126/160\n",
      "4263/4263 [==============================] - 3s 651us/step - loss: 0.7340 - acc: 0.6805\n",
      "Epoch 127/160\n",
      "4263/4263 [==============================] - 3s 638us/step - loss: 0.7228 - acc: 0.6845\n",
      "Epoch 128/160\n",
      "4263/4263 [==============================] - 3s 650us/step - loss: 0.7260 - acc: 0.6897\n",
      "Epoch 129/160\n",
      "4263/4263 [==============================] - 3s 649us/step - loss: 0.7314 - acc: 0.6784\n",
      "Epoch 130/160\n",
      "4263/4263 [==============================] - 3s 654us/step - loss: 0.7261 - acc: 0.6810\n",
      "Epoch 131/160\n",
      "4263/4263 [==============================] - 3s 648us/step - loss: 0.7261 - acc: 0.6873\n",
      "Epoch 132/160\n",
      "4263/4263 [==============================] - 3s 645us/step - loss: 0.7211 - acc: 0.6897\n",
      "Epoch 133/160\n",
      "4263/4263 [==============================] - 3s 657us/step - loss: 0.7286 - acc: 0.6847\n",
      "Epoch 134/160\n",
      "4263/4263 [==============================] - 3s 653us/step - loss: 0.7213 - acc: 0.6939\n",
      "Epoch 135/160\n",
      "4263/4263 [==============================] - 3s 729us/step - loss: 0.7190 - acc: 0.6899\n",
      "Epoch 136/160\n",
      "4263/4263 [==============================] - 3s 730us/step - loss: 0.7166 - acc: 0.6958\n",
      "Epoch 137/160\n",
      "4263/4263 [==============================] - 3s 645us/step - loss: 0.7233 - acc: 0.6941\n",
      "Epoch 138/160\n",
      "4263/4263 [==============================] - 3s 641us/step - loss: 0.7127 - acc: 0.6894\n",
      "Epoch 139/160\n",
      "4263/4263 [==============================] - 3s 646us/step - loss: 0.7188 - acc: 0.6864\n",
      "Epoch 140/160\n",
      "4263/4263 [==============================] - 3s 642us/step - loss: 0.7186 - acc: 0.6829\n",
      "Epoch 141/160\n",
      "4263/4263 [==============================] - 3s 637us/step - loss: 0.7398 - acc: 0.6812\n",
      "Epoch 142/160\n",
      "4263/4263 [==============================] - 3s 642us/step - loss: 0.7234 - acc: 0.6894\n",
      "Epoch 143/160\n",
      "4263/4263 [==============================] - 3s 641us/step - loss: 0.7254 - acc: 0.6892\n",
      "Epoch 144/160\n",
      "4263/4263 [==============================] - 3s 640us/step - loss: 0.7190 - acc: 0.6906\n",
      "Epoch 145/160\n",
      "4263/4263 [==============================] - 3s 634us/step - loss: 0.7212 - acc: 0.6871\n",
      "Epoch 146/160\n",
      "4263/4263 [==============================] - 3s 640us/step - loss: 0.7186 - acc: 0.6913\n",
      "Epoch 147/160\n",
      "4263/4263 [==============================] - 3s 637us/step - loss: 0.7237 - acc: 0.6897\n",
      "Epoch 148/160\n",
      "4263/4263 [==============================] - 3s 641us/step - loss: 0.7264 - acc: 0.6854\n",
      "Epoch 149/160\n",
      "4263/4263 [==============================] - 3s 644us/step - loss: 0.7229 - acc: 0.6904\n",
      "Epoch 150/160\n",
      "4263/4263 [==============================] - 3s 640us/step - loss: 0.7094 - acc: 0.6962\n",
      "Epoch 151/160\n",
      "4263/4263 [==============================] - 3s 639us/step - loss: 0.7096 - acc: 0.6894\n",
      "Epoch 152/160\n",
      "4263/4263 [==============================] - 3s 636us/step - loss: 0.7091 - acc: 0.6913\n",
      "Epoch 153/160\n",
      "4263/4263 [==============================] - 3s 642us/step - loss: 0.7272 - acc: 0.6819\n",
      "Epoch 154/160\n",
      "4263/4263 [==============================] - 3s 631us/step - loss: 0.7157 - acc: 0.6922\n",
      "Epoch 155/160\n",
      "4263/4263 [==============================] - 3s 639us/step - loss: 0.7082 - acc: 0.6951\n",
      "Epoch 156/160\n",
      "4263/4263 [==============================] - 3s 637us/step - loss: 0.7065 - acc: 0.6967\n",
      "Epoch 157/160\n",
      "4263/4263 [==============================] - 3s 641us/step - loss: 0.7064 - acc: 0.6927\n",
      "Epoch 158/160\n",
      "4263/4263 [==============================] - 3s 635us/step - loss: 0.7097 - acc: 0.7000\n",
      "Epoch 159/160\n",
      "4263/4263 [==============================] - 3s 641us/step - loss: 0.7009 - acc: 0.6979\n",
      "Epoch 160/160\n",
      "4263/4263 [==============================] - 3s 637us/step - loss: 0.7080 - acc: 0.6939\n",
      "1066/1066 [==============================] - 1s 1ms/step\n",
      "4263/4263 [==============================] - 1s 197us/step\n",
      "\n",
      "acc: 66.60%\n",
      "\n",
      "acc: 70.11%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df['Class'].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d,y_d,test_size=0.2,random_state=42)\n",
    "\n",
    "y_train_2=pd.get_dummies(y_train)\n",
    "y_test_2=pd.get_dummies(y_test)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(650, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=160, callbacks=[early_stopping_monitor],batch_size=50)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "scores_2 = model.evaluate(X_train, y_train_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_2[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/160\n",
      "4263/4263 [==============================] - 8s 2ms/step - loss: 1.0657 - acc: 0.5616\n",
      "Epoch 2/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.9089 - acc: 0.5968\n",
      "Epoch 3/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.9046 - acc: 0.5954\n",
      "Epoch 4/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8840 - acc: 0.6169\n",
      "Epoch 5/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8727 - acc: 0.6111\n",
      "Epoch 6/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8691 - acc: 0.6263\n",
      "Epoch 7/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8609 - acc: 0.6270\n",
      "Epoch 8/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8490 - acc: 0.6383\n",
      "Epoch 9/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8537 - acc: 0.6319\n",
      "Epoch 10/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8443 - acc: 0.6334\n",
      "Epoch 11/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8356 - acc: 0.6373\n",
      "Epoch 12/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8310 - acc: 0.6416\n",
      "Epoch 13/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8245 - acc: 0.6402\n",
      "Epoch 14/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8267 - acc: 0.6498\n",
      "Epoch 15/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8230 - acc: 0.6427\n",
      "Epoch 16/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8213 - acc: 0.6449\n",
      "Epoch 17/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8203 - acc: 0.6449\n",
      "Epoch 18/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8179 - acc: 0.6526\n",
      "Epoch 19/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8085 - acc: 0.6542\n",
      "Epoch 20/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8090 - acc: 0.6542\n",
      "Epoch 21/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7953 - acc: 0.6643\n",
      "Epoch 22/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8015 - acc: 0.6531\n",
      "Epoch 23/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7965 - acc: 0.6582\n",
      "Epoch 24/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7968 - acc: 0.6554\n",
      "Epoch 25/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7916 - acc: 0.6592\n",
      "Epoch 26/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7977 - acc: 0.6545\n",
      "Epoch 27/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7973 - acc: 0.6526\n",
      "Epoch 28/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7977 - acc: 0.6573\n",
      "Epoch 29/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7962 - acc: 0.6528\n",
      "Epoch 30/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7932 - acc: 0.6545\n",
      "Epoch 31/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7866 - acc: 0.6601\n",
      "Epoch 32/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7777 - acc: 0.6636\n",
      "Epoch 33/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7855 - acc: 0.6646\n",
      "Epoch 34/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7787 - acc: 0.6610\n",
      "Epoch 35/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7903 - acc: 0.6608\n",
      "Epoch 36/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7795 - acc: 0.6589\n",
      "Epoch 37/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7757 - acc: 0.6662\n",
      "Epoch 38/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7816 - acc: 0.6570\n",
      "Epoch 39/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7950 - acc: 0.6610\n",
      "Epoch 40/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7746 - acc: 0.6648\n",
      "Epoch 41/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7752 - acc: 0.6646\n",
      "Epoch 42/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7777 - acc: 0.6631\n",
      "Epoch 43/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7799 - acc: 0.6657\n",
      "Epoch 44/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7818 - acc: 0.6624\n",
      "Epoch 45/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7787 - acc: 0.6606\n",
      "Epoch 46/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7716 - acc: 0.6554\n",
      "Epoch 47/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7803 - acc: 0.6589\n",
      "Epoch 48/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7739 - acc: 0.6561\n",
      "Epoch 49/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7720 - acc: 0.6601\n",
      "Epoch 50/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7600 - acc: 0.6676\n",
      "Epoch 51/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7667 - acc: 0.6692\n",
      "Epoch 52/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7681 - acc: 0.6629\n",
      "Epoch 53/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7661 - acc: 0.6608\n",
      "Epoch 54/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7570 - acc: 0.6660\n",
      "Epoch 55/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7614 - acc: 0.6629\n",
      "Epoch 56/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7663 - acc: 0.6589\n",
      "Epoch 57/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7709 - acc: 0.6728\n",
      "Epoch 58/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7683 - acc: 0.6639\n",
      "Epoch 59/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7667 - acc: 0.6664\n",
      "Epoch 60/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7654 - acc: 0.6667\n",
      "Epoch 61/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7651 - acc: 0.6629\n",
      "Epoch 62/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7678 - acc: 0.6690\n",
      "Epoch 63/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7610 - acc: 0.6667\n",
      "Epoch 64/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7539 - acc: 0.6685\n",
      "Epoch 65/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7594 - acc: 0.6732\n",
      "Epoch 66/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7572 - acc: 0.6721\n",
      "Epoch 67/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7555 - acc: 0.6716\n",
      "Epoch 68/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7650 - acc: 0.6664\n",
      "Epoch 69/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7592 - acc: 0.6721\n",
      "Epoch 70/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7565 - acc: 0.6753\n",
      "Epoch 71/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7576 - acc: 0.6704\n",
      "Epoch 72/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7617 - acc: 0.6707\n",
      "Epoch 73/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7537 - acc: 0.6753\n",
      "Epoch 74/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7599 - acc: 0.6697\n",
      "Epoch 75/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7545 - acc: 0.6702\n",
      "Epoch 76/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7579 - acc: 0.6704\n",
      "Epoch 77/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7519 - acc: 0.6716\n",
      "Epoch 78/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7547 - acc: 0.6721\n",
      "Epoch 79/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7525 - acc: 0.6700\n",
      "Epoch 80/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7462 - acc: 0.6751\n",
      "Epoch 81/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7514 - acc: 0.6803\n",
      "Epoch 82/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7430 - acc: 0.6742\n",
      "Epoch 83/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7562 - acc: 0.6709\n",
      "Epoch 84/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7595 - acc: 0.6653\n",
      "Epoch 85/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7424 - acc: 0.6803\n",
      "Epoch 86/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7594 - acc: 0.6671\n",
      "Epoch 87/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7522 - acc: 0.6760\n",
      "Epoch 88/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7480 - acc: 0.6756\n",
      "Epoch 89/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7502 - acc: 0.6753\n",
      "Epoch 90/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7457 - acc: 0.6763\n",
      "Epoch 91/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7423 - acc: 0.6714\n",
      "Epoch 92/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7440 - acc: 0.6829\n",
      "Epoch 93/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7468 - acc: 0.6805\n",
      "Epoch 94/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7595 - acc: 0.6751\n",
      "Epoch 95/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7498 - acc: 0.6763\n",
      "Epoch 96/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7407 - acc: 0.6763\n",
      "Epoch 97/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7398 - acc: 0.6775\n",
      "Epoch 98/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7535 - acc: 0.6678\n",
      "Epoch 99/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7503 - acc: 0.6746\n",
      "Epoch 100/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7403 - acc: 0.6763\n",
      "Epoch 101/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7425 - acc: 0.6779\n",
      "Epoch 102/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7534 - acc: 0.6723\n",
      "Epoch 103/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7462 - acc: 0.6730\n",
      "Epoch 104/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7495 - acc: 0.6763\n",
      "Epoch 105/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7425 - acc: 0.6803\n",
      "Epoch 106/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7371 - acc: 0.6826\n",
      "Epoch 107/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7393 - acc: 0.6789\n",
      "Epoch 108/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7387 - acc: 0.6807\n",
      "Epoch 109/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7439 - acc: 0.6800\n",
      "Epoch 110/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7392 - acc: 0.6786\n",
      "Epoch 111/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7608 - acc: 0.6744\n",
      "Epoch 112/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7515 - acc: 0.6749\n",
      "Epoch 113/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7440 - acc: 0.6782\n",
      "Epoch 114/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7564 - acc: 0.6716\n",
      "Epoch 115/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7469 - acc: 0.6765\n",
      "Epoch 116/160\n",
      "4263/4263 [==============================] - 4s 985us/step - loss: 0.7397 - acc: 0.6831\n",
      "Epoch 117/160\n",
      "4263/4263 [==============================] - 4s 977us/step - loss: 0.7320 - acc: 0.6852\n",
      "Epoch 118/160\n",
      "4263/4263 [==============================] - 4s 984us/step - loss: 0.7305 - acc: 0.6836\n",
      "Epoch 119/160\n",
      "4263/4263 [==============================] - 4s 976us/step - loss: 0.7431 - acc: 0.6833\n",
      "Epoch 120/160\n",
      "4263/4263 [==============================] - 4s 981us/step - loss: 0.7377 - acc: 0.6821\n",
      "Epoch 121/160\n",
      "4263/4263 [==============================] - 4s 981us/step - loss: 0.7296 - acc: 0.6871\n",
      "Epoch 122/160\n",
      "4263/4263 [==============================] - 4s 973us/step - loss: 0.7410 - acc: 0.6850\n",
      "Epoch 123/160\n",
      "4263/4263 [==============================] - 4s 973us/step - loss: 0.7363 - acc: 0.6817\n",
      "Epoch 124/160\n",
      "4263/4263 [==============================] - 4s 977us/step - loss: 0.7398 - acc: 0.6847\n",
      "Epoch 125/160\n",
      "4263/4263 [==============================] - 4s 975us/step - loss: 0.7370 - acc: 0.6824\n",
      "Epoch 126/160\n",
      "4263/4263 [==============================] - 4s 974us/step - loss: 0.7283 - acc: 0.6854\n",
      "Epoch 127/160\n",
      "4263/4263 [==============================] - 4s 980us/step - loss: 0.7317 - acc: 0.6798\n",
      "Epoch 128/160\n",
      "4263/4263 [==============================] - 4s 969us/step - loss: 0.7475 - acc: 0.6760\n",
      "Epoch 129/160\n",
      "4263/4263 [==============================] - 4s 977us/step - loss: 0.7441 - acc: 0.6723\n",
      "Epoch 130/160\n",
      "4263/4263 [==============================] - 4s 976us/step - loss: 0.7266 - acc: 0.6805\n",
      "Epoch 131/160\n",
      "4263/4263 [==============================] - 4s 979us/step - loss: 0.7359 - acc: 0.6800\n",
      "Epoch 132/160\n",
      "4263/4263 [==============================] - 4s 975us/step - loss: 0.7343 - acc: 0.6824\n",
      "Epoch 133/160\n",
      "4263/4263 [==============================] - 4s 973us/step - loss: 0.7316 - acc: 0.6817\n",
      "Epoch 134/160\n",
      "4263/4263 [==============================] - 4s 973us/step - loss: 0.7307 - acc: 0.6833\n",
      "Epoch 135/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7262 - acc: 0.6833\n",
      "Epoch 136/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7368 - acc: 0.6824\n",
      "Epoch 137/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7277 - acc: 0.6831\n",
      "Epoch 138/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7221 - acc: 0.6904\n",
      "Epoch 139/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7258 - acc: 0.6843\n",
      "Epoch 140/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7275 - acc: 0.6894\n",
      "Epoch 141/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7231 - acc: 0.6897\n",
      "Epoch 142/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7263 - acc: 0.6873\n",
      "Epoch 143/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7219 - acc: 0.6904\n",
      "Epoch 144/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7444 - acc: 0.6782\n",
      "Epoch 145/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7346 - acc: 0.6878\n",
      "Epoch 146/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7295 - acc: 0.6882\n",
      "Epoch 147/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7280 - acc: 0.6838\n",
      "Epoch 148/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7276 - acc: 0.6868\n",
      "Epoch 149/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7224 - acc: 0.6829\n",
      "Epoch 150/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7481 - acc: 0.6723\n",
      "Epoch 151/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7321 - acc: 0.6847\n",
      "Epoch 152/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7214 - acc: 0.6850\n",
      "Epoch 153/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7185 - acc: 0.6840\n",
      "Epoch 154/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7227 - acc: 0.6829\n",
      "Epoch 155/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7158 - acc: 0.6833\n",
      "Epoch 156/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7335 - acc: 0.6878\n",
      "Epoch 157/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7352 - acc: 0.6847\n",
      "Epoch 158/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7228 - acc: 0.6845\n",
      "Epoch 159/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7126 - acc: 0.6958\n",
      "Epoch 160/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7129 - acc: 0.6901\n",
      "1066/1066 [==============================] - 1s 1ms/step\n",
      "4263/4263 [==============================] - 1s 263us/step\n",
      "\n",
      "acc: 66.89%\n",
      "\n",
      "acc: 69.62%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df['Class'].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d,y_d,test_size=0.2,random_state=42)\n",
    "\n",
    "y_train_2=pd.get_dummies(y_train)\n",
    "y_test_2=pd.get_dummies(y_test)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(650, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(1200, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.35))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=160, callbacks=[early_stopping_monitor],batch_size=50)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "scores_2 = model.evaluate(X_train, y_train_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_2[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/160\n",
      "4263/4263 [==============================] - 8s 2ms/step - loss: 1.0452 - acc: 0.5602\n",
      "Epoch 2/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.9092 - acc: 0.6005\n",
      "Epoch 3/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8844 - acc: 0.5961\n",
      "Epoch 4/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8749 - acc: 0.6078\n",
      "Epoch 5/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8696 - acc: 0.6165\n",
      "Epoch 6/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8548 - acc: 0.6315\n",
      "Epoch 7/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8572 - acc: 0.6289\n",
      "Epoch 8/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8526 - acc: 0.6345\n",
      "Epoch 9/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8449 - acc: 0.6376\n",
      "Epoch 10/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8325 - acc: 0.6470\n",
      "Epoch 11/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8346 - acc: 0.6388\n",
      "Epoch 12/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8468 - acc: 0.6289\n",
      "Epoch 13/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8316 - acc: 0.6366\n",
      "Epoch 14/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8264 - acc: 0.6392\n",
      "Epoch 15/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8254 - acc: 0.6507\n",
      "Epoch 16/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8122 - acc: 0.6495\n",
      "Epoch 17/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8292 - acc: 0.6423\n",
      "Epoch 18/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8120 - acc: 0.6545\n",
      "Epoch 19/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8006 - acc: 0.6563\n",
      "Epoch 20/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8034 - acc: 0.6554\n",
      "Epoch 21/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8086 - acc: 0.6493\n",
      "Epoch 22/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8020 - acc: 0.6575\n",
      "Epoch 23/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7977 - acc: 0.6559\n",
      "Epoch 24/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.8021 - acc: 0.6521\n",
      "Epoch 25/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7832 - acc: 0.6561\n",
      "Epoch 26/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7908 - acc: 0.6589\n",
      "Epoch 27/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7910 - acc: 0.6636\n",
      "Epoch 28/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7861 - acc: 0.6610\n",
      "Epoch 29/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7874 - acc: 0.6664\n",
      "Epoch 30/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7898 - acc: 0.6650\n",
      "Epoch 31/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7902 - acc: 0.6587\n",
      "Epoch 32/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7907 - acc: 0.6538\n",
      "Epoch 33/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7801 - acc: 0.6646\n",
      "Epoch 34/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7788 - acc: 0.6580\n",
      "Epoch 35/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7909 - acc: 0.6535\n",
      "Epoch 36/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7739 - acc: 0.6669\n",
      "Epoch 37/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7738 - acc: 0.6648\n",
      "Epoch 38/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7956 - acc: 0.6502\n",
      "Epoch 39/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7822 - acc: 0.6608\n",
      "Epoch 40/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7889 - acc: 0.6664\n",
      "Epoch 41/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7749 - acc: 0.6636\n",
      "Epoch 42/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7773 - acc: 0.6582\n",
      "Epoch 43/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7685 - acc: 0.6653\n",
      "Epoch 44/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7684 - acc: 0.6594\n",
      "Epoch 45/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7672 - acc: 0.6671\n",
      "Epoch 46/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7762 - acc: 0.6653\n",
      "Epoch 47/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7747 - acc: 0.6603\n",
      "Epoch 48/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7716 - acc: 0.6685\n",
      "Epoch 49/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7707 - acc: 0.6613\n",
      "Epoch 50/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7655 - acc: 0.6662\n",
      "Epoch 51/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7656 - acc: 0.6716\n",
      "Epoch 52/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7695 - acc: 0.6606\n",
      "Epoch 53/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7710 - acc: 0.6631\n",
      "Epoch 54/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7697 - acc: 0.6660\n",
      "Epoch 55/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7615 - acc: 0.6669\n",
      "Epoch 56/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7637 - acc: 0.6664\n",
      "Epoch 57/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7728 - acc: 0.6615\n",
      "Epoch 58/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7765 - acc: 0.6578\n",
      "Epoch 59/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7626 - acc: 0.6678\n",
      "Epoch 60/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7541 - acc: 0.6704\n",
      "Epoch 61/160\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 0.7520 - acc: 0.6730\n",
      "Epoch 62/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7658 - acc: 0.6648\n",
      "Epoch 63/160\n",
      "4263/4263 [==============================] - 4s 994us/step - loss: 0.7539 - acc: 0.6711\n",
      "Epoch 64/160\n",
      "4263/4263 [==============================] - 4s 985us/step - loss: 0.7727 - acc: 0.6540\n",
      "Epoch 65/160\n",
      "4263/4263 [==============================] - 4s 977us/step - loss: 0.7579 - acc: 0.6700\n",
      "Epoch 66/160\n",
      "4263/4263 [==============================] - 4s 981us/step - loss: 0.7562 - acc: 0.6648\n",
      "Epoch 67/160\n",
      "4263/4263 [==============================] - 4s 986us/step - loss: 0.7608 - acc: 0.6624\n",
      "Epoch 68/160\n",
      "4263/4263 [==============================] - 4s 982us/step - loss: 0.7642 - acc: 0.6660\n",
      "Epoch 69/160\n",
      "4263/4263 [==============================] - 4s 984us/step - loss: 0.7590 - acc: 0.6629\n",
      "Epoch 70/160\n",
      "4263/4263 [==============================] - 4s 982us/step - loss: 0.7644 - acc: 0.6620\n",
      "Epoch 71/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7684 - acc: 0.6678\n",
      "Epoch 72/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7722 - acc: 0.6615\n",
      "Epoch 73/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7642 - acc: 0.6660\n",
      "Epoch 74/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7587 - acc: 0.6643\n",
      "Epoch 75/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7546 - acc: 0.6744\n",
      "Epoch 76/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7642 - acc: 0.6585\n",
      "Epoch 77/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7573 - acc: 0.6685\n",
      "Epoch 78/160\n",
      "4263/4263 [==============================] - 7s 2ms/step - loss: 0.7676 - acc: 0.6643\n",
      "Epoch 79/160\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 0.7621 - acc: 0.6681\n",
      "Epoch 80/160\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 0.7531 - acc: 0.6692\n",
      "Epoch 81/160\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 0.7455 - acc: 0.6751\n",
      "Epoch 82/160\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 0.7555 - acc: 0.6718\n",
      "Epoch 83/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 6s 1ms/step - loss: 0.7494 - acc: 0.6763\n",
      "Epoch 84/160\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 0.7561 - acc: 0.6770\n",
      "Epoch 85/160\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 0.7592 - acc: 0.6622\n",
      "Epoch 86/160\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 0.7588 - acc: 0.6707\n",
      "Epoch 87/160\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 0.7526 - acc: 0.6681\n",
      "Epoch 88/160\n",
      "4263/4263 [==============================] - 6s 2ms/step - loss: 0.7516 - acc: 0.6746\n",
      "Epoch 89/160\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 0.7467 - acc: 0.6707\n",
      "Epoch 90/160\n",
      "4263/4263 [==============================] - 7s 2ms/step - loss: 0.7561 - acc: 0.6725\n",
      "Epoch 91/160\n",
      "4263/4263 [==============================] - 6s 2ms/step - loss: 0.7464 - acc: 0.6744\n",
      "Epoch 92/160\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 0.7503 - acc: 0.6742\n",
      "Epoch 93/160\n",
      "4263/4263 [==============================] - 7s 2ms/step - loss: 0.7485 - acc: 0.6730\n",
      "Epoch 94/160\n",
      "4263/4263 [==============================] - 7s 2ms/step - loss: 0.7451 - acc: 0.6779\n",
      "Epoch 95/160\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 0.7473 - acc: 0.6688\n",
      "Epoch 96/160\n",
      "4263/4263 [==============================] - 7s 2ms/step - loss: 0.7502 - acc: 0.6690\n",
      "Epoch 97/160\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 0.7408 - acc: 0.6791\n",
      "Epoch 98/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7450 - acc: 0.6732\n",
      "Epoch 99/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7628 - acc: 0.6624\n",
      "Epoch 100/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7475 - acc: 0.6791\n",
      "Epoch 101/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7412 - acc: 0.6753\n",
      "Epoch 102/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7535 - acc: 0.6725\n",
      "Epoch 103/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7448 - acc: 0.6678\n",
      "Epoch 104/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7406 - acc: 0.6803\n",
      "Epoch 105/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7451 - acc: 0.6690\n",
      "Epoch 106/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7421 - acc: 0.6770\n",
      "Epoch 107/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7456 - acc: 0.6793\n",
      "Epoch 108/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7506 - acc: 0.6671\n",
      "Epoch 109/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7436 - acc: 0.6796\n",
      "Epoch 110/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7514 - acc: 0.6753\n",
      "Epoch 111/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7427 - acc: 0.6768\n",
      "Epoch 112/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7372 - acc: 0.6768\n",
      "Epoch 113/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7431 - acc: 0.6716\n",
      "Epoch 114/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7400 - acc: 0.6730\n",
      "Epoch 115/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7381 - acc: 0.6803\n",
      "Epoch 116/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7476 - acc: 0.6704\n",
      "Epoch 117/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7545 - acc: 0.6739\n",
      "Epoch 118/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7420 - acc: 0.6789\n",
      "Epoch 119/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7393 - acc: 0.6803\n",
      "Epoch 120/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7382 - acc: 0.6838\n",
      "Epoch 121/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7480 - acc: 0.6742\n",
      "Epoch 122/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7317 - acc: 0.6812\n",
      "Epoch 123/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7357 - acc: 0.6798\n",
      "Epoch 124/160\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 0.7436 - acc: 0.6714\n",
      "Epoch 125/160\n",
      "4263/4263 [==============================] - 8s 2ms/step - loss: 0.7396 - acc: 0.6744\n",
      "Epoch 126/160\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 0.7363 - acc: 0.6831\n",
      "Epoch 127/160\n",
      "4263/4263 [==============================] - 7s 2ms/step - loss: 0.7365 - acc: 0.6803\n",
      "Epoch 128/160\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 0.7409 - acc: 0.6812\n",
      "Epoch 129/160\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 0.7290 - acc: 0.6861\n",
      "Epoch 130/160\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 0.7318 - acc: 0.6817\n",
      "Epoch 131/160\n",
      "4263/4263 [==============================] - 7s 2ms/step - loss: 0.7261 - acc: 0.6878\n",
      "Epoch 132/160\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 0.7332 - acc: 0.6812\n",
      "Epoch 133/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7378 - acc: 0.6777\n",
      "Epoch 134/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7263 - acc: 0.6807\n",
      "Epoch 135/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7283 - acc: 0.6838\n",
      "Epoch 136/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7257 - acc: 0.6962\n",
      "Epoch 137/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7425 - acc: 0.6760\n",
      "Epoch 138/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7412 - acc: 0.6829\n",
      "Epoch 139/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7316 - acc: 0.6882\n",
      "Epoch 140/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7383 - acc: 0.6746\n",
      "Epoch 141/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7262 - acc: 0.6882\n",
      "Epoch 142/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7250 - acc: 0.6897\n",
      "Epoch 143/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7243 - acc: 0.6913\n",
      "Epoch 144/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7307 - acc: 0.6826\n",
      "Epoch 145/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7225 - acc: 0.6894\n",
      "Epoch 146/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7265 - acc: 0.6854\n",
      "Epoch 147/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7263 - acc: 0.6854\n",
      "Epoch 148/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7246 - acc: 0.6892\n",
      "Epoch 149/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7278 - acc: 0.6871\n",
      "Epoch 150/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7206 - acc: 0.6847\n",
      "Epoch 151/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7154 - acc: 0.6958\n",
      "Epoch 152/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7229 - acc: 0.6890\n",
      "Epoch 153/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7269 - acc: 0.6852\n",
      "Epoch 154/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7313 - acc: 0.6871\n",
      "Epoch 155/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7344 - acc: 0.6824\n",
      "Epoch 156/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7268 - acc: 0.6843\n",
      "Epoch 157/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7247 - acc: 0.6922\n",
      "Epoch 158/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7215 - acc: 0.6899\n",
      "Epoch 159/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7182 - acc: 0.6878\n",
      "Epoch 160/160\n",
      "4263/4263 [==============================] - 5s 1ms/step - loss: 0.7240 - acc: 0.6854\n",
      "1066/1066 [==============================] - 2s 1ms/step\n",
      "4263/4263 [==============================] - 1s 212us/step\n",
      "\n",
      "acc: 67.64%\n",
      "\n",
      "acc: 69.74%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df['Class'].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d,y_d,test_size=0.2,random_state=42)\n",
    "\n",
    "y_train_2=pd.get_dummies(y_train)\n",
    "y_test_2=pd.get_dummies(y_test)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(650, activation='relu'))\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "model.add(Dense(1200, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=160, callbacks=[early_stopping_monitor],batch_size=50)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "scores_2 = model.evaluate(X_train, y_train_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_2[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/160\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 1.0460 - acc: 0.5576\n",
      "Epoch 2/160\n",
      "4263/4263 [==============================] - 2s 455us/step - loss: 0.9226 - acc: 0.5986\n",
      "Epoch 3/160\n",
      "4263/4263 [==============================] - 2s 514us/step - loss: 0.8796 - acc: 0.6061\n",
      "Epoch 4/160\n",
      "4263/4263 [==============================] - 2s 559us/step - loss: 0.8833 - acc: 0.6219\n",
      "Epoch 5/160\n",
      "4263/4263 [==============================] - 3s 607us/step - loss: 0.8650 - acc: 0.6242\n",
      "Epoch 6/160\n",
      "4263/4263 [==============================] - 2s 511us/step - loss: 0.8498 - acc: 0.6277\n",
      "Epoch 7/160\n",
      "4263/4263 [==============================] - 2s 462us/step - loss: 0.8499 - acc: 0.6327\n",
      "Epoch 8/160\n",
      "4263/4263 [==============================] - 2s 476us/step - loss: 0.8496 - acc: 0.6235\n",
      "Epoch 9/160\n",
      "4263/4263 [==============================] - 2s 473us/step - loss: 0.8433 - acc: 0.6357\n",
      "Epoch 10/160\n",
      "4263/4263 [==============================] - 2s 469us/step - loss: 0.8305 - acc: 0.6383\n",
      "Epoch 11/160\n",
      "4263/4263 [==============================] - 2s 468us/step - loss: 0.8330 - acc: 0.6441\n",
      "Epoch 12/160\n",
      "4263/4263 [==============================] - 2s 463us/step - loss: 0.8236 - acc: 0.6409\n",
      "Epoch 13/160\n",
      "4263/4263 [==============================] - 2s 466us/step - loss: 0.8241 - acc: 0.6399\n",
      "Epoch 14/160\n",
      "4263/4263 [==============================] - 2s 468us/step - loss: 0.8177 - acc: 0.6420\n",
      "Epoch 15/160\n",
      "4263/4263 [==============================] - 2s 474us/step - loss: 0.8071 - acc: 0.6521\n",
      "Epoch 16/160\n",
      "4263/4263 [==============================] - 2s 459us/step - loss: 0.8121 - acc: 0.6453\n",
      "Epoch 17/160\n",
      "4263/4263 [==============================] - 2s 467us/step - loss: 0.8127 - acc: 0.6465\n",
      "Epoch 18/160\n",
      "4263/4263 [==============================] - 2s 465us/step - loss: 0.8041 - acc: 0.6519\n",
      "Epoch 19/160\n",
      "4263/4263 [==============================] - 2s 452us/step - loss: 0.8056 - acc: 0.6587\n",
      "Epoch 20/160\n",
      "4263/4263 [==============================] - 2s 467us/step - loss: 0.8115 - acc: 0.6472\n",
      "Epoch 21/160\n",
      "4263/4263 [==============================] - 2s 477us/step - loss: 0.7890 - acc: 0.6585\n",
      "Epoch 22/160\n",
      "4263/4263 [==============================] - 2s 462us/step - loss: 0.7935 - acc: 0.6563\n",
      "Epoch 23/160\n",
      "4263/4263 [==============================] - 2s 469us/step - loss: 0.7924 - acc: 0.6563\n",
      "Epoch 24/160\n",
      "4263/4263 [==============================] - 2s 467us/step - loss: 0.7987 - acc: 0.6531\n",
      "Epoch 25/160\n",
      "4263/4263 [==============================] - 2s 467us/step - loss: 0.7897 - acc: 0.6615\n",
      "Epoch 26/160\n",
      "4263/4263 [==============================] - 2s 466us/step - loss: 0.7864 - acc: 0.6608\n",
      "Epoch 27/160\n",
      "4263/4263 [==============================] - 2s 467us/step - loss: 0.7865 - acc: 0.6662\n",
      "Epoch 28/160\n",
      "4263/4263 [==============================] - 2s 469us/step - loss: 0.7855 - acc: 0.6631\n",
      "Epoch 29/160\n",
      "4263/4263 [==============================] - 2s 464us/step - loss: 0.7876 - acc: 0.6606\n",
      "Epoch 30/160\n",
      "4263/4263 [==============================] - 2s 480us/step - loss: 0.7871 - acc: 0.6629\n",
      "Epoch 31/160\n",
      "4263/4263 [==============================] - 2s 456us/step - loss: 0.7807 - acc: 0.6601\n",
      "Epoch 32/160\n",
      "4263/4263 [==============================] - 2s 465us/step - loss: 0.7720 - acc: 0.6563\n",
      "Epoch 33/160\n",
      "4263/4263 [==============================] - 2s 469us/step - loss: 0.7847 - acc: 0.6646\n",
      "Epoch 34/160\n",
      "4263/4263 [==============================] - 2s 562us/step - loss: 0.7752 - acc: 0.6624\n",
      "Epoch 35/160\n",
      "4263/4263 [==============================] - 2s 559us/step - loss: 0.7841 - acc: 0.6549\n",
      "Epoch 36/160\n",
      "4263/4263 [==============================] - 2s 465us/step - loss: 0.7693 - acc: 0.6634\n",
      "Epoch 37/160\n",
      "4263/4263 [==============================] - 2s 466us/step - loss: 0.7684 - acc: 0.6655\n",
      "Epoch 38/160\n",
      "4263/4263 [==============================] - 2s 475us/step - loss: 0.7739 - acc: 0.6690\n",
      "Epoch 39/160\n",
      "4263/4263 [==============================] - 2s 463us/step - loss: 0.7676 - acc: 0.6671\n",
      "Epoch 40/160\n",
      "4263/4263 [==============================] - 2s 469us/step - loss: 0.7780 - acc: 0.6599\n",
      "Epoch 41/160\n",
      "4263/4263 [==============================] - 2s 572us/step - loss: 0.7759 - acc: 0.6585\n",
      "Epoch 42/160\n",
      "4263/4263 [==============================] - 2s 557us/step - loss: 0.7684 - acc: 0.6711\n",
      "Epoch 43/160\n",
      "4263/4263 [==============================] - 2s 473us/step - loss: 0.7659 - acc: 0.6692\n",
      "Epoch 44/160\n",
      "4263/4263 [==============================] - 2s 464us/step - loss: 0.7718 - acc: 0.6650\n",
      "Epoch 45/160\n",
      "4263/4263 [==============================] - 2s 474us/step - loss: 0.7705 - acc: 0.6620\n",
      "Epoch 46/160\n",
      "4263/4263 [==============================] - 2s 460us/step - loss: 0.7594 - acc: 0.6697\n",
      "Epoch 47/160\n",
      "4263/4263 [==============================] - 2s 457us/step - loss: 0.7653 - acc: 0.6660\n",
      "Epoch 48/160\n",
      "4263/4263 [==============================] - 2s 465us/step - loss: 0.7643 - acc: 0.6700\n",
      "Epoch 49/160\n",
      "4263/4263 [==============================] - 2s 473us/step - loss: 0.7641 - acc: 0.6650\n",
      "Epoch 50/160\n",
      "4263/4263 [==============================] - 2s 472us/step - loss: 0.7547 - acc: 0.6725\n",
      "Epoch 51/160\n",
      "4263/4263 [==============================] - 2s 470us/step - loss: 0.7542 - acc: 0.6669\n",
      "Epoch 52/160\n",
      "4263/4263 [==============================] - 3s 660us/step - loss: 0.7600 - acc: 0.6671\n",
      "Epoch 53/160\n",
      "4263/4263 [==============================] - 2s 473us/step - loss: 0.7607 - acc: 0.6683\n",
      "Epoch 54/160\n",
      "4263/4263 [==============================] - 2s 472us/step - loss: 0.7574 - acc: 0.6700\n",
      "Epoch 55/160\n",
      "4263/4263 [==============================] - 2s 464us/step - loss: 0.7681 - acc: 0.6639\n",
      "Epoch 56/160\n",
      "4263/4263 [==============================] - 2s 469us/step - loss: 0.7661 - acc: 0.6685\n",
      "Epoch 57/160\n",
      "4263/4263 [==============================] - 2s 483us/step - loss: 0.7531 - acc: 0.6631\n",
      "Epoch 58/160\n",
      "4263/4263 [==============================] - 2s 467us/step - loss: 0.7613 - acc: 0.6721\n",
      "Epoch 59/160\n",
      "4263/4263 [==============================] - 2s 470us/step - loss: 0.7568 - acc: 0.6730\n",
      "Epoch 60/160\n",
      "4263/4263 [==============================] - 2s 462us/step - loss: 0.7559 - acc: 0.6714\n",
      "Epoch 61/160\n",
      "4263/4263 [==============================] - 2s 462us/step - loss: 0.7510 - acc: 0.6728\n",
      "Epoch 62/160\n",
      "4263/4263 [==============================] - 2s 478us/step - loss: 0.7512 - acc: 0.6751\n",
      "Epoch 63/160\n",
      "4263/4263 [==============================] - 2s 464us/step - loss: 0.7479 - acc: 0.6714\n",
      "Epoch 64/160\n",
      "4263/4263 [==============================] - 3s 591us/step - loss: 0.7540 - acc: 0.6744\n",
      "Epoch 65/160\n",
      "4263/4263 [==============================] - 2s 574us/step - loss: 0.7511 - acc: 0.6662\n",
      "Epoch 66/160\n",
      "4263/4263 [==============================] - 2s 551us/step - loss: 0.7565 - acc: 0.6700\n",
      "Epoch 67/160\n",
      "4263/4263 [==============================] - 2s 551us/step - loss: 0.7517 - acc: 0.6695\n",
      "Epoch 68/160\n",
      "4263/4263 [==============================] - 2s 561us/step - loss: 0.7668 - acc: 0.6650\n",
      "Epoch 69/160\n",
      "4263/4263 [==============================] - 2s 497us/step - loss: 0.7553 - acc: 0.6707\n",
      "Epoch 70/160\n",
      "4263/4263 [==============================] - 2s 476us/step - loss: 0.7556 - acc: 0.6744\n",
      "Epoch 71/160\n",
      "4263/4263 [==============================] - 2s 482us/step - loss: 0.7527 - acc: 0.6700\n",
      "Epoch 72/160\n",
      "4263/4263 [==============================] - 2s 484us/step - loss: 0.7464 - acc: 0.6728\n",
      "Epoch 73/160\n",
      "4263/4263 [==============================] - 2s 477us/step - loss: 0.7479 - acc: 0.6760\n",
      "Epoch 74/160\n",
      "4263/4263 [==============================] - 2s 476us/step - loss: 0.7510 - acc: 0.6749\n",
      "Epoch 75/160\n",
      "4263/4263 [==============================] - 2s 463us/step - loss: 0.7489 - acc: 0.6709\n",
      "Epoch 76/160\n",
      "4263/4263 [==============================] - 2s 486us/step - loss: 0.7522 - acc: 0.6765\n",
      "Epoch 77/160\n",
      "4263/4263 [==============================] - 2s 473us/step - loss: 0.7613 - acc: 0.6681\n",
      "Epoch 78/160\n",
      "4263/4263 [==============================] - 2s 473us/step - loss: 0.7424 - acc: 0.6714\n",
      "Epoch 79/160\n",
      "4263/4263 [==============================] - 2s 462us/step - loss: 0.7464 - acc: 0.6836\n",
      "Epoch 80/160\n",
      "4263/4263 [==============================] - 2s 475us/step - loss: 0.7451 - acc: 0.6789\n",
      "Epoch 81/160\n",
      "4263/4263 [==============================] - 2s 468us/step - loss: 0.7360 - acc: 0.6779\n",
      "Epoch 82/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 2s 449us/step - loss: 0.7376 - acc: 0.6763\n",
      "Epoch 83/160\n",
      "4263/4263 [==============================] - 2s 473us/step - loss: 0.7503 - acc: 0.6723\n",
      "Epoch 84/160\n",
      "4263/4263 [==============================] - 2s 458us/step - loss: 0.7336 - acc: 0.6821\n",
      "Epoch 85/160\n",
      "4263/4263 [==============================] - 2s 455us/step - loss: 0.7369 - acc: 0.6824\n",
      "Epoch 86/160\n",
      "4263/4263 [==============================] - 2s 469us/step - loss: 0.7385 - acc: 0.6817\n",
      "Epoch 87/160\n",
      "4263/4263 [==============================] - 2s 452us/step - loss: 0.7374 - acc: 0.6800\n",
      "Epoch 88/160\n",
      "4263/4263 [==============================] - 2s 457us/step - loss: 0.7386 - acc: 0.6817\n",
      "Epoch 89/160\n",
      "4263/4263 [==============================] - 2s 461us/step - loss: 0.7380 - acc: 0.6765\n",
      "Epoch 90/160\n",
      "4263/4263 [==============================] - 2s 449us/step - loss: 0.7419 - acc: 0.6770\n",
      "Epoch 91/160\n",
      "4263/4263 [==============================] - 2s 462us/step - loss: 0.7371 - acc: 0.6831\n",
      "Epoch 92/160\n",
      "4263/4263 [==============================] - 2s 466us/step - loss: 0.7470 - acc: 0.6793\n",
      "Epoch 93/160\n",
      "4263/4263 [==============================] - 2s 473us/step - loss: 0.7348 - acc: 0.6843\n",
      "Epoch 94/160\n",
      "4263/4263 [==============================] - 2s 452us/step - loss: 0.7327 - acc: 0.6831\n",
      "Epoch 95/160\n",
      "4263/4263 [==============================] - 2s 465us/step - loss: 0.7386 - acc: 0.6784\n",
      "Epoch 96/160\n",
      "4263/4263 [==============================] - 2s 459us/step - loss: 0.7364 - acc: 0.6803\n",
      "Epoch 97/160\n",
      "4263/4263 [==============================] - 2s 544us/step - loss: 0.7317 - acc: 0.6826\n",
      "Epoch 98/160\n",
      "4263/4263 [==============================] - 2s 562us/step - loss: 0.7581 - acc: 0.6758\n",
      "Epoch 99/160\n",
      "4263/4263 [==============================] - 2s 530us/step - loss: 0.7318 - acc: 0.6810\n",
      "Epoch 100/160\n",
      "4263/4263 [==============================] - 2s 479us/step - loss: 0.7353 - acc: 0.6800\n",
      "Epoch 101/160\n",
      "4263/4263 [==============================] - 2s 457us/step - loss: 0.7343 - acc: 0.6836\n",
      "Epoch 102/160\n",
      "4263/4263 [==============================] - 3s 604us/step - loss: 0.7351 - acc: 0.6845\n",
      "Epoch 103/160\n",
      "4263/4263 [==============================] - 2s 551us/step - loss: 0.7368 - acc: 0.6824\n",
      "Epoch 104/160\n",
      "4263/4263 [==============================] - 2s 524us/step - loss: 0.7314 - acc: 0.6814\n",
      "Epoch 105/160\n",
      "4263/4263 [==============================] - 2s 540us/step - loss: 0.7326 - acc: 0.6845\n",
      "Epoch 106/160\n",
      "4263/4263 [==============================] - 2s 557us/step - loss: 0.7345 - acc: 0.6784\n",
      "Epoch 107/160\n",
      "4263/4263 [==============================] - 2s 476us/step - loss: 0.7355 - acc: 0.6852\n",
      "Epoch 108/160\n",
      "4263/4263 [==============================] - 2s 499us/step - loss: 0.7236 - acc: 0.6894\n",
      "Epoch 109/160\n",
      "4263/4263 [==============================] - 2s 497us/step - loss: 0.7422 - acc: 0.6786\n",
      "Epoch 110/160\n",
      "4263/4263 [==============================] - 2s 490us/step - loss: 0.7402 - acc: 0.6793\n",
      "Epoch 111/160\n",
      "4263/4263 [==============================] - 2s 515us/step - loss: 0.7308 - acc: 0.6836\n",
      "Epoch 112/160\n",
      "4263/4263 [==============================] - 2s 489us/step - loss: 0.7293 - acc: 0.6878\n",
      "Epoch 113/160\n",
      "4263/4263 [==============================] - 2s 498us/step - loss: 0.7286 - acc: 0.6875\n",
      "Epoch 114/160\n",
      "4263/4263 [==============================] - 2s 503us/step - loss: 0.7312 - acc: 0.6829\n",
      "Epoch 115/160\n",
      "4263/4263 [==============================] - 2s 488us/step - loss: 0.7226 - acc: 0.6868\n",
      "Epoch 116/160\n",
      "4263/4263 [==============================] - 2s 494us/step - loss: 0.7313 - acc: 0.6829\n",
      "Epoch 117/160\n",
      "4263/4263 [==============================] - 2s 493us/step - loss: 0.7201 - acc: 0.6897\n",
      "Epoch 118/160\n",
      "4263/4263 [==============================] - 2s 499us/step - loss: 0.7273 - acc: 0.6897\n",
      "Epoch 119/160\n",
      "4263/4263 [==============================] - 2s 505us/step - loss: 0.7245 - acc: 0.6873\n",
      "Epoch 120/160\n",
      "4263/4263 [==============================] - 2s 564us/step - loss: 0.7212 - acc: 0.6875\n",
      "Epoch 121/160\n",
      "4263/4263 [==============================] - 3s 617us/step - loss: 0.7197 - acc: 0.6908\n",
      "Epoch 122/160\n",
      "4263/4263 [==============================] - 3s 763us/step - loss: 0.7198 - acc: 0.6854\n",
      "Epoch 123/160\n",
      "4263/4263 [==============================] - 3s 615us/step - loss: 0.7207 - acc: 0.6911\n",
      "Epoch 124/160\n",
      "4263/4263 [==============================] - 3s 605us/step - loss: 0.7289 - acc: 0.6821\n",
      "Epoch 125/160\n",
      "4263/4263 [==============================] - 2s 526us/step - loss: 0.7205 - acc: 0.6894\n",
      "Epoch 126/160\n",
      "4263/4263 [==============================] - 2s 485us/step - loss: 0.7207 - acc: 0.6880\n",
      "Epoch 127/160\n",
      "4263/4263 [==============================] - 2s 460us/step - loss: 0.7121 - acc: 0.6934\n",
      "Epoch 128/160\n",
      "4263/4263 [==============================] - 2s 462us/step - loss: 0.7128 - acc: 0.6936\n",
      "Epoch 129/160\n",
      "4263/4263 [==============================] - 2s 470us/step - loss: 0.7249 - acc: 0.6847\n",
      "Epoch 130/160\n",
      "4263/4263 [==============================] - 2s 457us/step - loss: 0.7181 - acc: 0.6943\n",
      "Epoch 131/160\n",
      "4263/4263 [==============================] - 2s 447us/step - loss: 0.7265 - acc: 0.6899\n",
      "Epoch 132/160\n",
      "4263/4263 [==============================] - 2s 466us/step - loss: 0.7175 - acc: 0.6845\n",
      "Epoch 133/160\n",
      "4263/4263 [==============================] - 2s 453us/step - loss: 0.7107 - acc: 0.6955\n",
      "Epoch 134/160\n",
      "4263/4263 [==============================] - 2s 464us/step - loss: 0.7142 - acc: 0.6934\n",
      "Epoch 135/160\n",
      "4263/4263 [==============================] - 2s 462us/step - loss: 0.7301 - acc: 0.6854 0s - loss: 0.7298 - acc:\n",
      "Epoch 136/160\n",
      "4263/4263 [==============================] - 2s 464us/step - loss: 0.7303 - acc: 0.6885\n",
      "Epoch 137/160\n",
      "4263/4263 [==============================] - 2s 467us/step - loss: 0.7175 - acc: 0.6965\n",
      "Epoch 138/160\n",
      "4263/4263 [==============================] - 2s 494us/step - loss: 0.7075 - acc: 0.6946\n",
      "Epoch 139/160\n",
      "4263/4263 [==============================] - 2s 464us/step - loss: 0.7100 - acc: 0.6913\n",
      "Epoch 140/160\n",
      "4263/4263 [==============================] - 2s 467us/step - loss: 0.7078 - acc: 0.6974\n",
      "Epoch 141/160\n",
      "4263/4263 [==============================] - 2s 457us/step - loss: 0.7099 - acc: 0.6904\n",
      "Epoch 142/160\n",
      "4263/4263 [==============================] - 2s 462us/step - loss: 0.7093 - acc: 0.6995\n",
      "Epoch 143/160\n",
      "4263/4263 [==============================] - 2s 450us/step - loss: 0.7069 - acc: 0.6955\n",
      "Epoch 144/160\n",
      "4263/4263 [==============================] - 2s 459us/step - loss: 0.6995 - acc: 0.7004\n",
      "Epoch 145/160\n",
      "4263/4263 [==============================] - 2s 460us/step - loss: 0.7194 - acc: 0.6852\n",
      "Epoch 146/160\n",
      "4263/4263 [==============================] - 2s 546us/step - loss: 0.7202 - acc: 0.6882\n",
      "Epoch 147/160\n",
      "4263/4263 [==============================] - 2s 492us/step - loss: 0.7083 - acc: 0.6979\n",
      "Epoch 148/160\n",
      "4263/4263 [==============================] - 2s 469us/step - loss: 0.7051 - acc: 0.6997\n",
      "Epoch 149/160\n",
      "4263/4263 [==============================] - 2s 480us/step - loss: 0.7028 - acc: 0.6965\n",
      "Epoch 150/160\n",
      "4263/4263 [==============================] - 2s 476us/step - loss: 0.7007 - acc: 0.6993\n",
      "Epoch 151/160\n",
      "4263/4263 [==============================] - 2s 470us/step - loss: 0.7058 - acc: 0.6934\n",
      "Epoch 152/160\n",
      "4263/4263 [==============================] - 2s 478us/step - loss: 0.7208 - acc: 0.6920\n",
      "Epoch 153/160\n",
      "4263/4263 [==============================] - 2s 472us/step - loss: 0.7211 - acc: 0.6843\n",
      "Epoch 154/160\n",
      "4263/4263 [==============================] - 2s 479us/step - loss: 0.7012 - acc: 0.6972\n",
      "Epoch 155/160\n",
      "4263/4263 [==============================] - 2s 468us/step - loss: 0.7067 - acc: 0.6948\n",
      "Epoch 156/160\n",
      "4263/4263 [==============================] - 2s 486us/step - loss: 0.6959 - acc: 0.6993\n",
      "Epoch 157/160\n",
      "4263/4263 [==============================] - 2s 473us/step - loss: 0.6943 - acc: 0.7033\n",
      "Epoch 158/160\n",
      "4263/4263 [==============================] - 2s 469us/step - loss: 0.7028 - acc: 0.6976\n",
      "Epoch 159/160\n",
      "4263/4263 [==============================] - 2s 497us/step - loss: 0.6951 - acc: 0.6976\n",
      "Epoch 160/160\n",
      "4263/4263 [==============================] - 2s 469us/step - loss: 0.7066 - acc: 0.6925\n",
      "1066/1066 [==============================] - 2s 2ms/step\n",
      "4263/4263 [==============================] - 1s 184us/step\n",
      "\n",
      "acc: 66.79%\n",
      "\n",
      "acc: 70.23%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df['Class'].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d,y_d,test_size=0.2,random_state=42)\n",
    "\n",
    "y_train_2=pd.get_dummies(y_train)\n",
    "y_test_2=pd.get_dummies(y_test)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(600, activation='relu'))\n",
    "#model.add(Dense(1000, activation='relu'))\n",
    "#model.add(Dense(1200, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=160, callbacks=[early_stopping_monitor],batch_size=50)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "scores_2 = model.evaluate(X_train, y_train_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_2[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/160\n",
      "4263/4263 [==============================] - 8s 2ms/step - loss: 1.0388 - acc: 0.5602\n",
      "Epoch 2/160\n",
      "4263/4263 [==============================] - 3s 609us/step - loss: 0.9082 - acc: 0.6010\n",
      "Epoch 3/160\n",
      "4263/4263 [==============================] - 2s 576us/step - loss: 0.8812 - acc: 0.6200\n",
      "Epoch 4/160\n",
      "4263/4263 [==============================] - 2s 552us/step - loss: 0.8779 - acc: 0.6221\n",
      "Epoch 5/160\n",
      "4263/4263 [==============================] - 2s 577us/step - loss: 0.8503 - acc: 0.6256\n",
      "Epoch 6/160\n",
      "4263/4263 [==============================] - 3s 656us/step - loss: 0.8576 - acc: 0.6329\n",
      "Epoch 7/160\n",
      "4263/4263 [==============================] - 2s 469us/step - loss: 0.8426 - acc: 0.6388\n",
      "Epoch 8/160\n",
      "4263/4263 [==============================] - 3s 639us/step - loss: 0.8432 - acc: 0.6296\n",
      "Epoch 9/160\n",
      "4263/4263 [==============================] - 2s 584us/step - loss: 0.8369 - acc: 0.6404\n",
      "Epoch 10/160\n",
      "4263/4263 [==============================] - 2s 509us/step - loss: 0.8388 - acc: 0.6329\n",
      "Epoch 11/160\n",
      "4263/4263 [==============================] - 3s 631us/step - loss: 0.8257 - acc: 0.6338\n",
      "Epoch 12/160\n",
      "4263/4263 [==============================] - 2s 528us/step - loss: 0.8320 - acc: 0.6418\n",
      "Epoch 13/160\n",
      "4263/4263 [==============================] - 2s 473us/step - loss: 0.8289 - acc: 0.6474\n",
      "Epoch 14/160\n",
      "4263/4263 [==============================] - 2s 576us/step - loss: 0.8140 - acc: 0.6458\n",
      "Epoch 15/160\n",
      "4263/4263 [==============================] - 2s 580us/step - loss: 0.8097 - acc: 0.6554\n",
      "Epoch 16/160\n",
      "4263/4263 [==============================] - 2s 570us/step - loss: 0.8048 - acc: 0.6460\n",
      "Epoch 17/160\n",
      "4263/4263 [==============================] - 2s 494us/step - loss: 0.8033 - acc: 0.6505\n",
      "Epoch 18/160\n",
      "4263/4263 [==============================] - 2s 481us/step - loss: 0.7986 - acc: 0.6512\n",
      "Epoch 19/160\n",
      "4263/4263 [==============================] - 2s 498us/step - loss: 0.7864 - acc: 0.6634\n",
      "Epoch 20/160\n",
      "4263/4263 [==============================] - 2s 487us/step - loss: 0.7972 - acc: 0.6549\n",
      "Epoch 21/160\n",
      "4263/4263 [==============================] - 2s 499us/step - loss: 0.7957 - acc: 0.6545\n",
      "Epoch 22/160\n",
      "4263/4263 [==============================] - 2s 515us/step - loss: 0.7986 - acc: 0.6510\n",
      "Epoch 23/160\n",
      "4263/4263 [==============================] - 2s 520us/step - loss: 0.7892 - acc: 0.6615\n",
      "Epoch 24/160\n",
      "4263/4263 [==============================] - 2s 516us/step - loss: 0.7944 - acc: 0.6514\n",
      "Epoch 25/160\n",
      "4263/4263 [==============================] - 2s 521us/step - loss: 0.7858 - acc: 0.6634\n",
      "Epoch 26/160\n",
      "4263/4263 [==============================] - 2s 498us/step - loss: 0.7884 - acc: 0.6514\n",
      "Epoch 27/160\n",
      "4263/4263 [==============================] - 2s 491us/step - loss: 0.7732 - acc: 0.6660\n",
      "Epoch 28/160\n",
      "4263/4263 [==============================] - 2s 495us/step - loss: 0.7741 - acc: 0.6573\n",
      "Epoch 29/160\n",
      "4263/4263 [==============================] - 2s 490us/step - loss: 0.7764 - acc: 0.6664\n",
      "Epoch 30/160\n",
      "4263/4263 [==============================] - 2s 497us/step - loss: 0.7772 - acc: 0.6580\n",
      "Epoch 31/160\n",
      "4263/4263 [==============================] - 2s 497us/step - loss: 0.7712 - acc: 0.6646\n",
      "Epoch 32/160\n",
      "4263/4263 [==============================] - 2s 482us/step - loss: 0.7705 - acc: 0.6688\n",
      "Epoch 33/160\n",
      "4263/4263 [==============================] - 2s 508us/step - loss: 0.7697 - acc: 0.6662\n",
      "Epoch 34/160\n",
      "4263/4263 [==============================] - 2s 485us/step - loss: 0.7743 - acc: 0.6617\n",
      "Epoch 35/160\n",
      "4263/4263 [==============================] - 2s 501us/step - loss: 0.7670 - acc: 0.6592\n",
      "Epoch 36/160\n",
      "4263/4263 [==============================] - 2s 511us/step - loss: 0.7707 - acc: 0.6685\n",
      "Epoch 37/160\n",
      "4263/4263 [==============================] - 2s 494us/step - loss: 0.7691 - acc: 0.6643\n",
      "Epoch 38/160\n",
      "4263/4263 [==============================] - 2s 502us/step - loss: 0.7742 - acc: 0.6664\n",
      "Epoch 39/160\n",
      "4263/4263 [==============================] - 2s 499us/step - loss: 0.7732 - acc: 0.6695\n",
      "Epoch 40/160\n",
      "4263/4263 [==============================] - 2s 502us/step - loss: 0.7656 - acc: 0.6683\n",
      "Epoch 41/160\n",
      "4263/4263 [==============================] - 2s 494us/step - loss: 0.7600 - acc: 0.6716\n",
      "Epoch 42/160\n",
      "4263/4263 [==============================] - 3s 669us/step - loss: 0.7688 - acc: 0.6631\n",
      "Epoch 43/160\n",
      "4263/4263 [==============================] - 3s 683us/step - loss: 0.7616 - acc: 0.6648\n",
      "Epoch 44/160\n",
      "4263/4263 [==============================] - 2s 445us/step - loss: 0.7659 - acc: 0.6596\n",
      "Epoch 45/160\n",
      "4263/4263 [==============================] - 3s 602us/step - loss: 0.7725 - acc: 0.6653\n",
      "Epoch 46/160\n",
      "4263/4263 [==============================] - 2s 537us/step - loss: 0.7624 - acc: 0.6667\n",
      "Epoch 47/160\n",
      "4263/4263 [==============================] - 2s 534us/step - loss: 0.7585 - acc: 0.6697\n",
      "Epoch 48/160\n",
      "4263/4263 [==============================] - 2s 519us/step - loss: 0.7643 - acc: 0.6683\n",
      "Epoch 49/160\n",
      "4263/4263 [==============================] - 2s 522us/step - loss: 0.7531 - acc: 0.6772\n",
      "Epoch 50/160\n",
      "4263/4263 [==============================] - 2s 520us/step - loss: 0.7629 - acc: 0.6676\n",
      "Epoch 51/160\n",
      "4263/4263 [==============================] - 2s 516us/step - loss: 0.7678 - acc: 0.6660\n",
      "Epoch 52/160\n",
      "4263/4263 [==============================] - 2s 508us/step - loss: 0.7597 - acc: 0.6728\n",
      "Epoch 53/160\n",
      "4263/4263 [==============================] - 2s 534us/step - loss: 0.7621 - acc: 0.6660\n",
      "Epoch 54/160\n",
      "4263/4263 [==============================] - 2s 506us/step - loss: 0.7561 - acc: 0.6714\n",
      "Epoch 55/160\n",
      "4263/4263 [==============================] - 2s 526us/step - loss: 0.7558 - acc: 0.6702\n",
      "Epoch 56/160\n",
      "4263/4263 [==============================] - 2s 507us/step - loss: 0.7471 - acc: 0.6772\n",
      "Epoch 57/160\n",
      "4263/4263 [==============================] - 2s 501us/step - loss: 0.7513 - acc: 0.6716\n",
      "Epoch 58/160\n",
      "4263/4263 [==============================] - 2s 580us/step - loss: 0.7544 - acc: 0.6700\n",
      "Epoch 59/160\n",
      "4263/4263 [==============================] - 2s 521us/step - loss: 0.7558 - acc: 0.6714\n",
      "Epoch 60/160\n",
      "4263/4263 [==============================] - 2s 509us/step - loss: 0.7459 - acc: 0.6758\n",
      "Epoch 61/160\n",
      "4263/4263 [==============================] - 2s 508us/step - loss: 0.7526 - acc: 0.6749\n",
      "Epoch 62/160\n",
      "4263/4263 [==============================] - 2s 512us/step - loss: 0.7540 - acc: 0.6721\n",
      "Epoch 63/160\n",
      "4263/4263 [==============================] - 2s 508us/step - loss: 0.7449 - acc: 0.6786\n",
      "Epoch 64/160\n",
      "4263/4263 [==============================] - 2s 510us/step - loss: 0.7480 - acc: 0.6725\n",
      "Epoch 65/160\n",
      "4263/4263 [==============================] - 2s 563us/step - loss: 0.7502 - acc: 0.6695\n",
      "Epoch 66/160\n",
      "4263/4263 [==============================] - 3s 643us/step - loss: 0.7528 - acc: 0.6735\n",
      "Epoch 67/160\n",
      "4263/4263 [==============================] - 3s 621us/step - loss: 0.7575 - acc: 0.6732\n",
      "Epoch 68/160\n",
      "4263/4263 [==============================] - 2s 517us/step - loss: 0.7446 - acc: 0.6728\n",
      "Epoch 69/160\n",
      "4263/4263 [==============================] - 2s 497us/step - loss: 0.7514 - acc: 0.6730\n",
      "Epoch 70/160\n",
      "4263/4263 [==============================] - 2s 553us/step - loss: 0.7472 - acc: 0.6805\n",
      "Epoch 71/160\n",
      "4263/4263 [==============================] - 3s 654us/step - loss: 0.7549 - acc: 0.6688\n",
      "Epoch 72/160\n",
      "4263/4263 [==============================] - 3s 604us/step - loss: 0.7362 - acc: 0.6824\n",
      "Epoch 73/160\n",
      "4263/4263 [==============================] - 3s 590us/step - loss: 0.7473 - acc: 0.6770\n",
      "Epoch 74/160\n",
      "4263/4263 [==============================] - 3s 605us/step - loss: 0.7453 - acc: 0.6779\n",
      "Epoch 75/160\n",
      "4263/4263 [==============================] - 3s 599us/step - loss: 0.7525 - acc: 0.6749\n",
      "Epoch 76/160\n",
      "4263/4263 [==============================] - 2s 584us/step - loss: 0.7416 - acc: 0.6749\n",
      "Epoch 77/160\n",
      "4263/4263 [==============================] - 2s 583us/step - loss: 0.7455 - acc: 0.6760\n",
      "Epoch 78/160\n",
      "4263/4263 [==============================] - 3s 594us/step - loss: 0.7466 - acc: 0.6807\n",
      "Epoch 79/160\n",
      "4263/4263 [==============================] - 2s 546us/step - loss: 0.7360 - acc: 0.6819\n",
      "Epoch 80/160\n",
      "4263/4263 [==============================] - 2s 568us/step - loss: 0.7424 - acc: 0.6784\n",
      "Epoch 81/160\n",
      "4263/4263 [==============================] - 3s 600us/step - loss: 0.7421 - acc: 0.6831\n",
      "Epoch 82/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 2s 503us/step - loss: 0.7388 - acc: 0.6739\n",
      "Epoch 83/160\n",
      "4263/4263 [==============================] - 2s 488us/step - loss: 0.7326 - acc: 0.6805\n",
      "Epoch 84/160\n",
      "4263/4263 [==============================] - 3s 613us/step - loss: 0.7337 - acc: 0.6890\n",
      "Epoch 85/160\n",
      "4263/4263 [==============================] - 3s 644us/step - loss: 0.7336 - acc: 0.6779\n",
      "Epoch 86/160\n",
      "4263/4263 [==============================] - 3s 603us/step - loss: 0.7443 - acc: 0.6777\n",
      "Epoch 87/160\n",
      "4263/4263 [==============================] - 3s 667us/step - loss: 0.7316 - acc: 0.6885\n",
      "Epoch 88/160\n",
      "4263/4263 [==============================] - 3s 639us/step - loss: 0.7352 - acc: 0.6735\n",
      "Epoch 89/160\n",
      "4263/4263 [==============================] - 2s 519us/step - loss: 0.7298 - acc: 0.6831\n",
      "Epoch 90/160\n",
      "4263/4263 [==============================] - 2s 494us/step - loss: 0.7352 - acc: 0.6817\n",
      "Epoch 91/160\n",
      "4263/4263 [==============================] - 2s 416us/step - loss: 0.7325 - acc: 0.6843\n",
      "Epoch 92/160\n",
      "4263/4263 [==============================] - 2s 441us/step - loss: 0.7383 - acc: 0.6819\n",
      "Epoch 93/160\n",
      "4263/4263 [==============================] - 2s 399us/step - loss: 0.7389 - acc: 0.6831\n",
      "Epoch 94/160\n",
      "4263/4263 [==============================] - 2s 394us/step - loss: 0.7311 - acc: 0.6838\n",
      "Epoch 95/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.7355 - acc: 0.6793\n",
      "Epoch 96/160\n",
      "4263/4263 [==============================] - 2s 397us/step - loss: 0.7423 - acc: 0.6730\n",
      "Epoch 97/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.7319 - acc: 0.6824\n",
      "Epoch 98/160\n",
      "4263/4263 [==============================] - 2s 402us/step - loss: 0.7288 - acc: 0.6810\n",
      "Epoch 99/160\n",
      "4263/4263 [==============================] - 2s 396us/step - loss: 0.7358 - acc: 0.6845\n",
      "Epoch 100/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.7300 - acc: 0.6807\n",
      "Epoch 101/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.7230 - acc: 0.6847\n",
      "Epoch 102/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.7327 - acc: 0.6817\n",
      "Epoch 103/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.7277 - acc: 0.6859\n",
      "Epoch 104/160\n",
      "4263/4263 [==============================] - 2s 389us/step - loss: 0.7310 - acc: 0.6873\n",
      "Epoch 105/160\n",
      "4263/4263 [==============================] - 2s 395us/step - loss: 0.7234 - acc: 0.6880\n",
      "Epoch 106/160\n",
      "4263/4263 [==============================] - 2s 389us/step - loss: 0.7197 - acc: 0.6878\n",
      "Epoch 107/160\n",
      "4263/4263 [==============================] - 2s 388us/step - loss: 0.7171 - acc: 0.6911\n",
      "Epoch 108/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.7224 - acc: 0.6868\n",
      "Epoch 109/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.7281 - acc: 0.6866\n",
      "Epoch 110/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.7256 - acc: 0.6904\n",
      "Epoch 111/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.7195 - acc: 0.6915\n",
      "Epoch 112/160\n",
      "4263/4263 [==============================] - 2s 389us/step - loss: 0.7183 - acc: 0.6850\n",
      "Epoch 113/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.7191 - acc: 0.6929\n",
      "Epoch 114/160\n",
      "4263/4263 [==============================] - 2s 395us/step - loss: 0.7116 - acc: 0.6983\n",
      "Epoch 115/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.7142 - acc: 0.6890\n",
      "Epoch 116/160\n",
      "4263/4263 [==============================] - 2s 388us/step - loss: 0.7156 - acc: 0.6878\n",
      "Epoch 117/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.7194 - acc: 0.6892\n",
      "Epoch 118/160\n",
      "4263/4263 [==============================] - 2s 389us/step - loss: 0.7184 - acc: 0.6892\n",
      "Epoch 119/160\n",
      "4263/4263 [==============================] - 2s 385us/step - loss: 0.7099 - acc: 0.6988\n",
      "Epoch 120/160\n",
      "4263/4263 [==============================] - 2s 397us/step - loss: 0.7168 - acc: 0.6854\n",
      "Epoch 121/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.7235 - acc: 0.6875\n",
      "Epoch 122/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.7144 - acc: 0.6927\n",
      "Epoch 123/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.7187 - acc: 0.6943\n",
      "Epoch 124/160\n",
      "4263/4263 [==============================] - 2s 388us/step - loss: 0.7081 - acc: 0.6915\n",
      "Epoch 125/160\n",
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.7185 - acc: 0.6908\n",
      "Epoch 126/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.7271 - acc: 0.6850\n",
      "Epoch 127/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.7234 - acc: 0.6906\n",
      "Epoch 128/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.7130 - acc: 0.6932\n",
      "Epoch 129/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.7132 - acc: 0.6817\n",
      "Epoch 130/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.7068 - acc: 0.6979\n",
      "Epoch 131/160\n",
      "4263/4263 [==============================] - 2s 390us/step - loss: 0.7153 - acc: 0.6920\n",
      "Epoch 132/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.7121 - acc: 0.6951\n",
      "Epoch 133/160\n",
      "4263/4263 [==============================] - 2s 388us/step - loss: 0.7067 - acc: 0.6981\n",
      "Epoch 134/160\n",
      "4263/4263 [==============================] - 2s 389us/step - loss: 0.7075 - acc: 0.6925\n",
      "Epoch 135/160\n",
      "4263/4263 [==============================] - 2s 395us/step - loss: 0.7108 - acc: 0.6936\n",
      "Epoch 136/160\n",
      "4263/4263 [==============================] - 2s 389us/step - loss: 0.7111 - acc: 0.6969\n",
      "Epoch 137/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.7068 - acc: 0.6906\n",
      "Epoch 138/160\n",
      "4263/4263 [==============================] - 2s 389us/step - loss: 0.7071 - acc: 0.6864\n",
      "Epoch 139/160\n",
      "4263/4263 [==============================] - 2s 385us/step - loss: 0.7007 - acc: 0.7011\n",
      "Epoch 140/160\n",
      "4263/4263 [==============================] - 2s 386us/step - loss: 0.7087 - acc: 0.6993\n",
      "Epoch 141/160\n",
      "4263/4263 [==============================] - 2s 389us/step - loss: 0.7153 - acc: 0.6920\n",
      "Epoch 142/160\n",
      "4263/4263 [==============================] - 2s 390us/step - loss: 0.7060 - acc: 0.6943\n",
      "Epoch 143/160\n",
      "4263/4263 [==============================] - 2s 389us/step - loss: 0.7002 - acc: 0.6995\n",
      "Epoch 144/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.7033 - acc: 0.6948\n",
      "Epoch 145/160\n",
      "4263/4263 [==============================] - 2s 390us/step - loss: 0.7013 - acc: 0.6974\n",
      "Epoch 146/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.7112 - acc: 0.6939\n",
      "Epoch 147/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.7014 - acc: 0.6965\n",
      "Epoch 148/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.6913 - acc: 0.6976\n",
      "Epoch 149/160\n",
      "4263/4263 [==============================] - 2s 388us/step - loss: 0.6972 - acc: 0.6983\n",
      "Epoch 150/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.6874 - acc: 0.7021\n",
      "Epoch 151/160\n",
      "4263/4263 [==============================] - 2s 388us/step - loss: 0.7093 - acc: 0.6901\n",
      "Epoch 152/160\n",
      "4263/4263 [==============================] - 2s 389us/step - loss: 0.6992 - acc: 0.6962\n",
      "Epoch 153/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.6878 - acc: 0.7044\n",
      "Epoch 154/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.6892 - acc: 0.7051\n",
      "Epoch 155/160\n",
      "4263/4263 [==============================] - 2s 388us/step - loss: 0.6919 - acc: 0.7089\n",
      "Epoch 156/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.6948 - acc: 0.6946\n",
      "Epoch 157/160\n",
      "4263/4263 [==============================] - 2s 389us/step - loss: 0.7282 - acc: 0.6892\n",
      "Epoch 158/160\n",
      "4263/4263 [==============================] - 2s 385us/step - loss: 0.6962 - acc: 0.6995\n",
      "Epoch 159/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.6935 - acc: 0.7037\n",
      "Epoch 160/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.6841 - acc: 0.7021\n",
      "1066/1066 [==============================] - 2s 1ms/step\n",
      "4263/4263 [==============================] - 1s 143us/step\n",
      "\n",
      "acc: 66.89%\n",
      "\n",
      "acc: 70.16%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df['Class'].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d,y_d,test_size=0.2,random_state=42)\n",
    "\n",
    "y_train_2=pd.get_dummies(y_train)\n",
    "y_test_2=pd.get_dummies(y_test)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(700, activation='relu'))\n",
    "#model.add(Dense(1000, activation='relu'))\n",
    "#model.add(Dense(1200, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=160, callbacks=[early_stopping_monitor],batch_size=50)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "scores_2 = model.evaluate(X_train, y_train_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_2[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/160\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 1.0172 - acc: 0.5766\n",
      "Epoch 2/160\n",
      "4263/4263 [==============================] - 2s 380us/step - loss: 0.9097 - acc: 0.5975\n",
      "Epoch 3/160\n",
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.8757 - acc: 0.6078\n",
      "Epoch 4/160\n",
      "4263/4263 [==============================] - 2s 381us/step - loss: 0.8584 - acc: 0.6298\n",
      "Epoch 5/160\n",
      "4263/4263 [==============================] - 2s 379us/step - loss: 0.8508 - acc: 0.6294\n",
      "Epoch 6/160\n",
      "4263/4263 [==============================] - 2s 442us/step - loss: 0.8414 - acc: 0.6390\n",
      "Epoch 7/160\n",
      "4263/4263 [==============================] - 2s 409us/step - loss: 0.8347 - acc: 0.6341\n",
      "Epoch 8/160\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.8284 - acc: 0.6413\n",
      "Epoch 9/160\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.8195 - acc: 0.6432\n",
      "Epoch 10/160\n",
      "4263/4263 [==============================] - 2s 372us/step - loss: 0.8282 - acc: 0.6425\n",
      "Epoch 11/160\n",
      "4263/4263 [==============================] - 2s 375us/step - loss: 0.8124 - acc: 0.6416\n",
      "Epoch 12/160\n",
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.8080 - acc: 0.6592\n",
      "Epoch 13/160\n",
      "4263/4263 [==============================] - 2s 372us/step - loss: 0.8181 - acc: 0.6465\n",
      "Epoch 14/160\n",
      "4263/4263 [==============================] - 2s 371us/step - loss: 0.8116 - acc: 0.6474\n",
      "Epoch 15/160\n",
      "4263/4263 [==============================] - 2s 374us/step - loss: 0.7947 - acc: 0.6624\n",
      "Epoch 16/160\n",
      "4263/4263 [==============================] - 2s 413us/step - loss: 0.8062 - acc: 0.6538\n",
      "Epoch 17/160\n",
      "4263/4263 [==============================] - 2s 378us/step - loss: 0.7945 - acc: 0.6585\n",
      "Epoch 18/160\n",
      "4263/4263 [==============================] - 2s 382us/step - loss: 0.7916 - acc: 0.6547\n",
      "Epoch 19/160\n",
      "4263/4263 [==============================] - 2s 368us/step - loss: 0.7905 - acc: 0.6521\n",
      "Epoch 20/160\n",
      "4263/4263 [==============================] - 2s 376us/step - loss: 0.7825 - acc: 0.6575\n",
      "Epoch 21/160\n",
      "4263/4263 [==============================] - 2s 381us/step - loss: 0.8004 - acc: 0.6540\n",
      "Epoch 22/160\n",
      "4263/4263 [==============================] - 2s 363us/step - loss: 0.7893 - acc: 0.6526\n",
      "Epoch 23/160\n",
      "4263/4263 [==============================] - 2s 366us/step - loss: 0.7852 - acc: 0.6613\n",
      "Epoch 24/160\n",
      "4263/4263 [==============================] - 2s 372us/step - loss: 0.7839 - acc: 0.6629\n",
      "Epoch 25/160\n",
      "4263/4263 [==============================] - 2s 413us/step - loss: 0.7790 - acc: 0.6594\n",
      "Epoch 26/160\n",
      "4263/4263 [==============================] - 2s 401us/step - loss: 0.7743 - acc: 0.6594\n",
      "Epoch 27/160\n",
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.7781 - acc: 0.6646\n",
      "Epoch 28/160\n",
      "4263/4263 [==============================] - 2s 377us/step - loss: 0.7726 - acc: 0.6657\n",
      "Epoch 29/160\n",
      "4263/4263 [==============================] - 2s 498us/step - loss: 0.7686 - acc: 0.6674\n",
      "Epoch 30/160\n",
      "4263/4263 [==============================] - 2s 448us/step - loss: 0.7710 - acc: 0.6641\n",
      "Epoch 31/160\n",
      "4263/4263 [==============================] - 2s 426us/step - loss: 0.7686 - acc: 0.6648\n",
      "Epoch 32/160\n",
      "4263/4263 [==============================] - 2s 446us/step - loss: 0.7675 - acc: 0.6681\n",
      "Epoch 33/160\n",
      "4263/4263 [==============================] - 2s 498us/step - loss: 0.7655 - acc: 0.6721\n",
      "Epoch 34/160\n",
      "4263/4263 [==============================] - 2s 421us/step - loss: 0.7707 - acc: 0.6615\n",
      "Epoch 35/160\n",
      "4263/4263 [==============================] - 2s 447us/step - loss: 0.7636 - acc: 0.6559\n",
      "Epoch 36/160\n",
      "4263/4263 [==============================] - 2s 449us/step - loss: 0.7655 - acc: 0.6639\n",
      "Epoch 37/160\n",
      "4263/4263 [==============================] - 2s 432us/step - loss: 0.7643 - acc: 0.6690\n",
      "Epoch 38/160\n",
      "4263/4263 [==============================] - 2s 398us/step - loss: 0.7644 - acc: 0.6655\n",
      "Epoch 39/160\n",
      "4263/4263 [==============================] - 2s 382us/step - loss: 0.7550 - acc: 0.6669\n",
      "Epoch 40/160\n",
      "4263/4263 [==============================] - 2s 384us/step - loss: 0.7664 - acc: 0.6601\n",
      "Epoch 41/160\n",
      "4263/4263 [==============================] - 2s 397us/step - loss: 0.7577 - acc: 0.6700\n",
      "Epoch 42/160\n",
      "4263/4263 [==============================] - 2s 386us/step - loss: 0.7583 - acc: 0.6721\n",
      "Epoch 43/160\n",
      "4263/4263 [==============================] - 2s 394us/step - loss: 0.7597 - acc: 0.6714\n",
      "Epoch 44/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.7583 - acc: 0.6676\n",
      "Epoch 45/160\n",
      "4263/4263 [==============================] - 2s 390us/step - loss: 0.7597 - acc: 0.6671\n",
      "Epoch 46/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.7525 - acc: 0.6728\n",
      "Epoch 47/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.7585 - acc: 0.6721\n",
      "Epoch 48/160\n",
      "4263/4263 [==============================] - 2s 385us/step - loss: 0.7558 - acc: 0.6716\n",
      "Epoch 49/160\n",
      "4263/4263 [==============================] - 2s 388us/step - loss: 0.7530 - acc: 0.6751\n",
      "Epoch 50/160\n",
      "4263/4263 [==============================] - 2s 390us/step - loss: 0.7564 - acc: 0.6744\n",
      "Epoch 51/160\n",
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.7500 - acc: 0.6692\n",
      "Epoch 52/160\n",
      "4263/4263 [==============================] - 2s 386us/step - loss: 0.7455 - acc: 0.6692\n",
      "Epoch 53/160\n",
      "4263/4263 [==============================] - 2s 398us/step - loss: 0.7485 - acc: 0.6753\n",
      "Epoch 54/160\n",
      "4263/4263 [==============================] - 2s 377us/step - loss: 0.7543 - acc: 0.6725\n",
      "Epoch 55/160\n",
      "4263/4263 [==============================] - 2s 368us/step - loss: 0.7562 - acc: 0.6702\n",
      "Epoch 56/160\n",
      "4263/4263 [==============================] - 2s 372us/step - loss: 0.7514 - acc: 0.6676\n",
      "Epoch 57/160\n",
      "4263/4263 [==============================] - 2s 373us/step - loss: 0.7489 - acc: 0.6742\n",
      "Epoch 58/160\n",
      "4263/4263 [==============================] - 2s 368us/step - loss: 0.7528 - acc: 0.6758\n",
      "Epoch 59/160\n",
      "4263/4263 [==============================] - 2s 373us/step - loss: 0.7504 - acc: 0.6779\n",
      "Epoch 60/160\n",
      "4263/4263 [==============================] - 2s 364us/step - loss: 0.7424 - acc: 0.6753\n",
      "Epoch 61/160\n",
      "4263/4263 [==============================] - 2s 369us/step - loss: 0.7463 - acc: 0.6737\n",
      "Epoch 62/160\n",
      "4263/4263 [==============================] - 2s 369us/step - loss: 0.7475 - acc: 0.6789\n",
      "Epoch 63/160\n",
      "4263/4263 [==============================] - 2s 366us/step - loss: 0.7401 - acc: 0.6791\n",
      "Epoch 64/160\n",
      "4263/4263 [==============================] - 2s 362us/step - loss: 0.7435 - acc: 0.6751\n",
      "Epoch 65/160\n",
      "4263/4263 [==============================] - 2s 374us/step - loss: 0.7402 - acc: 0.6803\n",
      "Epoch 66/160\n",
      "4263/4263 [==============================] - 2s 373us/step - loss: 0.7417 - acc: 0.6782\n",
      "Epoch 67/160\n",
      "4263/4263 [==============================] - 2s 358us/step - loss: 0.7384 - acc: 0.6798\n",
      "Epoch 68/160\n",
      "4263/4263 [==============================] - 2s 370us/step - loss: 0.7342 - acc: 0.6807\n",
      "Epoch 69/160\n",
      "4263/4263 [==============================] - 2s 394us/step - loss: 0.7373 - acc: 0.6758\n",
      "Epoch 70/160\n",
      "4263/4263 [==============================] - 2s 379us/step - loss: 0.7359 - acc: 0.6836\n",
      "Epoch 71/160\n",
      "4263/4263 [==============================] - 2s 375us/step - loss: 0.7444 - acc: 0.6765\n",
      "Epoch 72/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.7362 - acc: 0.6786 1s \n",
      "Epoch 73/160\n",
      "4263/4263 [==============================] - 2s 362us/step - loss: 0.7422 - acc: 0.6770\n",
      "Epoch 74/160\n",
      "4263/4263 [==============================] - 2s 363us/step - loss: 0.7393 - acc: 0.6753\n",
      "Epoch 75/160\n",
      "4263/4263 [==============================] - 2s 369us/step - loss: 0.7288 - acc: 0.6791\n",
      "Epoch 76/160\n",
      "4263/4263 [==============================] - 2s 363us/step - loss: 0.7320 - acc: 0.6840\n",
      "Epoch 77/160\n",
      "4263/4263 [==============================] - 2s 362us/step - loss: 0.7284 - acc: 0.6859\n",
      "Epoch 78/160\n",
      "4263/4263 [==============================] - 2s 360us/step - loss: 0.7322 - acc: 0.6786\n",
      "Epoch 79/160\n",
      "4263/4263 [==============================] - 2s 360us/step - loss: 0.7243 - acc: 0.6852\n",
      "Epoch 80/160\n",
      "4263/4263 [==============================] - 2s 389us/step - loss: 0.7346 - acc: 0.6873\n",
      "Epoch 81/160\n",
      "4263/4263 [==============================] - 2s 388us/step - loss: 0.7367 - acc: 0.6840\n",
      "Epoch 82/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 2s 382us/step - loss: 0.7294 - acc: 0.6807\n",
      "Epoch 83/160\n",
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.7196 - acc: 0.6953\n",
      "Epoch 84/160\n",
      "4263/4263 [==============================] - 2s 379us/step - loss: 0.7229 - acc: 0.6838\n",
      "Epoch 85/160\n",
      "4263/4263 [==============================] - 2s 378us/step - loss: 0.7227 - acc: 0.6861\n",
      "Epoch 86/160\n",
      "4263/4263 [==============================] - 2s 370us/step - loss: 0.7257 - acc: 0.6819\n",
      "Epoch 87/160\n",
      "4263/4263 [==============================] - 2s 496us/step - loss: 0.7203 - acc: 0.6918\n",
      "Epoch 88/160\n",
      "4263/4263 [==============================] - 2s 413us/step - loss: 0.7160 - acc: 0.6887\n",
      "Epoch 89/160\n",
      "4263/4263 [==============================] - 2s 397us/step - loss: 0.7310 - acc: 0.6810\n",
      "Epoch 90/160\n",
      "4263/4263 [==============================] - 2s 436us/step - loss: 0.7205 - acc: 0.6901\n",
      "Epoch 91/160\n",
      "4263/4263 [==============================] - 2s 415us/step - loss: 0.7209 - acc: 0.6915\n",
      "Epoch 92/160\n",
      "4263/4263 [==============================] - 2s 373us/step - loss: 0.7256 - acc: 0.6875\n",
      "Epoch 93/160\n",
      "4263/4263 [==============================] - 2s 463us/step - loss: 0.7331 - acc: 0.6777\n",
      "Epoch 94/160\n",
      "4263/4263 [==============================] - 2s 427us/step - loss: 0.7153 - acc: 0.6929\n",
      "Epoch 95/160\n",
      "4263/4263 [==============================] - 2s 453us/step - loss: 0.7200 - acc: 0.6871\n",
      "Epoch 96/160\n",
      "4263/4263 [==============================] - 2s 418us/step - loss: 0.7170 - acc: 0.6904\n",
      "Epoch 97/160\n",
      "4263/4263 [==============================] - 2s 479us/step - loss: 0.7306 - acc: 0.6826\n",
      "Epoch 98/160\n",
      "4263/4263 [==============================] - 2s 447us/step - loss: 0.7155 - acc: 0.6892\n",
      "Epoch 99/160\n",
      "4263/4263 [==============================] - 2s 421us/step - loss: 0.7150 - acc: 0.6927\n",
      "Epoch 100/160\n",
      "4263/4263 [==============================] - 2s 412us/step - loss: 0.7184 - acc: 0.6899\n",
      "Epoch 101/160\n",
      "4263/4263 [==============================] - 2s 437us/step - loss: 0.7066 - acc: 0.6906\n",
      "Epoch 102/160\n",
      "4263/4263 [==============================] - 2s 434us/step - loss: 0.7106 - acc: 0.6929\n",
      "Epoch 103/160\n",
      "4263/4263 [==============================] - 2s 409us/step - loss: 0.7081 - acc: 0.6960\n",
      "Epoch 104/160\n",
      "4263/4263 [==============================] - 2s 413us/step - loss: 0.7124 - acc: 0.6922\n",
      "Epoch 105/160\n",
      "4263/4263 [==============================] - 2s 399us/step - loss: 0.7056 - acc: 0.6892\n",
      "Epoch 106/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.7101 - acc: 0.6901\n",
      "Epoch 107/160\n",
      "4263/4263 [==============================] - 2s 402us/step - loss: 0.7042 - acc: 0.6946\n",
      "Epoch 108/160\n",
      "4263/4263 [==============================] - 2s 389us/step - loss: 0.7088 - acc: 0.6932\n",
      "Epoch 109/160\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.7046 - acc: 0.6943\n",
      "Epoch 110/160\n",
      "4263/4263 [==============================] - 2s 496us/step - loss: 0.7171 - acc: 0.6979\n",
      "Epoch 111/160\n",
      "4263/4263 [==============================] - 2s 490us/step - loss: 0.7052 - acc: 0.6951\n",
      "Epoch 112/160\n",
      "4263/4263 [==============================] - 2s 459us/step - loss: 0.7035 - acc: 0.7033\n",
      "Epoch 113/160\n",
      "4263/4263 [==============================] - 2s 396us/step - loss: 0.7034 - acc: 0.6948\n",
      "Epoch 114/160\n",
      "4263/4263 [==============================] - 2s 386us/step - loss: 0.7067 - acc: 0.6906\n",
      "Epoch 115/160\n",
      "4263/4263 [==============================] - 2s 399us/step - loss: 0.6951 - acc: 0.7019\n",
      "Epoch 116/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.6921 - acc: 0.6988\n",
      "Epoch 117/160\n",
      "4263/4263 [==============================] - 2s 396us/step - loss: 0.6994 - acc: 0.6941\n",
      "Epoch 118/160\n",
      "4263/4263 [==============================] - 2s 394us/step - loss: 0.6989 - acc: 0.6967\n",
      "Epoch 119/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.6973 - acc: 0.6965\n",
      "Epoch 120/160\n",
      "4263/4263 [==============================] - 2s 399us/step - loss: 0.6856 - acc: 0.7011\n",
      "Epoch 121/160\n",
      "4263/4263 [==============================] - 2s 390us/step - loss: 0.6996 - acc: 0.7002\n",
      "Epoch 122/160\n",
      "4263/4263 [==============================] - 2s 388us/step - loss: 0.6920 - acc: 0.6929\n",
      "Epoch 123/160\n",
      "4263/4263 [==============================] - 2s 394us/step - loss: 0.6997 - acc: 0.6997\n",
      "Epoch 124/160\n",
      "4263/4263 [==============================] - 2s 400us/step - loss: 0.6823 - acc: 0.7054\n",
      "Epoch 125/160\n",
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.6841 - acc: 0.7030\n",
      "Epoch 126/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.6891 - acc: 0.7033\n",
      "Epoch 127/160\n",
      "4263/4263 [==============================] - 2s 396us/step - loss: 0.6845 - acc: 0.7028\n",
      "Epoch 128/160\n",
      "4263/4263 [==============================] - 2s 385us/step - loss: 0.6970 - acc: 0.6965\n",
      "Epoch 129/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.6811 - acc: 0.7040\n",
      "Epoch 130/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.7042 - acc: 0.6934\n",
      "Epoch 131/160\n",
      "4263/4263 [==============================] - 2s 396us/step - loss: 0.6853 - acc: 0.7019\n",
      "Epoch 132/160\n",
      "4263/4263 [==============================] - 2s 402us/step - loss: 0.6885 - acc: 0.7023\n",
      "Epoch 133/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.7028 - acc: 0.6934\n",
      "Epoch 134/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.6937 - acc: 0.6967\n",
      "Epoch 135/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.6784 - acc: 0.7021\n",
      "Epoch 136/160\n",
      "4263/4263 [==============================] - 2s 403us/step - loss: 0.6878 - acc: 0.6993\n",
      "Epoch 137/160\n",
      "4263/4263 [==============================] - 2s 405us/step - loss: 0.6797 - acc: 0.7019\n",
      "Epoch 138/160\n",
      "4263/4263 [==============================] - 2s 411us/step - loss: 0.6928 - acc: 0.6908\n",
      "Epoch 139/160\n",
      "4263/4263 [==============================] - 2s 394us/step - loss: 0.6780 - acc: 0.7021\n",
      "Epoch 140/160\n",
      "4263/4263 [==============================] - 2s 390us/step - loss: 0.6741 - acc: 0.7091\n",
      "Epoch 141/160\n",
      "4263/4263 [==============================] - 2s 395us/step - loss: 0.6782 - acc: 0.7000\n",
      "Epoch 142/160\n",
      "4263/4263 [==============================] - 2s 403us/step - loss: 0.6690 - acc: 0.7061\n",
      "Epoch 143/160\n",
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.6777 - acc: 0.7044\n",
      "Epoch 144/160\n",
      "4263/4263 [==============================] - 2s 382us/step - loss: 0.6823 - acc: 0.7037\n",
      "Epoch 145/160\n",
      "4263/4263 [==============================] - 2s 404us/step - loss: 0.6830 - acc: 0.7002\n",
      "Epoch 146/160\n",
      "4263/4263 [==============================] - 2s 398us/step - loss: 0.6706 - acc: 0.7136\n",
      "Epoch 147/160\n",
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.6701 - acc: 0.7112\n",
      "Epoch 148/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.6764 - acc: 0.7089\n",
      "Epoch 149/160\n",
      "4263/4263 [==============================] - 2s 390us/step - loss: 0.6711 - acc: 0.7021\n",
      "Epoch 150/160\n",
      "4263/4263 [==============================] - 2s 386us/step - loss: 0.6762 - acc: 0.7112\n",
      "Epoch 151/160\n",
      "4263/4263 [==============================] - 2s 395us/step - loss: 0.6649 - acc: 0.7101\n",
      "Epoch 152/160\n",
      "4263/4263 [==============================] - 2s 407us/step - loss: 0.6740 - acc: 0.7040\n",
      "Epoch 153/160\n",
      "4263/4263 [==============================] - 2s 394us/step - loss: 0.6791 - acc: 0.7035\n",
      "Epoch 154/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.6627 - acc: 0.7051\n",
      "Epoch 155/160\n",
      "4263/4263 [==============================] - 2s 419us/step - loss: 0.6674 - acc: 0.7096\n",
      "Epoch 156/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.6509 - acc: 0.7159\n",
      "Epoch 157/160\n",
      "4263/4263 [==============================] - 2s 406us/step - loss: 0.6541 - acc: 0.7133\n",
      "Epoch 158/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.6796 - acc: 0.7016\n",
      "Epoch 159/160\n",
      "4263/4263 [==============================] - 2s 524us/step - loss: 0.6762 - acc: 0.7115\n",
      "Epoch 160/160\n",
      "4263/4263 [==============================] - 2s 420us/step - loss: 0.6572 - acc: 0.7129\n",
      "1066/1066 [==============================] - 2s 2ms/step\n",
      "4263/4263 [==============================] - 1s 204us/step\n",
      "\n",
      "acc: 68.20%\n",
      "\n",
      "acc: 72.88%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df['Class'].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d,y_d,test_size=0.2,random_state=42)\n",
    "\n",
    "y_train_2=pd.get_dummies(y_train)\n",
    "y_test_2=pd.get_dummies(y_test)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(700, activation='relu'))\n",
    "#model.add(Dense(1000, activation='relu'))\n",
    "#model.add(Dense(1200, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=160, callbacks=[early_stopping_monitor],batch_size=50)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "scores_2 = model.evaluate(X_train, y_train_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_2[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/160\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 1.0206 - acc: 0.5660\n",
      "Epoch 2/160\n",
      "4263/4263 [==============================] - 2s 497us/step - loss: 0.9006 - acc: 0.6026\n",
      "Epoch 3/160\n",
      "4263/4263 [==============================] - 2s 438us/step - loss: 0.8669 - acc: 0.6172\n",
      "Epoch 4/160\n",
      "4263/4263 [==============================] - 2s 371us/step - loss: 0.8490 - acc: 0.6289\n",
      "Epoch 5/160\n",
      "4263/4263 [==============================] - 2s 361us/step - loss: 0.8449 - acc: 0.6388\n",
      "Epoch 6/160\n",
      "4263/4263 [==============================] - 2s 364us/step - loss: 0.8476 - acc: 0.6305\n",
      "Epoch 7/160\n",
      "4263/4263 [==============================] - 2s 366us/step - loss: 0.8390 - acc: 0.6359\n",
      "Epoch 8/160\n",
      "4263/4263 [==============================] - 2s 361us/step - loss: 0.8330 - acc: 0.6418\n",
      "Epoch 9/160\n",
      "4263/4263 [==============================] - 2s 363us/step - loss: 0.8327 - acc: 0.6411\n",
      "Epoch 10/160\n",
      "4263/4263 [==============================] - 2s 355us/step - loss: 0.8158 - acc: 0.6420\n",
      "Epoch 11/160\n",
      "4263/4263 [==============================] - 2s 364us/step - loss: 0.8107 - acc: 0.6446\n",
      "Epoch 12/160\n",
      "4263/4263 [==============================] - 2s 360us/step - loss: 0.8138 - acc: 0.6512\n",
      "Epoch 13/160\n",
      "4263/4263 [==============================] - 2s 367us/step - loss: 0.8194 - acc: 0.6486\n",
      "Epoch 14/160\n",
      "4263/4263 [==============================] - 2s 362us/step - loss: 0.8048 - acc: 0.6592\n",
      "Epoch 15/160\n",
      "4263/4263 [==============================] - 2s 365us/step - loss: 0.8062 - acc: 0.6545\n",
      "Epoch 16/160\n",
      "4263/4263 [==============================] - 2s 366us/step - loss: 0.7995 - acc: 0.6542\n",
      "Epoch 17/160\n",
      "4263/4263 [==============================] - 1s 302us/step - loss: 0.7922 - acc: 0.6608\n",
      "Epoch 18/160\n",
      "4263/4263 [==============================] - 1s 292us/step - loss: 0.7868 - acc: 0.6587\n",
      "Epoch 19/160\n",
      "4263/4263 [==============================] - 1s 278us/step - loss: 0.7882 - acc: 0.6610\n",
      "Epoch 20/160\n",
      "4263/4263 [==============================] - 1s 288us/step - loss: 0.7876 - acc: 0.6573\n",
      "Epoch 21/160\n",
      "4263/4263 [==============================] - 1s 282us/step - loss: 0.7928 - acc: 0.6606\n",
      "Epoch 22/160\n",
      "4263/4263 [==============================] - 1s 283us/step - loss: 0.7853 - acc: 0.6606\n",
      "Epoch 23/160\n",
      "4263/4263 [==============================] - 1s 289us/step - loss: 0.7721 - acc: 0.6657\n",
      "Epoch 24/160\n",
      "4263/4263 [==============================] - 1s 286us/step - loss: 0.7770 - acc: 0.6620\n",
      "Epoch 25/160\n",
      "4263/4263 [==============================] - 1s 287us/step - loss: 0.7780 - acc: 0.6570\n",
      "Epoch 26/160\n",
      "4263/4263 [==============================] - 1s 281us/step - loss: 0.7780 - acc: 0.6608\n",
      "Epoch 27/160\n",
      "4263/4263 [==============================] - 1s 285us/step - loss: 0.7733 - acc: 0.6615\n",
      "Epoch 28/160\n",
      "4263/4263 [==============================] - 1s 285us/step - loss: 0.7759 - acc: 0.6624\n",
      "Epoch 29/160\n",
      "4263/4263 [==============================] - 1s 288us/step - loss: 0.7709 - acc: 0.6641\n",
      "Epoch 30/160\n",
      "4263/4263 [==============================] - 1s 282us/step - loss: 0.7745 - acc: 0.6617\n",
      "Epoch 31/160\n",
      "4263/4263 [==============================] - 1s 283us/step - loss: 0.7733 - acc: 0.6587\n",
      "Epoch 32/160\n",
      "4263/4263 [==============================] - 1s 290us/step - loss: 0.7821 - acc: 0.6594\n",
      "Epoch 33/160\n",
      "4263/4263 [==============================] - 1s 282us/step - loss: 0.7685 - acc: 0.6606\n",
      "Epoch 34/160\n",
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.7731 - acc: 0.6636\n",
      "Epoch 35/160\n",
      "4263/4263 [==============================] - 1s 307us/step - loss: 0.7760 - acc: 0.6650\n",
      "Epoch 36/160\n",
      "4263/4263 [==============================] - 1s 285us/step - loss: 0.7686 - acc: 0.6664\n",
      "Epoch 37/160\n",
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.7667 - acc: 0.6631\n",
      "Epoch 38/160\n",
      "4263/4263 [==============================] - 1s 285us/step - loss: 0.7736 - acc: 0.6639\n",
      "Epoch 39/160\n",
      "4263/4263 [==============================] - 1s 281us/step - loss: 0.7685 - acc: 0.6653\n",
      "Epoch 40/160\n",
      "4263/4263 [==============================] - 1s 286us/step - loss: 0.7694 - acc: 0.6615\n",
      "Epoch 41/160\n",
      "4263/4263 [==============================] - 1s 282us/step - loss: 0.7648 - acc: 0.6697\n",
      "Epoch 42/160\n",
      "4263/4263 [==============================] - 1s 281us/step - loss: 0.7602 - acc: 0.6636\n",
      "Epoch 43/160\n",
      "4263/4263 [==============================] - 1s 282us/step - loss: 0.7566 - acc: 0.6615\n",
      "Epoch 44/160\n",
      "4263/4263 [==============================] - 1s 285us/step - loss: 0.7587 - acc: 0.6739\n",
      "Epoch 45/160\n",
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.7551 - acc: 0.6765\n",
      "Epoch 46/160\n",
      "4263/4263 [==============================] - 1s 280us/step - loss: 0.7550 - acc: 0.6704\n",
      "Epoch 47/160\n",
      "4263/4263 [==============================] - 1s 283us/step - loss: 0.7620 - acc: 0.6641\n",
      "Epoch 48/160\n",
      "4263/4263 [==============================] - 1s 286us/step - loss: 0.7602 - acc: 0.6709\n",
      "Epoch 49/160\n",
      "4263/4263 [==============================] - 1s 280us/step - loss: 0.7539 - acc: 0.6702\n",
      "Epoch 50/160\n",
      "4263/4263 [==============================] - 1s 281us/step - loss: 0.7584 - acc: 0.6676\n",
      "Epoch 51/160\n",
      "4263/4263 [==============================] - 1s 279us/step - loss: 0.7622 - acc: 0.6700\n",
      "Epoch 52/160\n",
      "4263/4263 [==============================] - 1s 287us/step - loss: 0.7618 - acc: 0.6709\n",
      "Epoch 53/160\n",
      "4263/4263 [==============================] - 1s 292us/step - loss: 0.7560 - acc: 0.6735\n",
      "Epoch 54/160\n",
      "4263/4263 [==============================] - 1s 277us/step - loss: 0.7513 - acc: 0.6779\n",
      "Epoch 55/160\n",
      "4263/4263 [==============================] - 1s 280us/step - loss: 0.7558 - acc: 0.6655\n",
      "Epoch 56/160\n",
      "4263/4263 [==============================] - 1s 282us/step - loss: 0.7597 - acc: 0.6636\n",
      "Epoch 57/160\n",
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.7489 - acc: 0.6730\n",
      "Epoch 58/160\n",
      "4263/4263 [==============================] - 1s 281us/step - loss: 0.7537 - acc: 0.6742\n",
      "Epoch 59/160\n",
      "4263/4263 [==============================] - 1s 288us/step - loss: 0.7495 - acc: 0.6758\n",
      "Epoch 60/160\n",
      "4263/4263 [==============================] - 1s 281us/step - loss: 0.7472 - acc: 0.6732\n",
      "Epoch 61/160\n",
      "4263/4263 [==============================] - 1s 292us/step - loss: 0.7574 - acc: 0.6697\n",
      "Epoch 62/160\n",
      "4263/4263 [==============================] - 1s 282us/step - loss: 0.7447 - acc: 0.6728\n",
      "Epoch 63/160\n",
      "4263/4263 [==============================] - 1s 282us/step - loss: 0.7411 - acc: 0.6768\n",
      "Epoch 64/160\n",
      "4263/4263 [==============================] - 1s 283us/step - loss: 0.7457 - acc: 0.6744\n",
      "Epoch 65/160\n",
      "4263/4263 [==============================] - 1s 343us/step - loss: 0.7437 - acc: 0.6775\n",
      "Epoch 66/160\n",
      "4263/4263 [==============================] - 1s 309us/step - loss: 0.7382 - acc: 0.6772\n",
      "Epoch 67/160\n",
      "4263/4263 [==============================] - 1s 311us/step - loss: 0.7425 - acc: 0.6789\n",
      "Epoch 68/160\n",
      "4263/4263 [==============================] - 1s 317us/step - loss: 0.7411 - acc: 0.6791\n",
      "Epoch 69/160\n",
      "4263/4263 [==============================] - 1s 304us/step - loss: 0.7472 - acc: 0.6718\n",
      "Epoch 70/160\n",
      "4263/4263 [==============================] - 1s 288us/step - loss: 0.7440 - acc: 0.6775\n",
      "Epoch 71/160\n",
      "4263/4263 [==============================] - 1s 296us/step - loss: 0.7483 - acc: 0.6770\n",
      "Epoch 72/160\n",
      "4263/4263 [==============================] - 1s 295us/step - loss: 0.7550 - acc: 0.6714\n",
      "Epoch 73/160\n",
      "4263/4263 [==============================] - 1s 311us/step - loss: 0.7428 - acc: 0.6807\n",
      "Epoch 74/160\n",
      "4263/4263 [==============================] - 1s 330us/step - loss: 0.7410 - acc: 0.6789\n",
      "Epoch 75/160\n",
      "4263/4263 [==============================] - 1s 296us/step - loss: 0.7485 - acc: 0.6819\n",
      "Epoch 76/160\n",
      "4263/4263 [==============================] - 1s 299us/step - loss: 0.7375 - acc: 0.6840\n",
      "Epoch 77/160\n",
      "4263/4263 [==============================] - 1s 290us/step - loss: 0.7398 - acc: 0.6798\n",
      "Epoch 78/160\n",
      "4263/4263 [==============================] - 1s 286us/step - loss: 0.7412 - acc: 0.6751\n",
      "Epoch 79/160\n",
      "4263/4263 [==============================] - 1s 287us/step - loss: 0.7399 - acc: 0.6791\n",
      "Epoch 80/160\n",
      "4263/4263 [==============================] - 1s 296us/step - loss: 0.7368 - acc: 0.6798\n",
      "Epoch 81/160\n",
      "4263/4263 [==============================] - 1s 293us/step - loss: 0.7321 - acc: 0.6824\n",
      "Epoch 82/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 1s 286us/step - loss: 0.7331 - acc: 0.6838\n",
      "Epoch 83/160\n",
      "4263/4263 [==============================] - 1s 285us/step - loss: 0.7347 - acc: 0.6850\n",
      "Epoch 84/160\n",
      "4263/4263 [==============================] - 1s 287us/step - loss: 0.7282 - acc: 0.6866\n",
      "Epoch 85/160\n",
      "4263/4263 [==============================] - 1s 283us/step - loss: 0.7317 - acc: 0.6826\n",
      "Epoch 86/160\n",
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.7345 - acc: 0.6854\n",
      "Epoch 87/160\n",
      "4263/4263 [==============================] - 1s 282us/step - loss: 0.7431 - acc: 0.6812\n",
      "Epoch 88/160\n",
      "4263/4263 [==============================] - 1s 286us/step - loss: 0.7385 - acc: 0.6796\n",
      "Epoch 89/160\n",
      "4263/4263 [==============================] - 1s 288us/step - loss: 0.7360 - acc: 0.6840\n",
      "Epoch 90/160\n",
      "4263/4263 [==============================] - 1s 282us/step - loss: 0.7296 - acc: 0.6803\n",
      "Epoch 91/160\n",
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.7303 - acc: 0.6871\n",
      "Epoch 92/160\n",
      "4263/4263 [==============================] - 1s 289us/step - loss: 0.7335 - acc: 0.6782\n",
      "Epoch 93/160\n",
      "4263/4263 [==============================] - 1s 287us/step - loss: 0.7238 - acc: 0.6821\n",
      "Epoch 94/160\n",
      "4263/4263 [==============================] - 1s 292us/step - loss: 0.7226 - acc: 0.6826\n",
      "Epoch 95/160\n",
      "4263/4263 [==============================] - 1s 287us/step - loss: 0.7266 - acc: 0.6861\n",
      "Epoch 96/160\n",
      "4263/4263 [==============================] - 1s 287us/step - loss: 0.7230 - acc: 0.6850\n",
      "Epoch 97/160\n",
      "4263/4263 [==============================] - 1s 285us/step - loss: 0.7237 - acc: 0.6850\n",
      "Epoch 98/160\n",
      "4263/4263 [==============================] - 1s 285us/step - loss: 0.7214 - acc: 0.6885\n",
      "Epoch 99/160\n",
      "4263/4263 [==============================] - 1s 283us/step - loss: 0.7171 - acc: 0.6927\n",
      "Epoch 100/160\n",
      "4263/4263 [==============================] - 1s 286us/step - loss: 0.7274 - acc: 0.6873\n",
      "Epoch 101/160\n",
      "4263/4263 [==============================] - 1s 289us/step - loss: 0.7259 - acc: 0.6829\n",
      "Epoch 102/160\n",
      "4263/4263 [==============================] - 1s 287us/step - loss: 0.7335 - acc: 0.6859\n",
      "Epoch 103/160\n",
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.7203 - acc: 0.6890\n",
      "Epoch 104/160\n",
      "4263/4263 [==============================] - 1s 282us/step - loss: 0.7245 - acc: 0.6861\n",
      "Epoch 105/160\n",
      "4263/4263 [==============================] - 1s 286us/step - loss: 0.7296 - acc: 0.6852\n",
      "Epoch 106/160\n",
      "4263/4263 [==============================] - 1s 282us/step - loss: 0.7227 - acc: 0.6845\n",
      "Epoch 107/160\n",
      "4263/4263 [==============================] - 1s 285us/step - loss: 0.7211 - acc: 0.6948\n",
      "Epoch 108/160\n",
      "4263/4263 [==============================] - 1s 286us/step - loss: 0.7187 - acc: 0.6927\n",
      "Epoch 109/160\n",
      "4263/4263 [==============================] - 1s 289us/step - loss: 0.7193 - acc: 0.6866\n",
      "Epoch 110/160\n",
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.7243 - acc: 0.6789\n",
      "Epoch 111/160\n",
      "4263/4263 [==============================] - 1s 285us/step - loss: 0.7097 - acc: 0.6929\n",
      "Epoch 112/160\n",
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.7097 - acc: 0.6932\n",
      "Epoch 113/160\n",
      "4263/4263 [==============================] - 1s 293us/step - loss: 0.7261 - acc: 0.6861\n",
      "Epoch 114/160\n",
      "4263/4263 [==============================] - 1s 283us/step - loss: 0.7150 - acc: 0.6878\n",
      "Epoch 115/160\n",
      "4263/4263 [==============================] - 1s 287us/step - loss: 0.7139 - acc: 0.6908\n",
      "Epoch 116/160\n",
      "4263/4263 [==============================] - 1s 283us/step - loss: 0.7073 - acc: 0.6979\n",
      "Epoch 117/160\n",
      "4263/4263 [==============================] - 1s 291us/step - loss: 0.7111 - acc: 0.6941\n",
      "Epoch 118/160\n",
      "4263/4263 [==============================] - 1s 284us/step - loss: 0.7131 - acc: 0.6821\n",
      "Epoch 119/160\n",
      "4263/4263 [==============================] - 1s 283us/step - loss: 0.7146 - acc: 0.6875\n",
      "Epoch 120/160\n",
      "4263/4263 [==============================] - 1s 286us/step - loss: 0.7099 - acc: 0.6981\n",
      "Epoch 121/160\n",
      "4263/4263 [==============================] - 1s 310us/step - loss: 0.7073 - acc: 0.6946\n",
      "Epoch 122/160\n",
      "4263/4263 [==============================] - 1s 327us/step - loss: 0.7134 - acc: 0.6960\n",
      "Epoch 123/160\n",
      "4263/4263 [==============================] - 1s 294us/step - loss: 0.7056 - acc: 0.6932\n",
      "Epoch 124/160\n",
      "4263/4263 [==============================] - 1s 292us/step - loss: 0.7034 - acc: 0.6958\n",
      "Epoch 125/160\n",
      "4263/4263 [==============================] - 1s 304us/step - loss: 0.7024 - acc: 0.6995\n",
      "Epoch 126/160\n",
      "4263/4263 [==============================] - 1s 292us/step - loss: 0.7109 - acc: 0.6878\n",
      "Epoch 127/160\n",
      "4263/4263 [==============================] - 1s 296us/step - loss: 0.7055 - acc: 0.6911\n",
      "Epoch 128/160\n",
      "4263/4263 [==============================] - 1s 299us/step - loss: 0.7012 - acc: 0.6908\n",
      "Epoch 129/160\n",
      "4263/4263 [==============================] - 1s 300us/step - loss: 0.7202 - acc: 0.6887\n",
      "Epoch 130/160\n",
      "4263/4263 [==============================] - 1s 296us/step - loss: 0.7153 - acc: 0.6885\n",
      "Epoch 131/160\n",
      "4263/4263 [==============================] - 1s 298us/step - loss: 0.7110 - acc: 0.6894\n",
      "Epoch 132/160\n",
      "4263/4263 [==============================] - 1s 292us/step - loss: 0.7173 - acc: 0.6847\n",
      "Epoch 133/160\n",
      "4263/4263 [==============================] - 1s 337us/step - loss: 0.7077 - acc: 0.6927\n",
      "Epoch 134/160\n",
      "4263/4263 [==============================] - 2s 372us/step - loss: 0.7004 - acc: 0.6946\n",
      "Epoch 135/160\n",
      "4263/4263 [==============================] - 2s 379us/step - loss: 0.6960 - acc: 0.6927\n",
      "Epoch 136/160\n",
      "4263/4263 [==============================] - 2s 399us/step - loss: 0.6930 - acc: 0.6904\n",
      "Epoch 137/160\n",
      "4263/4263 [==============================] - 2s 372us/step - loss: 0.7043 - acc: 0.6943\n",
      "Epoch 138/160\n",
      "4263/4263 [==============================] - 2s 372us/step - loss: 0.7039 - acc: 0.6934\n",
      "Epoch 139/160\n",
      "4263/4263 [==============================] - 2s 374us/step - loss: 0.7056 - acc: 0.6974\n",
      "Epoch 140/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.6917 - acc: 0.6979\n",
      "Epoch 141/160\n",
      "4263/4263 [==============================] - 2s 412us/step - loss: 0.6992 - acc: 0.6953\n",
      "Epoch 142/160\n",
      "4263/4263 [==============================] - 2s 495us/step - loss: 0.7136 - acc: 0.6906\n",
      "Epoch 143/160\n",
      "4263/4263 [==============================] - 2s 508us/step - loss: 0.7122 - acc: 0.6897\n",
      "Epoch 144/160\n",
      "4263/4263 [==============================] - 2s 446us/step - loss: 0.6922 - acc: 0.6983\n",
      "Epoch 145/160\n",
      "4263/4263 [==============================] - 2s 412us/step - loss: 0.7052 - acc: 0.6936\n",
      "Epoch 146/160\n",
      "4263/4263 [==============================] - 2s 419us/step - loss: 0.6929 - acc: 0.6969\n",
      "Epoch 147/160\n",
      "4263/4263 [==============================] - 2s 405us/step - loss: 0.6951 - acc: 0.6953\n",
      "Epoch 148/160\n",
      "4263/4263 [==============================] - 2s 430us/step - loss: 0.6922 - acc: 0.6974\n",
      "Epoch 149/160\n",
      "4263/4263 [==============================] - 2s 431us/step - loss: 0.6928 - acc: 0.6972\n",
      "Epoch 150/160\n",
      "4263/4263 [==============================] - 2s 454us/step - loss: 0.6881 - acc: 0.6995\n",
      "Epoch 151/160\n",
      "4263/4263 [==============================] - 2s 416us/step - loss: 0.6895 - acc: 0.7049\n",
      "Epoch 152/160\n",
      "4263/4263 [==============================] - 2s 376us/step - loss: 0.6835 - acc: 0.7019\n",
      "Epoch 153/160\n",
      "4263/4263 [==============================] - 2s 384us/step - loss: 0.7065 - acc: 0.6890\n",
      "Epoch 154/160\n",
      "4263/4263 [==============================] - 2s 381us/step - loss: 0.6927 - acc: 0.6979\n",
      "Epoch 155/160\n",
      "4263/4263 [==============================] - 2s 374us/step - loss: 0.6829 - acc: 0.7011\n",
      "Epoch 156/160\n",
      "4263/4263 [==============================] - 2s 373us/step - loss: 0.6925 - acc: 0.7042\n",
      "Epoch 157/160\n",
      "4263/4263 [==============================] - 2s 473us/step - loss: 0.6937 - acc: 0.6953\n",
      "Epoch 158/160\n",
      "4263/4263 [==============================] - 2s 463us/step - loss: 0.6901 - acc: 0.7033\n",
      "Epoch 159/160\n",
      "4263/4263 [==============================] - 2s 441us/step - loss: 0.6832 - acc: 0.7042\n",
      "Epoch 160/160\n",
      "4263/4263 [==============================] - 2s 394us/step - loss: 0.6859 - acc: 0.7037\n",
      "1066/1066 [==============================] - 2s 2ms/step\n",
      "4263/4263 [==============================] - 1s 179us/step\n",
      "\n",
      "acc: 65.57%\n",
      "\n",
      "acc: 70.35%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df['Class'].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d,y_d,test_size=0.2,random_state=42)\n",
    "\n",
    "y_train_2=pd.get_dummies(y_train)\n",
    "y_test_2=pd.get_dummies(y_test)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "#model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(700, activation='relu'))\n",
    "#model.add(Dense(1000, activation='relu'))\n",
    "#model.add(Dense(1200, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=160, callbacks=[early_stopping_monitor],batch_size=50)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "scores_2 = model.evaluate(X_train, y_train_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_2[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/160\n",
      "4263/4263 [==============================] - 7s 2ms/step - loss: 1.0468 - acc: 0.5599\n",
      "Epoch 2/160\n",
      "4263/4263 [==============================] - 2s 410us/step - loss: 0.9073 - acc: 0.6003\n",
      "Epoch 3/160\n",
      "4263/4263 [==============================] - 2s 408us/step - loss: 0.8753 - acc: 0.6097\n",
      "Epoch 4/160\n",
      "4263/4263 [==============================] - 2s 440us/step - loss: 0.8575 - acc: 0.6193\n",
      "Epoch 5/160\n",
      "4263/4263 [==============================] - 2s 377us/step - loss: 0.8502 - acc: 0.6132\n",
      "Epoch 6/160\n",
      "4263/4263 [==============================] - 2s 373us/step - loss: 0.8372 - acc: 0.6350\n",
      "Epoch 7/160\n",
      "4263/4263 [==============================] - 2s 362us/step - loss: 0.8539 - acc: 0.6242\n",
      "Epoch 8/160\n",
      "4263/4263 [==============================] - 2s 370us/step - loss: 0.8375 - acc: 0.6373\n",
      "Epoch 9/160\n",
      "4263/4263 [==============================] - 2s 373us/step - loss: 0.8347 - acc: 0.6376\n",
      "Epoch 10/160\n",
      "4263/4263 [==============================] - 2s 367us/step - loss: 0.8321 - acc: 0.6359\n",
      "Epoch 11/160\n",
      "4263/4263 [==============================] - 2s 363us/step - loss: 0.8202 - acc: 0.6470\n",
      "Epoch 12/160\n",
      "4263/4263 [==============================] - 2s 381us/step - loss: 0.8091 - acc: 0.6460\n",
      "Epoch 13/160\n",
      "4263/4263 [==============================] - 2s 369us/step - loss: 0.8156 - acc: 0.6545\n",
      "Epoch 14/160\n",
      "4263/4263 [==============================] - 2s 364us/step - loss: 0.8073 - acc: 0.6521\n",
      "Epoch 15/160\n",
      "4263/4263 [==============================] - 2s 368us/step - loss: 0.8075 - acc: 0.6472\n",
      "Epoch 16/160\n",
      "4263/4263 [==============================] - 2s 375us/step - loss: 0.8069 - acc: 0.6481\n",
      "Epoch 17/160\n",
      "4263/4263 [==============================] - 2s 370us/step - loss: 0.8024 - acc: 0.6549\n",
      "Epoch 18/160\n",
      "4263/4263 [==============================] - 2s 415us/step - loss: 0.7971 - acc: 0.6587\n",
      "Epoch 19/160\n",
      "4263/4263 [==============================] - 2s 370us/step - loss: 0.7982 - acc: 0.6524\n",
      "Epoch 20/160\n",
      "4263/4263 [==============================] - 2s 369us/step - loss: 0.7994 - acc: 0.6568\n",
      "Epoch 21/160\n",
      "4263/4263 [==============================] - 2s 372us/step - loss: 0.7881 - acc: 0.6606\n",
      "Epoch 22/160\n",
      "4263/4263 [==============================] - 2s 372us/step - loss: 0.7982 - acc: 0.6563\n",
      "Epoch 23/160\n",
      "4263/4263 [==============================] - 2s 372us/step - loss: 0.7873 - acc: 0.6538\n",
      "Epoch 24/160\n",
      "4263/4263 [==============================] - 2s 369us/step - loss: 0.7815 - acc: 0.6554\n",
      "Epoch 25/160\n",
      "4263/4263 [==============================] - 2s 380us/step - loss: 0.7831 - acc: 0.6627\n",
      "Epoch 26/160\n",
      "4263/4263 [==============================] - 2s 370us/step - loss: 0.7796 - acc: 0.6627\n",
      "Epoch 27/160\n",
      "4263/4263 [==============================] - 2s 370us/step - loss: 0.7785 - acc: 0.6596\n",
      "Epoch 28/160\n",
      "4263/4263 [==============================] - 2s 381us/step - loss: 0.7707 - acc: 0.6692\n",
      "Epoch 29/160\n",
      "4263/4263 [==============================] - 2s 368us/step - loss: 0.7835 - acc: 0.6620\n",
      "Epoch 30/160\n",
      "4263/4263 [==============================] - 2s 364us/step - loss: 0.7779 - acc: 0.6636\n",
      "Epoch 31/160\n",
      "4263/4263 [==============================] - 2s 371us/step - loss: 0.7809 - acc: 0.6559\n",
      "Epoch 32/160\n",
      "4263/4263 [==============================] - 2s 370us/step - loss: 0.7791 - acc: 0.6592\n",
      "Epoch 33/160\n",
      "4263/4263 [==============================] - 2s 359us/step - loss: 0.7812 - acc: 0.6627\n",
      "Epoch 34/160\n",
      "4263/4263 [==============================] - 2s 371us/step - loss: 0.7768 - acc: 0.6627\n",
      "Epoch 35/160\n",
      "4263/4263 [==============================] - 2s 365us/step - loss: 0.7674 - acc: 0.6606\n",
      "Epoch 36/160\n",
      "4263/4263 [==============================] - 2s 364us/step - loss: 0.7731 - acc: 0.6664\n",
      "Epoch 37/160\n",
      "4263/4263 [==============================] - 2s 365us/step - loss: 0.7675 - acc: 0.6683\n",
      "Epoch 38/160\n",
      "4263/4263 [==============================] - 2s 431us/step - loss: 0.7601 - acc: 0.6622\n",
      "Epoch 39/160\n",
      "4263/4263 [==============================] - 2s 377us/step - loss: 0.7664 - acc: 0.6648\n",
      "Epoch 40/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.7615 - acc: 0.6688\n",
      "Epoch 41/160\n",
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.7598 - acc: 0.6709\n",
      "Epoch 42/160\n",
      "4263/4263 [==============================] - 2s 372us/step - loss: 0.7649 - acc: 0.6721\n",
      "Epoch 43/160\n",
      "4263/4263 [==============================] - 2s 376us/step - loss: 0.7681 - acc: 0.6634\n",
      "Epoch 44/160\n",
      "4263/4263 [==============================] - 2s 374us/step - loss: 0.7684 - acc: 0.6671\n",
      "Epoch 45/160\n",
      "4263/4263 [==============================] - 2s 385us/step - loss: 0.7664 - acc: 0.6697\n",
      "Epoch 46/160\n",
      "4263/4263 [==============================] - 2s 402us/step - loss: 0.7550 - acc: 0.6718\n",
      "Epoch 47/160\n",
      "4263/4263 [==============================] - 2s 389us/step - loss: 0.7563 - acc: 0.6718\n",
      "Epoch 48/160\n",
      "4263/4263 [==============================] - 2s 380us/step - loss: 0.7572 - acc: 0.6711\n",
      "Epoch 49/160\n",
      "4263/4263 [==============================] - 2s 376us/step - loss: 0.7550 - acc: 0.6688\n",
      "Epoch 50/160\n",
      "4263/4263 [==============================] - 2s 378us/step - loss: 0.7549 - acc: 0.6697\n",
      "Epoch 51/160\n",
      "4263/4263 [==============================] - 2s 376us/step - loss: 0.7658 - acc: 0.6655\n",
      "Epoch 52/160\n",
      "4263/4263 [==============================] - 2s 378us/step - loss: 0.7523 - acc: 0.6760\n",
      "Epoch 53/160\n",
      "4263/4263 [==============================] - 2s 386us/step - loss: 0.7610 - acc: 0.6653\n",
      "Epoch 54/160\n",
      "4263/4263 [==============================] - 2s 369us/step - loss: 0.7559 - acc: 0.6695\n",
      "Epoch 55/160\n",
      "4263/4263 [==============================] - 2s 376us/step - loss: 0.7548 - acc: 0.6685\n",
      "Epoch 56/160\n",
      "4263/4263 [==============================] - 2s 380us/step - loss: 0.7528 - acc: 0.6742\n",
      "Epoch 57/160\n",
      "4263/4263 [==============================] - 2s 372us/step - loss: 0.7579 - acc: 0.6732\n",
      "Epoch 58/160\n",
      "4263/4263 [==============================] - 2s 372us/step - loss: 0.7483 - acc: 0.6763\n",
      "Epoch 59/160\n",
      "4263/4263 [==============================] - 2s 386us/step - loss: 0.7475 - acc: 0.6697\n",
      "Epoch 60/160\n",
      "4263/4263 [==============================] - 2s 377us/step - loss: 0.7557 - acc: 0.6730\n",
      "Epoch 61/160\n",
      "4263/4263 [==============================] - 2s 372us/step - loss: 0.7539 - acc: 0.6730\n",
      "Epoch 62/160\n",
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.7502 - acc: 0.6742\n",
      "Epoch 63/160\n",
      "4263/4263 [==============================] - 2s 382us/step - loss: 0.7481 - acc: 0.6735\n",
      "Epoch 64/160\n",
      "4263/4263 [==============================] - 2s 375us/step - loss: 0.7598 - acc: 0.6714\n",
      "Epoch 65/160\n",
      "4263/4263 [==============================] - 2s 382us/step - loss: 0.7477 - acc: 0.6718\n",
      "Epoch 66/160\n",
      "4263/4263 [==============================] - 2s 375us/step - loss: 0.7513 - acc: 0.6784\n",
      "Epoch 67/160\n",
      "4263/4263 [==============================] - 2s 385us/step - loss: 0.7496 - acc: 0.6796\n",
      "Epoch 68/160\n",
      "4263/4263 [==============================] - 2s 417us/step - loss: 0.7521 - acc: 0.6742\n",
      "Epoch 69/160\n",
      "4263/4263 [==============================] - 2s 482us/step - loss: 0.7426 - acc: 0.6805\n",
      "Epoch 70/160\n",
      "4263/4263 [==============================] - 2s 427us/step - loss: 0.7432 - acc: 0.6760\n",
      "Epoch 71/160\n",
      "4263/4263 [==============================] - 2s 410us/step - loss: 0.7454 - acc: 0.6718\n",
      "Epoch 72/160\n",
      "4263/4263 [==============================] - 2s 450us/step - loss: 0.7479 - acc: 0.6716\n",
      "Epoch 73/160\n",
      "4263/4263 [==============================] - 2s 428us/step - loss: 0.7485 - acc: 0.6718\n",
      "Epoch 74/160\n",
      "4263/4263 [==============================] - 2s 402us/step - loss: 0.7415 - acc: 0.6782\n",
      "Epoch 75/160\n",
      "4263/4263 [==============================] - 2s 379us/step - loss: 0.7426 - acc: 0.6772\n",
      "Epoch 76/160\n",
      "4263/4263 [==============================] - 2s 413us/step - loss: 0.7471 - acc: 0.6775\n",
      "Epoch 77/160\n",
      "4263/4263 [==============================] - 2s 403us/step - loss: 0.7600 - acc: 0.6678\n",
      "Epoch 78/160\n",
      "4263/4263 [==============================] - 2s 400us/step - loss: 0.7533 - acc: 0.6732\n",
      "Epoch 79/160\n",
      "4263/4263 [==============================] - 2s 423us/step - loss: 0.7490 - acc: 0.6772\n",
      "Epoch 80/160\n",
      "4263/4263 [==============================] - 2s 396us/step - loss: 0.7416 - acc: 0.6732\n",
      "Epoch 81/160\n",
      "4263/4263 [==============================] - 2s 396us/step - loss: 0.7452 - acc: 0.6714\n",
      "Epoch 82/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 2s 467us/step - loss: 0.7391 - acc: 0.6760\n",
      "Epoch 83/160\n",
      "4263/4263 [==============================] - 2s 465us/step - loss: 0.7443 - acc: 0.6765\n",
      "Epoch 84/160\n",
      "4263/4263 [==============================] - 2s 453us/step - loss: 0.7451 - acc: 0.6765\n",
      "Epoch 85/160\n",
      "4263/4263 [==============================] - 2s 440us/step - loss: 0.7399 - acc: 0.6798\n",
      "Epoch 86/160\n",
      "4263/4263 [==============================] - 2s 403us/step - loss: 0.7417 - acc: 0.6779\n",
      "Epoch 87/160\n",
      "4263/4263 [==============================] - 2s 435us/step - loss: 0.7345 - acc: 0.6821\n",
      "Epoch 88/160\n",
      "4263/4263 [==============================] - 2s 452us/step - loss: 0.7390 - acc: 0.6821\n",
      "Epoch 89/160\n",
      "4263/4263 [==============================] - 2s 442us/step - loss: 0.7428 - acc: 0.6793\n",
      "Epoch 90/160\n",
      "4263/4263 [==============================] - 2s 456us/step - loss: 0.7421 - acc: 0.6749\n",
      "Epoch 91/160\n",
      "4263/4263 [==============================] - 2s 399us/step - loss: 0.7455 - acc: 0.6728\n",
      "Epoch 92/160\n",
      "4263/4263 [==============================] - 2s 403us/step - loss: 0.7469 - acc: 0.6800\n",
      "Epoch 93/160\n",
      "4263/4263 [==============================] - 2s 419us/step - loss: 0.7374 - acc: 0.6873\n",
      "Epoch 94/160\n",
      "4263/4263 [==============================] - 2s 459us/step - loss: 0.7349 - acc: 0.6861\n",
      "Epoch 95/160\n",
      "4263/4263 [==============================] - 2s 488us/step - loss: 0.7363 - acc: 0.6789\n",
      "Epoch 96/160\n",
      "4263/4263 [==============================] - 2s 434us/step - loss: 0.7337 - acc: 0.6812\n",
      "Epoch 97/160\n",
      "4263/4263 [==============================] - 2s 385us/step - loss: 0.7343 - acc: 0.6819\n",
      "Epoch 98/160\n",
      "4263/4263 [==============================] - 1s 292us/step - loss: 0.7426 - acc: 0.6789\n",
      "Epoch 99/160\n",
      "4263/4263 [==============================] - 1s 301us/step - loss: 0.7291 - acc: 0.6817\n",
      "Epoch 100/160\n",
      "4263/4263 [==============================] - 1s 293us/step - loss: 0.7300 - acc: 0.6878\n",
      "Epoch 101/160\n",
      "4263/4263 [==============================] - 1s 295us/step - loss: 0.7298 - acc: 0.6838\n",
      "Epoch 102/160\n",
      "4263/4263 [==============================] - 1s 295us/step - loss: 0.7299 - acc: 0.6847\n",
      "Epoch 103/160\n",
      "4263/4263 [==============================] - 1s 295us/step - loss: 0.7318 - acc: 0.6897\n",
      "Epoch 104/160\n",
      "4263/4263 [==============================] - 1s 295us/step - loss: 0.7281 - acc: 0.6887\n",
      "Epoch 105/160\n",
      "4263/4263 [==============================] - 1s 292us/step - loss: 0.7395 - acc: 0.6744\n",
      "Epoch 106/160\n",
      "4263/4263 [==============================] - 1s 301us/step - loss: 0.7309 - acc: 0.6878\n",
      "Epoch 107/160\n",
      "4263/4263 [==============================] - 1s 298us/step - loss: 0.7284 - acc: 0.6845\n",
      "Epoch 108/160\n",
      "4263/4263 [==============================] - 1s 293us/step - loss: 0.7283 - acc: 0.6864\n",
      "Epoch 109/160\n",
      "4263/4263 [==============================] - 1s 295us/step - loss: 0.7268 - acc: 0.6857\n",
      "Epoch 110/160\n",
      "4263/4263 [==============================] - 1s 300us/step - loss: 0.7306 - acc: 0.6847\n",
      "Epoch 111/160\n",
      "4263/4263 [==============================] - 1s 298us/step - loss: 0.7252 - acc: 0.6857\n",
      "Epoch 112/160\n",
      "4263/4263 [==============================] - 1s 295us/step - loss: 0.7255 - acc: 0.6817\n",
      "Epoch 113/160\n",
      "4263/4263 [==============================] - 1s 292us/step - loss: 0.7285 - acc: 0.6810\n",
      "Epoch 114/160\n",
      "4263/4263 [==============================] - 1s 300us/step - loss: 0.7279 - acc: 0.6885\n",
      "Epoch 115/160\n",
      "4263/4263 [==============================] - 1s 297us/step - loss: 0.7302 - acc: 0.6850\n",
      "Epoch 116/160\n",
      "4263/4263 [==============================] - 1s 289us/step - loss: 0.7431 - acc: 0.6756\n",
      "Epoch 117/160\n",
      "4263/4263 [==============================] - 1s 290us/step - loss: 0.7262 - acc: 0.6904\n",
      "Epoch 118/160\n",
      "4263/4263 [==============================] - 1s 300us/step - loss: 0.7195 - acc: 0.6885\n",
      "Epoch 119/160\n",
      "4263/4263 [==============================] - 1s 293us/step - loss: 0.7301 - acc: 0.6838\n",
      "Epoch 120/160\n",
      "4263/4263 [==============================] - 1s 290us/step - loss: 0.7172 - acc: 0.6906\n",
      "Epoch 121/160\n",
      "4263/4263 [==============================] - 1s 288us/step - loss: 0.7164 - acc: 0.6845\n",
      "Epoch 122/160\n",
      "4263/4263 [==============================] - 1s 296us/step - loss: 0.7156 - acc: 0.6887\n",
      "Epoch 123/160\n",
      "4263/4263 [==============================] - 1s 294us/step - loss: 0.7262 - acc: 0.6859\n",
      "Epoch 124/160\n",
      "4263/4263 [==============================] - 1s 297us/step - loss: 0.7265 - acc: 0.6880\n",
      "Epoch 125/160\n",
      "4263/4263 [==============================] - 1s 292us/step - loss: 0.7173 - acc: 0.6908\n",
      "Epoch 126/160\n",
      "4263/4263 [==============================] - 1s 297us/step - loss: 0.7159 - acc: 0.6958\n",
      "Epoch 127/160\n",
      "4263/4263 [==============================] - 1s 289us/step - loss: 0.7119 - acc: 0.6932\n",
      "Epoch 128/160\n",
      "4263/4263 [==============================] - 1s 289us/step - loss: 0.7091 - acc: 0.6908\n",
      "Epoch 129/160\n",
      "4263/4263 [==============================] - 1s 292us/step - loss: 0.7212 - acc: 0.6864\n",
      "Epoch 130/160\n",
      "4263/4263 [==============================] - 1s 298us/step - loss: 0.7157 - acc: 0.6897\n",
      "Epoch 131/160\n",
      "4263/4263 [==============================] - 1s 302us/step - loss: 0.7183 - acc: 0.6880\n",
      "Epoch 132/160\n",
      "4263/4263 [==============================] - 1s 287us/step - loss: 0.7089 - acc: 0.6943\n",
      "Epoch 133/160\n",
      "4263/4263 [==============================] - 1s 292us/step - loss: 0.7136 - acc: 0.6939\n",
      "Epoch 134/160\n",
      "4263/4263 [==============================] - 1s 295us/step - loss: 0.7097 - acc: 0.6936\n",
      "Epoch 135/160\n",
      "4263/4263 [==============================] - 1s 293us/step - loss: 0.7150 - acc: 0.6894\n",
      "Epoch 136/160\n",
      "4263/4263 [==============================] - 1s 297us/step - loss: 0.7190 - acc: 0.6875\n",
      "Epoch 137/160\n",
      "4263/4263 [==============================] - 1s 291us/step - loss: 0.7127 - acc: 0.6932\n",
      "Epoch 138/160\n",
      "4263/4263 [==============================] - 1s 296us/step - loss: 0.7222 - acc: 0.6887\n",
      "Epoch 139/160\n",
      "4263/4263 [==============================] - 1s 292us/step - loss: 0.7166 - acc: 0.6880\n",
      "Epoch 140/160\n",
      "4263/4263 [==============================] - 1s 286us/step - loss: 0.7148 - acc: 0.6899\n",
      "Epoch 141/160\n",
      "4263/4263 [==============================] - 1s 291us/step - loss: 0.7121 - acc: 0.6939\n",
      "Epoch 142/160\n",
      "4263/4263 [==============================] - 1s 292us/step - loss: 0.7012 - acc: 0.6969\n",
      "Epoch 143/160\n",
      "4263/4263 [==============================] - 1s 290us/step - loss: 0.7061 - acc: 0.6960\n",
      "Epoch 144/160\n",
      "4263/4263 [==============================] - 1s 289us/step - loss: 0.7023 - acc: 0.6929\n",
      "Epoch 145/160\n",
      "4263/4263 [==============================] - 1s 292us/step - loss: 0.7027 - acc: 0.6948\n",
      "Epoch 146/160\n",
      "4263/4263 [==============================] - 1s 297us/step - loss: 0.7140 - acc: 0.6894\n",
      "Epoch 147/160\n",
      "4263/4263 [==============================] - 1s 290us/step - loss: 0.7094 - acc: 0.6974\n",
      "Epoch 148/160\n",
      "4263/4263 [==============================] - 1s 289us/step - loss: 0.7291 - acc: 0.6838\n",
      "Epoch 149/160\n",
      "4263/4263 [==============================] - 1s 290us/step - loss: 0.7031 - acc: 0.6962\n",
      "Epoch 150/160\n",
      "4263/4263 [==============================] - 1s 298us/step - loss: 0.6994 - acc: 0.6953\n",
      "Epoch 151/160\n",
      "4263/4263 [==============================] - 1s 293us/step - loss: 0.6968 - acc: 0.6997\n",
      "Epoch 152/160\n",
      "4263/4263 [==============================] - 1s 288us/step - loss: 0.7056 - acc: 0.6913\n",
      "Epoch 153/160\n",
      "4263/4263 [==============================] - 1s 292us/step - loss: 0.6949 - acc: 0.6995\n",
      "Epoch 154/160\n",
      "4263/4263 [==============================] - 1s 294us/step - loss: 0.7069 - acc: 0.6986\n",
      "Epoch 155/160\n",
      "4263/4263 [==============================] - 1s 291us/step - loss: 0.7046 - acc: 0.6955\n",
      "Epoch 156/160\n",
      "4263/4263 [==============================] - 1s 287us/step - loss: 0.7037 - acc: 0.6955\n",
      "Epoch 157/160\n",
      "4263/4263 [==============================] - 1s 290us/step - loss: 0.7008 - acc: 0.6983\n",
      "Epoch 158/160\n",
      "4263/4263 [==============================] - 1s 294us/step - loss: 0.6960 - acc: 0.6986\n",
      "Epoch 159/160\n",
      "4263/4263 [==============================] - 1s 293us/step - loss: 0.6976 - acc: 0.6969\n",
      "Epoch 160/160\n",
      "4263/4263 [==============================] - 1s 287us/step - loss: 0.6950 - acc: 0.7028\n",
      "1066/1066 [==============================] - 2s 2ms/step\n",
      "4263/4263 [==============================] - 1s 137us/step\n",
      "\n",
      "acc: 67.35%\n",
      "\n",
      "acc: 71.17%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df['Class'].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d,y_d,test_size=0.2,random_state=42)\n",
    "\n",
    "y_train_2=pd.get_dummies(y_train)\n",
    "y_test_2=pd.get_dummies(y_test)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(650, activation='relu'))\n",
    "#model.add(Dense(1000, activation='relu'))\n",
    "#model.add(Dense(1200, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=160, callbacks=[early_stopping_monitor],batch_size=50)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "scores_2 = model.evaluate(X_train, y_train_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_2[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/160\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 1.0473 - acc: 0.5620\n",
      "Epoch 2/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.9095 - acc: 0.5972\n",
      "Epoch 3/160\n",
      "4263/4263 [==============================] - 2s 429us/step - loss: 0.8787 - acc: 0.6115\n",
      "Epoch 4/160\n",
      "4263/4263 [==============================] - 2s 438us/step - loss: 0.8693 - acc: 0.6127\n",
      "Epoch 5/160\n",
      "4263/4263 [==============================] - 2s 489us/step - loss: 0.8472 - acc: 0.6327\n",
      "Epoch 6/160\n",
      "4263/4263 [==============================] - 2s 520us/step - loss: 0.8355 - acc: 0.6434\n",
      "Epoch 7/160\n",
      "4263/4263 [==============================] - 2s 395us/step - loss: 0.8410 - acc: 0.6282\n",
      "Epoch 8/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.8346 - acc: 0.6446 1s - \n",
      "Epoch 9/160\n",
      "4263/4263 [==============================] - 2s 400us/step - loss: 0.8260 - acc: 0.6369\n",
      "Epoch 10/160\n",
      "4263/4263 [==============================] - 2s 389us/step - loss: 0.8267 - acc: 0.6338\n",
      "Epoch 11/160\n",
      "4263/4263 [==============================] - 2s 382us/step - loss: 0.8289 - acc: 0.6366\n",
      "Epoch 12/160\n",
      "4263/4263 [==============================] - 2s 390us/step - loss: 0.8179 - acc: 0.6453\n",
      "Epoch 13/160\n",
      "4263/4263 [==============================] - 2s 376us/step - loss: 0.8108 - acc: 0.6498\n",
      "Epoch 14/160\n",
      "4263/4263 [==============================] - 2s 386us/step - loss: 0.8106 - acc: 0.6517\n",
      "Epoch 15/160\n",
      "4263/4263 [==============================] - 2s 396us/step - loss: 0.8041 - acc: 0.6521\n",
      "Epoch 16/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.8017 - acc: 0.6563\n",
      "Epoch 17/160\n",
      "4263/4263 [==============================] - 2s 385us/step - loss: 0.8105 - acc: 0.6517\n",
      "Epoch 18/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.7966 - acc: 0.6535\n",
      "Epoch 19/160\n",
      "4263/4263 [==============================] - 2s 377us/step - loss: 0.7893 - acc: 0.6578\n",
      "Epoch 20/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.7908 - acc: 0.6517\n",
      "Epoch 21/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.7873 - acc: 0.6561\n",
      "Epoch 22/160\n",
      "4263/4263 [==============================] - 2s 381us/step - loss: 0.7840 - acc: 0.6660\n",
      "Epoch 23/160\n",
      "4263/4263 [==============================] - 2s 388us/step - loss: 0.7824 - acc: 0.6627\n",
      "Epoch 24/160\n",
      "4263/4263 [==============================] - 2s 395us/step - loss: 0.7959 - acc: 0.6460\n",
      "Epoch 25/160\n",
      "4263/4263 [==============================] - 2s 385us/step - loss: 0.7869 - acc: 0.6580\n",
      "Epoch 26/160\n",
      "4263/4263 [==============================] - 2s 384us/step - loss: 0.7895 - acc: 0.6582\n",
      "Epoch 27/160\n",
      "4263/4263 [==============================] - 2s 401us/step - loss: 0.7722 - acc: 0.6601\n",
      "Epoch 28/160\n",
      "4263/4263 [==============================] - 2s 382us/step - loss: 0.7767 - acc: 0.6613\n",
      "Epoch 29/160\n",
      "4263/4263 [==============================] - 2s 384us/step - loss: 0.7757 - acc: 0.6660\n",
      "Epoch 30/160\n",
      "4263/4263 [==============================] - 2s 388us/step - loss: 0.7751 - acc: 0.6575\n",
      "Epoch 31/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.7831 - acc: 0.6521\n",
      "Epoch 32/160\n",
      "4263/4263 [==============================] - 2s 384us/step - loss: 0.7727 - acc: 0.6629\n",
      "Epoch 33/160\n",
      "4263/4263 [==============================] - 2s 394us/step - loss: 0.7777 - acc: 0.6582\n",
      "Epoch 34/160\n",
      "4263/4263 [==============================] - 2s 386us/step - loss: 0.7788 - acc: 0.6631\n",
      "Epoch 35/160\n",
      "4263/4263 [==============================] - 2s 404us/step - loss: 0.7725 - acc: 0.6664\n",
      "Epoch 36/160\n",
      "4263/4263 [==============================] - 2s 409us/step - loss: 0.7670 - acc: 0.6627\n",
      "Epoch 37/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.7593 - acc: 0.6685\n",
      "Epoch 38/160\n",
      "4263/4263 [==============================] - 2s 385us/step - loss: 0.7625 - acc: 0.6702\n",
      "Epoch 39/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.7646 - acc: 0.6702\n",
      "Epoch 40/160\n",
      "4263/4263 [==============================] - 2s 385us/step - loss: 0.7600 - acc: 0.6685\n",
      "Epoch 41/160\n",
      "4263/4263 [==============================] - 2s 384us/step - loss: 0.7610 - acc: 0.6692\n",
      "Epoch 42/160\n",
      "4263/4263 [==============================] - 2s 394us/step - loss: 0.7684 - acc: 0.6641\n",
      "Epoch 43/160\n",
      "4263/4263 [==============================] - 2s 389us/step - loss: 0.7585 - acc: 0.6683\n",
      "Epoch 44/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.7636 - acc: 0.6646\n",
      "Epoch 45/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.7644 - acc: 0.6669\n",
      "Epoch 46/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.7571 - acc: 0.6671\n",
      "Epoch 47/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.7603 - acc: 0.6639\n",
      "Epoch 48/160\n",
      "4263/4263 [==============================] - 2s 394us/step - loss: 0.7539 - acc: 0.6634\n",
      "Epoch 49/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.7551 - acc: 0.6735\n",
      "Epoch 50/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.7609 - acc: 0.6648\n",
      "Epoch 51/160\n",
      "4263/4263 [==============================] - 2s 382us/step - loss: 0.7526 - acc: 0.6732\n",
      "Epoch 52/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.7583 - acc: 0.6643\n",
      "Epoch 53/160\n",
      "4263/4263 [==============================] - 2s 388us/step - loss: 0.7649 - acc: 0.6758\n",
      "Epoch 54/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.7591 - acc: 0.6704\n",
      "Epoch 55/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.7533 - acc: 0.6768\n",
      "Epoch 56/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.7473 - acc: 0.6760\n",
      "Epoch 57/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.7558 - acc: 0.6709\n",
      "Epoch 58/160\n",
      "4263/4263 [==============================] - 2s 384us/step - loss: 0.7486 - acc: 0.6732\n",
      "Epoch 59/160\n",
      "4263/4263 [==============================] - 2s 382us/step - loss: 0.7439 - acc: 0.6714\n",
      "Epoch 60/160\n",
      "4263/4263 [==============================] - 2s 390us/step - loss: 0.7515 - acc: 0.6683\n",
      "Epoch 61/160\n",
      "4263/4263 [==============================] - 2s 385us/step - loss: 0.7461 - acc: 0.6716\n",
      "Epoch 62/160\n",
      "4263/4263 [==============================] - 2s 384us/step - loss: 0.7568 - acc: 0.6707\n",
      "Epoch 63/160\n",
      "4263/4263 [==============================] - 2s 385us/step - loss: 0.7396 - acc: 0.6782\n",
      "Epoch 64/160\n",
      "4263/4263 [==============================] - 2s 384us/step - loss: 0.7381 - acc: 0.6782\n",
      "Epoch 65/160\n",
      "4263/4263 [==============================] - 2s 390us/step - loss: 0.7449 - acc: 0.6758\n",
      "Epoch 66/160\n",
      "4263/4263 [==============================] - 2s 397us/step - loss: 0.7450 - acc: 0.6739\n",
      "Epoch 67/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.7549 - acc: 0.6709\n",
      "Epoch 68/160\n",
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.7419 - acc: 0.6779\n",
      "Epoch 69/160\n",
      "4263/4263 [==============================] - 2s 389us/step - loss: 0.7483 - acc: 0.6742\n",
      "Epoch 70/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.7419 - acc: 0.6817\n",
      "Epoch 71/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.7402 - acc: 0.6786\n",
      "Epoch 72/160\n",
      "4263/4263 [==============================] - 2s 394us/step - loss: 0.7372 - acc: 0.6824\n",
      "Epoch 73/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.7342 - acc: 0.6857\n",
      "Epoch 74/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.7393 - acc: 0.6843\n",
      "Epoch 75/160\n",
      "4263/4263 [==============================] - 2s 388us/step - loss: 0.7329 - acc: 0.6819\n",
      "Epoch 76/160\n",
      "4263/4263 [==============================] - 2s 390us/step - loss: 0.7256 - acc: 0.6904\n",
      "Epoch 77/160\n",
      "4263/4263 [==============================] - 2s 386us/step - loss: 0.7328 - acc: 0.6836\n",
      "Epoch 78/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.7325 - acc: 0.6857\n",
      "Epoch 79/160\n",
      "4263/4263 [==============================] - 2s 382us/step - loss: 0.7323 - acc: 0.6838\n",
      "Epoch 80/160\n",
      "4263/4263 [==============================] - 2s 379us/step - loss: 0.7291 - acc: 0.6934\n",
      "Epoch 81/160\n",
      "4263/4263 [==============================] - 2s 395us/step - loss: 0.7674 - acc: 0.6714\n",
      "Epoch 82/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 2s 379us/step - loss: 0.7367 - acc: 0.6831\n",
      "Epoch 83/160\n",
      "4263/4263 [==============================] - 2s 384us/step - loss: 0.7286 - acc: 0.6861\n",
      "Epoch 84/160\n",
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.7373 - acc: 0.6796\n",
      "Epoch 85/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.7342 - acc: 0.6814\n",
      "Epoch 86/160\n",
      "4263/4263 [==============================] - 2s 380us/step - loss: 0.7236 - acc: 0.6847\n",
      "Epoch 87/160\n",
      "4263/4263 [==============================] - 2s 384us/step - loss: 0.7339 - acc: 0.6821\n",
      "Epoch 88/160\n",
      "4263/4263 [==============================] - 2s 386us/step - loss: 0.7358 - acc: 0.6777\n",
      "Epoch 89/160\n",
      "4263/4263 [==============================] - 2s 390us/step - loss: 0.7246 - acc: 0.6873\n",
      "Epoch 90/160\n",
      "4263/4263 [==============================] - 2s 394us/step - loss: 0.7195 - acc: 0.6908\n",
      "Epoch 91/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.7261 - acc: 0.6925\n",
      "Epoch 92/160\n",
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.7336 - acc: 0.6784\n",
      "Epoch 93/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.7199 - acc: 0.6925\n",
      "Epoch 94/160\n",
      "4263/4263 [==============================] - 2s 377us/step - loss: 0.7217 - acc: 0.6890\n",
      "Epoch 95/160\n",
      "4263/4263 [==============================] - 2s 371us/step - loss: 0.7210 - acc: 0.6864\n",
      "Epoch 96/160\n",
      "4263/4263 [==============================] - 2s 390us/step - loss: 0.7314 - acc: 0.6772\n",
      "Epoch 97/160\n",
      "4263/4263 [==============================] - 2s 386us/step - loss: 0.7177 - acc: 0.6885\n",
      "Epoch 98/160\n",
      "4263/4263 [==============================] - 2s 378us/step - loss: 0.7340 - acc: 0.6843\n",
      "Epoch 99/160\n",
      "4263/4263 [==============================] - 2s 388us/step - loss: 0.7180 - acc: 0.6897\n",
      "Epoch 100/160\n",
      "4263/4263 [==============================] - 2s 385us/step - loss: 0.7201 - acc: 0.6861\n",
      "Epoch 101/160\n",
      "4263/4263 [==============================] - 2s 382us/step - loss: 0.7213 - acc: 0.6847\n",
      "Epoch 102/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.7171 - acc: 0.6929\n",
      "Epoch 103/160\n",
      "4263/4263 [==============================] - 2s 384us/step - loss: 0.7048 - acc: 0.6943\n",
      "Epoch 104/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.7087 - acc: 0.6976\n",
      "Epoch 105/160\n",
      "4263/4263 [==============================] - 2s 522us/step - loss: 0.7167 - acc: 0.6871\n",
      "Epoch 106/160\n",
      "4263/4263 [==============================] - 2s 415us/step - loss: 0.7238 - acc: 0.6932\n",
      "Epoch 107/160\n",
      "4263/4263 [==============================] - 2s 434us/step - loss: 0.6992 - acc: 0.6983\n",
      "Epoch 108/160\n",
      "4263/4263 [==============================] - 2s 445us/step - loss: 0.7161 - acc: 0.6901\n",
      "Epoch 109/160\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.7052 - acc: 0.7021\n",
      "Epoch 110/160\n",
      "4263/4263 [==============================] - 2s 438us/step - loss: 0.7103 - acc: 0.6936\n",
      "Epoch 111/160\n",
      "4263/4263 [==============================] - 2s 502us/step - loss: 0.7051 - acc: 0.6915\n",
      "Epoch 112/160\n",
      "4263/4263 [==============================] - 2s 403us/step - loss: 0.7049 - acc: 0.6925\n",
      "Epoch 113/160\n",
      "4263/4263 [==============================] - 2s 467us/step - loss: 0.7037 - acc: 0.7004\n",
      "Epoch 114/160\n",
      "4263/4263 [==============================] - 2s 435us/step - loss: 0.7027 - acc: 0.6955\n",
      "Epoch 115/160\n",
      "4263/4263 [==============================] - 2s 423us/step - loss: 0.7055 - acc: 0.6899\n",
      "Epoch 116/160\n",
      "4263/4263 [==============================] - 2s 414us/step - loss: 0.7120 - acc: 0.6899\n",
      "Epoch 117/160\n",
      "4263/4263 [==============================] - 2s 446us/step - loss: 0.7083 - acc: 0.6873\n",
      "Epoch 118/160\n",
      "4263/4263 [==============================] - 2s 443us/step - loss: 0.7042 - acc: 0.6918\n",
      "Epoch 119/160\n",
      "4263/4263 [==============================] - 2s 411us/step - loss: 0.6961 - acc: 0.7023\n",
      "Epoch 120/160\n",
      "4263/4263 [==============================] - 2s 420us/step - loss: 0.6920 - acc: 0.6976\n",
      "Epoch 121/160\n",
      "4263/4263 [==============================] - 2s 480us/step - loss: 0.7053 - acc: 0.6918\n",
      "Epoch 122/160\n",
      "4263/4263 [==============================] - 2s 442us/step - loss: 0.7048 - acc: 0.6960\n",
      "Epoch 123/160\n",
      "4263/4263 [==============================] - 2s 415us/step - loss: 0.7003 - acc: 0.6986\n",
      "Epoch 124/160\n",
      "4263/4263 [==============================] - 2s 405us/step - loss: 0.6930 - acc: 0.7000\n",
      "Epoch 125/160\n",
      "4263/4263 [==============================] - 2s 413us/step - loss: 0.6940 - acc: 0.7026\n",
      "Epoch 126/160\n",
      "4263/4263 [==============================] - 2s 407us/step - loss: 0.7060 - acc: 0.6922\n",
      "Epoch 127/160\n",
      "4263/4263 [==============================] - 2s 415us/step - loss: 0.6933 - acc: 0.6955\n",
      "Epoch 128/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.6925 - acc: 0.6983\n",
      "Epoch 129/160\n",
      "4263/4263 [==============================] - 2s 433us/step - loss: 0.6965 - acc: 0.6948\n",
      "Epoch 130/160\n",
      "4263/4263 [==============================] - 2s 397us/step - loss: 0.6867 - acc: 0.6995\n",
      "Epoch 131/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.6843 - acc: 0.6988\n",
      "Epoch 132/160\n",
      "4263/4263 [==============================] - 2s 398us/step - loss: 0.6945 - acc: 0.6993\n",
      "Epoch 133/160\n",
      "4263/4263 [==============================] - 2s 390us/step - loss: 0.6917 - acc: 0.6988\n",
      "Epoch 134/160\n",
      "4263/4263 [==============================] - 2s 396us/step - loss: 0.6899 - acc: 0.7014\n",
      "Epoch 135/160\n",
      "4263/4263 [==============================] - 2s 399us/step - loss: 0.6810 - acc: 0.7033\n",
      "Epoch 136/160\n",
      "4263/4263 [==============================] - 2s 389us/step - loss: 0.6862 - acc: 0.6976\n",
      "Epoch 137/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.6899 - acc: 0.6997\n",
      "Epoch 138/160\n",
      "4263/4263 [==============================] - 2s 414us/step - loss: 0.6957 - acc: 0.7009\n",
      "Epoch 139/160\n",
      "4263/4263 [==============================] - 2s 405us/step - loss: 0.7080 - acc: 0.6859\n",
      "Epoch 140/160\n",
      "4263/4263 [==============================] - 2s 371us/step - loss: 0.6796 - acc: 0.7126\n",
      "Epoch 141/160\n",
      "4263/4263 [==============================] - 2s 388us/step - loss: 0.6859 - acc: 0.7019\n",
      "Epoch 142/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.6755 - acc: 0.7068\n",
      "Epoch 143/160\n",
      "4263/4263 [==============================] - 2s 389us/step - loss: 0.6783 - acc: 0.7033\n",
      "Epoch 144/160\n",
      "4263/4263 [==============================] - 2s 413us/step - loss: 0.6950 - acc: 0.7009\n",
      "Epoch 145/160\n",
      "4263/4263 [==============================] - 2s 403us/step - loss: 0.6719 - acc: 0.7094\n",
      "Epoch 146/160\n",
      "4263/4263 [==============================] - 2s 399us/step - loss: 0.6832 - acc: 0.7004\n",
      "Epoch 147/160\n",
      "4263/4263 [==============================] - 2s 405us/step - loss: 0.6761 - acc: 0.7058\n",
      "Epoch 148/160\n",
      "4263/4263 [==============================] - 2s 399us/step - loss: 0.6797 - acc: 0.7058\n",
      "Epoch 149/160\n",
      "4263/4263 [==============================] - 2s 520us/step - loss: 0.6793 - acc: 0.7049\n",
      "Epoch 150/160\n",
      "4263/4263 [==============================] - 2s 481us/step - loss: 0.6655 - acc: 0.7155\n",
      "Epoch 151/160\n",
      "4263/4263 [==============================] - 2s 445us/step - loss: 0.6727 - acc: 0.7051\n",
      "Epoch 152/160\n",
      "4263/4263 [==============================] - 2s 445us/step - loss: 0.6763 - acc: 0.7068\n",
      "Epoch 153/160\n",
      "4263/4263 [==============================] - 2s 490us/step - loss: 0.6733 - acc: 0.7040\n",
      "Epoch 154/160\n",
      "4263/4263 [==============================] - 2s 400us/step - loss: 0.6952 - acc: 0.6976\n",
      "Epoch 155/160\n",
      "4263/4263 [==============================] - 2s 404us/step - loss: 0.6625 - acc: 0.7084\n",
      "Epoch 156/160\n",
      "4263/4263 [==============================] - 2s 401us/step - loss: 0.6719 - acc: 0.7103\n",
      "Epoch 157/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.6621 - acc: 0.7112\n",
      "Epoch 158/160\n",
      "4263/4263 [==============================] - 2s 400us/step - loss: 0.6717 - acc: 0.7019\n",
      "Epoch 159/160\n",
      "4263/4263 [==============================] - 2s 526us/step - loss: 0.6827 - acc: 0.7117\n",
      "Epoch 160/160\n",
      "4263/4263 [==============================] - 2s 480us/step - loss: 0.6711 - acc: 0.7115\n",
      "1066/1066 [==============================] - 2s 2ms/step\n",
      "4263/4263 [==============================] - 1s 247us/step\n",
      "\n",
      "acc: 66.70%\n",
      "\n",
      "acc: 72.37%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df['Class'].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d,y_d,test_size=0.2,random_state=42)\n",
    "\n",
    "y_train_2=pd.get_dummies(y_train)\n",
    "y_test_2=pd.get_dummies(y_test)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(700, activation='relu'))\n",
    "#model.add(Dense(1000, activation='relu'))\n",
    "#model.add(Dense(1200, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=160,batch_size=50)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "scores_2 = model.evaluate(X_train, y_train_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_2[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/160\n",
      "4263/4263 [==============================] - 7s 2ms/step - loss: 1.0457 - acc: 0.5574\n",
      "Epoch 2/160\n",
      "4263/4263 [==============================] - 1s 239us/step - loss: 0.9274 - acc: 0.6010\n",
      "Epoch 3/160\n",
      "4263/4263 [==============================] - 1s 235us/step - loss: 0.8765 - acc: 0.6125\n",
      "Epoch 4/160\n",
      "4263/4263 [==============================] - 1s 234us/step - loss: 0.8744 - acc: 0.6158\n",
      "Epoch 5/160\n",
      "4263/4263 [==============================] - 1s 247us/step - loss: 0.8601 - acc: 0.6188\n",
      "Epoch 6/160\n",
      "4263/4263 [==============================] - 1s 235us/step - loss: 0.8415 - acc: 0.6310\n",
      "Epoch 7/160\n",
      "4263/4263 [==============================] - 1s 236us/step - loss: 0.8344 - acc: 0.6399\n",
      "Epoch 8/160\n",
      "4263/4263 [==============================] - 1s 239us/step - loss: 0.8357 - acc: 0.6331\n",
      "Epoch 9/160\n",
      "4263/4263 [==============================] - 1s 241us/step - loss: 0.8502 - acc: 0.6296\n",
      "Epoch 10/160\n",
      "4263/4263 [==============================] - 1s 233us/step - loss: 0.8360 - acc: 0.6312\n",
      "Epoch 11/160\n",
      "4263/4263 [==============================] - 1s 246us/step - loss: 0.8338 - acc: 0.6355\n",
      "Epoch 12/160\n",
      "4263/4263 [==============================] - 1s 228us/step - loss: 0.8179 - acc: 0.6395\n",
      "Epoch 13/160\n",
      "4263/4263 [==============================] - 1s 237us/step - loss: 0.8160 - acc: 0.6514\n",
      "Epoch 14/160\n",
      "4263/4263 [==============================] - 1s 235us/step - loss: 0.8177 - acc: 0.6498\n",
      "Epoch 15/160\n",
      "4263/4263 [==============================] - 1s 239us/step - loss: 0.8083 - acc: 0.6519\n",
      "Epoch 16/160\n",
      "4263/4263 [==============================] - 1s 235us/step - loss: 0.8098 - acc: 0.6524\n",
      "Epoch 17/160\n",
      "4263/4263 [==============================] - 1s 247us/step - loss: 0.8126 - acc: 0.6434\n",
      "Epoch 18/160\n",
      "4263/4263 [==============================] - 1s 245us/step - loss: 0.7927 - acc: 0.6575\n",
      "Epoch 19/160\n",
      "4263/4263 [==============================] - 1s 242us/step - loss: 0.8054 - acc: 0.6456\n",
      "Epoch 20/160\n",
      "4263/4263 [==============================] - 1s 235us/step - loss: 0.8047 - acc: 0.6474\n",
      "Epoch 21/160\n",
      "4263/4263 [==============================] - 1s 236us/step - loss: 0.7944 - acc: 0.6613\n",
      "Epoch 22/160\n",
      "4263/4263 [==============================] - 1s 249us/step - loss: 0.7872 - acc: 0.6643\n",
      "Epoch 23/160\n",
      "4263/4263 [==============================] - 1s 237us/step - loss: 0.7844 - acc: 0.6627\n",
      "Epoch 24/160\n",
      "4263/4263 [==============================] - 1s 238us/step - loss: 0.7761 - acc: 0.6589\n",
      "Epoch 25/160\n",
      "4263/4263 [==============================] - 1s 230us/step - loss: 0.7826 - acc: 0.6528\n",
      "Epoch 26/160\n",
      "4263/4263 [==============================] - 1s 229us/step - loss: 0.7837 - acc: 0.6615\n",
      "Epoch 27/160\n",
      "4263/4263 [==============================] - 1s 228us/step - loss: 0.7835 - acc: 0.6568\n",
      "Epoch 28/160\n",
      "4263/4263 [==============================] - 1s 237us/step - loss: 0.7789 - acc: 0.6610\n",
      "Epoch 29/160\n",
      "4263/4263 [==============================] - 1s 253us/step - loss: 0.7729 - acc: 0.6653\n",
      "Epoch 30/160\n",
      "4263/4263 [==============================] - 1s 234us/step - loss: 0.7663 - acc: 0.6634\n",
      "Epoch 31/160\n",
      "4263/4263 [==============================] - 1s 230us/step - loss: 0.7809 - acc: 0.6681\n",
      "Epoch 32/160\n",
      "4263/4263 [==============================] - 1s 229us/step - loss: 0.7783 - acc: 0.6578\n",
      "Epoch 33/160\n",
      "4263/4263 [==============================] - 1s 226us/step - loss: 0.7716 - acc: 0.6599\n",
      "Epoch 34/160\n",
      "4263/4263 [==============================] - 1s 249us/step - loss: 0.7662 - acc: 0.6674\n",
      "Epoch 35/160\n",
      "4263/4263 [==============================] - 1s 245us/step - loss: 0.7692 - acc: 0.6692\n",
      "Epoch 36/160\n",
      "4263/4263 [==============================] - 1s 232us/step - loss: 0.7676 - acc: 0.6695\n",
      "Epoch 37/160\n",
      "4263/4263 [==============================] - 1s 229us/step - loss: 0.7709 - acc: 0.6610\n",
      "Epoch 38/160\n",
      "4263/4263 [==============================] - 1s 239us/step - loss: 0.7578 - acc: 0.6742\n",
      "Epoch 39/160\n",
      "4263/4263 [==============================] - 1s 235us/step - loss: 0.7803 - acc: 0.6549\n",
      "Epoch 40/160\n",
      "4263/4263 [==============================] - 1s 244us/step - loss: 0.7679 - acc: 0.6697\n",
      "Epoch 41/160\n",
      "4263/4263 [==============================] - 1s 245us/step - loss: 0.7573 - acc: 0.6732\n",
      "Epoch 42/160\n",
      "4263/4263 [==============================] - 1s 232us/step - loss: 0.7714 - acc: 0.6617\n",
      "Epoch 43/160\n",
      "4263/4263 [==============================] - 1s 237us/step - loss: 0.7577 - acc: 0.6721\n",
      "Epoch 44/160\n",
      "4263/4263 [==============================] - 1s 243us/step - loss: 0.7573 - acc: 0.6700\n",
      "Epoch 45/160\n",
      "4263/4263 [==============================] - 1s 235us/step - loss: 0.7518 - acc: 0.6728\n",
      "Epoch 46/160\n",
      "4263/4263 [==============================] - 1s 239us/step - loss: 0.7482 - acc: 0.6746\n",
      "Epoch 47/160\n",
      "4263/4263 [==============================] - 1s 235us/step - loss: 0.7540 - acc: 0.6718\n",
      "Epoch 48/160\n",
      "4263/4263 [==============================] - 1s 237us/step - loss: 0.7675 - acc: 0.6634\n",
      "Epoch 49/160\n",
      "4263/4263 [==============================] - 1s 236us/step - loss: 0.7607 - acc: 0.6631\n",
      "Epoch 50/160\n",
      "4263/4263 [==============================] - 1s 234us/step - loss: 0.7559 - acc: 0.6704\n",
      "Epoch 51/160\n",
      "4263/4263 [==============================] - 1s 236us/step - loss: 0.7505 - acc: 0.6718\n",
      "Epoch 52/160\n",
      "4263/4263 [==============================] - 1s 237us/step - loss: 0.7517 - acc: 0.6751\n",
      "Epoch 53/160\n",
      "4263/4263 [==============================] - 1s 240us/step - loss: 0.7496 - acc: 0.6749\n",
      "Epoch 54/160\n",
      "4263/4263 [==============================] - 1s 239us/step - loss: 0.7509 - acc: 0.6772\n",
      "Epoch 55/160\n",
      "4263/4263 [==============================] - 1s 236us/step - loss: 0.7502 - acc: 0.6742\n",
      "Epoch 56/160\n",
      "4263/4263 [==============================] - 1s 230us/step - loss: 0.7518 - acc: 0.6714\n",
      "Epoch 57/160\n",
      "4263/4263 [==============================] - 1s 239us/step - loss: 0.7431 - acc: 0.6709\n",
      "Epoch 58/160\n",
      "4263/4263 [==============================] - 1s 242us/step - loss: 0.7463 - acc: 0.6739\n",
      "Epoch 59/160\n",
      "4263/4263 [==============================] - 1s 242us/step - loss: 0.7576 - acc: 0.6728\n",
      "Epoch 60/160\n",
      "4263/4263 [==============================] - 1s 230us/step - loss: 0.7531 - acc: 0.6763\n",
      "Epoch 61/160\n",
      "4263/4263 [==============================] - 1s 233us/step - loss: 0.7407 - acc: 0.6775\n",
      "Epoch 62/160\n",
      "4263/4263 [==============================] - 1s 240us/step - loss: 0.7459 - acc: 0.6753\n",
      "Epoch 63/160\n",
      "4263/4263 [==============================] - 1s 241us/step - loss: 0.7478 - acc: 0.6768\n",
      "Epoch 64/160\n",
      "4263/4263 [==============================] - 1s 247us/step - loss: 0.7374 - acc: 0.6775\n",
      "Epoch 65/160\n",
      "4263/4263 [==============================] - 1s 240us/step - loss: 0.7374 - acc: 0.6786\n",
      "Epoch 66/160\n",
      "4263/4263 [==============================] - 1s 231us/step - loss: 0.7296 - acc: 0.6843\n",
      "Epoch 67/160\n",
      "4263/4263 [==============================] - 1s 232us/step - loss: 0.7424 - acc: 0.6751\n",
      "Epoch 68/160\n",
      "4263/4263 [==============================] - 1s 238us/step - loss: 0.7410 - acc: 0.6782\n",
      "Epoch 69/160\n",
      "4263/4263 [==============================] - 1s 237us/step - loss: 0.7389 - acc: 0.6753\n",
      "Epoch 70/160\n",
      "4263/4263 [==============================] - 1s 242us/step - loss: 0.7325 - acc: 0.6796\n",
      "Epoch 71/160\n",
      "4263/4263 [==============================] - 1s 245us/step - loss: 0.7357 - acc: 0.6749\n",
      "Epoch 72/160\n",
      "4263/4263 [==============================] - 1s 237us/step - loss: 0.7373 - acc: 0.6847\n",
      "Epoch 73/160\n",
      "4263/4263 [==============================] - 1s 238us/step - loss: 0.7367 - acc: 0.6800\n",
      "Epoch 74/160\n",
      "4263/4263 [==============================] - 1s 234us/step - loss: 0.7483 - acc: 0.6800\n",
      "Epoch 75/160\n",
      "4263/4263 [==============================] - 1s 238us/step - loss: 0.7265 - acc: 0.6836\n",
      "Epoch 76/160\n",
      "4263/4263 [==============================] - 1s 247us/step - loss: 0.7382 - acc: 0.6800\n",
      "Epoch 77/160\n",
      "4263/4263 [==============================] - 1s 230us/step - loss: 0.7316 - acc: 0.6857\n",
      "Epoch 78/160\n",
      "4263/4263 [==============================] - 1s 241us/step - loss: 0.7316 - acc: 0.6854\n",
      "Epoch 79/160\n",
      "4263/4263 [==============================] - 1s 234us/step - loss: 0.7328 - acc: 0.6843\n",
      "Epoch 80/160\n",
      "4263/4263 [==============================] - 1s 236us/step - loss: 0.7343 - acc: 0.6784\n",
      "Epoch 81/160\n",
      "4263/4263 [==============================] - 1s 237us/step - loss: 0.7372 - acc: 0.6805\n",
      "Epoch 82/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 1s 239us/step - loss: 0.7285 - acc: 0.6843\n",
      "Epoch 83/160\n",
      "4263/4263 [==============================] - 1s 241us/step - loss: 0.7279 - acc: 0.6784\n",
      "Epoch 84/160\n",
      "4263/4263 [==============================] - 1s 244us/step - loss: 0.7239 - acc: 0.6838\n",
      "Epoch 85/160\n",
      "4263/4263 [==============================] - 1s 237us/step - loss: 0.7217 - acc: 0.6840\n",
      "Epoch 86/160\n",
      "4263/4263 [==============================] - 1s 234us/step - loss: 0.7197 - acc: 0.6887\n",
      "Epoch 87/160\n",
      "4263/4263 [==============================] - 1s 259us/step - loss: 0.7160 - acc: 0.6913\n",
      "Epoch 88/160\n",
      "4263/4263 [==============================] - 1s 278us/step - loss: 0.7207 - acc: 0.6845\n",
      "Epoch 89/160\n",
      "4263/4263 [==============================] - 1s 214us/step - loss: 0.7215 - acc: 0.6873\n",
      "Epoch 90/160\n",
      "4263/4263 [==============================] - 1s 217us/step - loss: 0.7266 - acc: 0.6829\n",
      "Epoch 91/160\n",
      "4263/4263 [==============================] - 1s 219us/step - loss: 0.7274 - acc: 0.6885\n",
      "Epoch 92/160\n",
      "4263/4263 [==============================] - 1s 216us/step - loss: 0.7124 - acc: 0.6969\n",
      "Epoch 93/160\n",
      "4263/4263 [==============================] - 1s 224us/step - loss: 0.7248 - acc: 0.6847\n",
      "Epoch 94/160\n",
      "4263/4263 [==============================] - 1s 217us/step - loss: 0.7364 - acc: 0.6885\n",
      "Epoch 95/160\n",
      "4263/4263 [==============================] - 1s 214us/step - loss: 0.7247 - acc: 0.6885\n",
      "Epoch 96/160\n",
      "4263/4263 [==============================] - 1s 211us/step - loss: 0.7119 - acc: 0.6936\n",
      "Epoch 97/160\n",
      "4263/4263 [==============================] - 1s 219us/step - loss: 0.7110 - acc: 0.6948\n",
      "Epoch 98/160\n",
      "4263/4263 [==============================] - 1s 214us/step - loss: 0.7154 - acc: 0.6887\n",
      "Epoch 99/160\n",
      "4263/4263 [==============================] - 1s 269us/step - loss: 0.7106 - acc: 0.6929\n",
      "Epoch 100/160\n",
      "4263/4263 [==============================] - 1s 279us/step - loss: 0.7163 - acc: 0.6854\n",
      "Epoch 101/160\n",
      "4263/4263 [==============================] - 1s 294us/step - loss: 0.7101 - acc: 0.6873\n",
      "Epoch 102/160\n",
      "4263/4263 [==============================] - 1s 226us/step - loss: 0.7061 - acc: 0.6953\n",
      "Epoch 103/160\n",
      "4263/4263 [==============================] - 1s 213us/step - loss: 0.7119 - acc: 0.6878\n",
      "Epoch 104/160\n",
      "4263/4263 [==============================] - 1s 215us/step - loss: 0.7088 - acc: 0.6974\n",
      "Epoch 105/160\n",
      "4263/4263 [==============================] - 1s 216us/step - loss: 0.7089 - acc: 0.6897\n",
      "Epoch 106/160\n",
      "4263/4263 [==============================] - 1s 218us/step - loss: 0.6976 - acc: 0.6908\n",
      "Epoch 107/160\n",
      "4263/4263 [==============================] - 1s 216us/step - loss: 0.7153 - acc: 0.6878\n",
      "Epoch 108/160\n",
      "4263/4263 [==============================] - 1s 214us/step - loss: 0.7080 - acc: 0.6932\n",
      "Epoch 109/160\n",
      "4263/4263 [==============================] - 1s 215us/step - loss: 0.7019 - acc: 0.6951\n",
      "Epoch 110/160\n",
      "4263/4263 [==============================] - 1s 209us/step - loss: 0.7086 - acc: 0.6901\n",
      "Epoch 111/160\n",
      "4263/4263 [==============================] - 1s 212us/step - loss: 0.6979 - acc: 0.7011\n",
      "Epoch 112/160\n",
      "4263/4263 [==============================] - 1s 212us/step - loss: 0.6935 - acc: 0.7014\n",
      "Epoch 113/160\n",
      "4263/4263 [==============================] - 1s 220us/step - loss: 0.7024 - acc: 0.6936\n",
      "Epoch 114/160\n",
      "4263/4263 [==============================] - 1s 226us/step - loss: 0.7070 - acc: 0.6897\n",
      "Epoch 115/160\n",
      "4263/4263 [==============================] - 1s 214us/step - loss: 0.7037 - acc: 0.6932\n",
      "Epoch 116/160\n",
      "4263/4263 [==============================] - 1s 215us/step - loss: 0.7035 - acc: 0.6936\n",
      "Epoch 117/160\n",
      "4263/4263 [==============================] - 1s 211us/step - loss: 0.6866 - acc: 0.7051\n",
      "Epoch 118/160\n",
      "4263/4263 [==============================] - 1s 212us/step - loss: 0.7023 - acc: 0.6946\n",
      "Epoch 119/160\n",
      "4263/4263 [==============================] - 1s 226us/step - loss: 0.6869 - acc: 0.6990\n",
      "Epoch 120/160\n",
      "4263/4263 [==============================] - 1s 222us/step - loss: 0.7040 - acc: 0.6920\n",
      "Epoch 121/160\n",
      "4263/4263 [==============================] - 1s 213us/step - loss: 0.7040 - acc: 0.6915\n",
      "Epoch 122/160\n",
      "4263/4263 [==============================] - 1s 216us/step - loss: 0.7018 - acc: 0.6943\n",
      "Epoch 123/160\n",
      "4263/4263 [==============================] - 1s 212us/step - loss: 0.6842 - acc: 0.7002\n",
      "Epoch 124/160\n",
      "4263/4263 [==============================] - 1s 217us/step - loss: 0.6885 - acc: 0.6969\n",
      "Epoch 125/160\n",
      "4263/4263 [==============================] - 1s 339us/step - loss: 0.6844 - acc: 0.7075\n",
      "Epoch 126/160\n",
      "4263/4263 [==============================] - 1s 263us/step - loss: 0.6868 - acc: 0.6997\n",
      "Epoch 127/160\n",
      "4263/4263 [==============================] - 1s 265us/step - loss: 0.6886 - acc: 0.7009\n",
      "Epoch 128/160\n",
      "4263/4263 [==============================] - 1s 247us/step - loss: 0.6945 - acc: 0.6943\n",
      "Epoch 129/160\n",
      "4263/4263 [==============================] - 1s 266us/step - loss: 0.6813 - acc: 0.7023\n",
      "Epoch 130/160\n",
      "4263/4263 [==============================] - 1s 279us/step - loss: 0.7040 - acc: 0.6967\n",
      "Epoch 131/160\n",
      "4263/4263 [==============================] - 1s 332us/step - loss: 0.6836 - acc: 0.6995\n",
      "Epoch 132/160\n",
      "4263/4263 [==============================] - 1s 237us/step - loss: 0.6836 - acc: 0.7058\n",
      "Epoch 133/160\n",
      "4263/4263 [==============================] - 1s 237us/step - loss: 0.6862 - acc: 0.7023\n",
      "Epoch 134/160\n",
      "4263/4263 [==============================] - 1s 255us/step - loss: 0.6889 - acc: 0.6990\n",
      "Epoch 135/160\n",
      "4263/4263 [==============================] - 1s 338us/step - loss: 0.6798 - acc: 0.7030\n",
      "Epoch 136/160\n",
      "4263/4263 [==============================] - 1s 271us/step - loss: 0.6839 - acc: 0.7014\n",
      "Epoch 137/160\n",
      "4263/4263 [==============================] - 1s 265us/step - loss: 0.6808 - acc: 0.7021\n",
      "Epoch 138/160\n",
      "4263/4263 [==============================] - 1s 247us/step - loss: 0.6975 - acc: 0.6901\n",
      "Epoch 139/160\n",
      "4263/4263 [==============================] - 1s 269us/step - loss: 0.6918 - acc: 0.6901\n",
      "Epoch 140/160\n",
      "4263/4263 [==============================] - 1s 258us/step - loss: 0.6688 - acc: 0.7061 0s - loss: 0.6597 - acc\n",
      "Epoch 141/160\n",
      "4263/4263 [==============================] - 1s 218us/step - loss: 0.6760 - acc: 0.7044\n",
      "Epoch 142/160\n",
      "4263/4263 [==============================] - 1s 220us/step - loss: 0.6952 - acc: 0.6995\n",
      "Epoch 143/160\n",
      "4263/4263 [==============================] - 1s 214us/step - loss: 0.6690 - acc: 0.7047\n",
      "Epoch 144/160\n",
      "4263/4263 [==============================] - 1s 211us/step - loss: 0.6669 - acc: 0.7103\n",
      "Epoch 145/160\n",
      "4263/4263 [==============================] - 1s 215us/step - loss: 0.6698 - acc: 0.7087\n",
      "Epoch 146/160\n",
      "4263/4263 [==============================] - 1s 213us/step - loss: 0.6786 - acc: 0.7009\n",
      "Epoch 147/160\n",
      "4263/4263 [==============================] - 1s 210us/step - loss: 0.6734 - acc: 0.7091\n",
      "Epoch 148/160\n",
      "4263/4263 [==============================] - 1s 219us/step - loss: 0.6640 - acc: 0.7117\n",
      "Epoch 149/160\n",
      "4263/4263 [==============================] - 1s 209us/step - loss: 0.6592 - acc: 0.7155\n",
      "Epoch 150/160\n",
      "4263/4263 [==============================] - 1s 212us/step - loss: 0.6535 - acc: 0.7101\n",
      "Epoch 151/160\n",
      "4263/4263 [==============================] - 1s 210us/step - loss: 0.6468 - acc: 0.7141\n",
      "Epoch 152/160\n",
      "4263/4263 [==============================] - 1s 207us/step - loss: 0.6600 - acc: 0.7143\n",
      "Epoch 153/160\n",
      "4263/4263 [==============================] - 1s 287us/step - loss: 0.6762 - acc: 0.7044\n",
      "Epoch 154/160\n",
      "4263/4263 [==============================] - 1s 288us/step - loss: 0.6888 - acc: 0.6969\n",
      "Epoch 155/160\n",
      "4263/4263 [==============================] - 1s 238us/step - loss: 0.6575 - acc: 0.7129\n",
      "Epoch 156/160\n",
      "4263/4263 [==============================] - 1s 277us/step - loss: 0.6773 - acc: 0.7019\n",
      "Epoch 157/160\n",
      "4263/4263 [==============================] - 1s 230us/step - loss: 0.6667 - acc: 0.7040\n",
      "Epoch 158/160\n",
      "4263/4263 [==============================] - 1s 231us/step - loss: 0.6547 - acc: 0.7129\n",
      "Epoch 159/160\n",
      "4263/4263 [==============================] - 1s 221us/step - loss: 0.6555 - acc: 0.7108\n",
      "Epoch 160/160\n",
      "4263/4263 [==============================] - 1s 254us/step - loss: 0.6407 - acc: 0.7185\n",
      "1066/1066 [==============================] - 2s 2ms/step\n",
      "4263/4263 [==============================] - 1s 248us/step\n",
      "\n",
      "acc: 66.42%\n",
      "\n",
      "acc: 72.39%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df['Class'].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d,y_d,test_size=0.2,random_state=42)\n",
    "\n",
    "y_train_2=pd.get_dummies(y_train)\n",
    "y_test_2=pd.get_dummies(y_test)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(700, activation='relu'))\n",
    "#model.add(Dense(1000, activation='relu'))\n",
    "#model.add(Dense(1200, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=160, callbacks=[early_stopping_monitor],batch_size=100)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "scores_2 = model.evaluate(X_train, y_train_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_2[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/160\n",
      "4263/4263 [==============================] - 7s 2ms/step - loss: 1.0328 - acc: 0.5602\n",
      "Epoch 2/160\n",
      "4263/4263 [==============================] - 3s 614us/step - loss: 0.8964 - acc: 0.6057\n",
      "Epoch 3/160\n",
      "4263/4263 [==============================] - 3s 610us/step - loss: 0.8741 - acc: 0.6188\n",
      "Epoch 4/160\n",
      "4263/4263 [==============================] - 3s 609us/step - loss: 0.8652 - acc: 0.6174\n",
      "Epoch 5/160\n",
      "4263/4263 [==============================] - 3s 601us/step - loss: 0.8501 - acc: 0.6294\n",
      "Epoch 6/160\n",
      "4263/4263 [==============================] - 3s 607us/step - loss: 0.8480 - acc: 0.6364\n",
      "Epoch 7/160\n",
      "4263/4263 [==============================] - 3s 596us/step - loss: 0.8396 - acc: 0.6371\n",
      "Epoch 8/160\n",
      "4263/4263 [==============================] - 3s 601us/step - loss: 0.8420 - acc: 0.6319\n",
      "Epoch 9/160\n",
      "4263/4263 [==============================] - 3s 594us/step - loss: 0.8272 - acc: 0.6418\n",
      "Epoch 10/160\n",
      "4263/4263 [==============================] - 3s 602us/step - loss: 0.8267 - acc: 0.6449\n",
      "Epoch 11/160\n",
      "4263/4263 [==============================] - 3s 594us/step - loss: 0.8116 - acc: 0.6524\n",
      "Epoch 12/160\n",
      "4263/4263 [==============================] - 3s 604us/step - loss: 0.8144 - acc: 0.6488\n",
      "Epoch 13/160\n",
      "4263/4263 [==============================] - 3s 594us/step - loss: 0.8097 - acc: 0.6495\n",
      "Epoch 14/160\n",
      "4263/4263 [==============================] - 3s 605us/step - loss: 0.8052 - acc: 0.6592\n",
      "Epoch 15/160\n",
      "4263/4263 [==============================] - 3s 603us/step - loss: 0.8014 - acc: 0.6561\n",
      "Epoch 16/160\n",
      "4263/4263 [==============================] - 3s 625us/step - loss: 0.8035 - acc: 0.6538\n",
      "Epoch 17/160\n",
      "4263/4263 [==============================] - 3s 609us/step - loss: 0.7990 - acc: 0.6566\n",
      "Epoch 18/160\n",
      "4263/4263 [==============================] - 3s 607us/step - loss: 0.7984 - acc: 0.6575\n",
      "Epoch 19/160\n",
      "4263/4263 [==============================] - 3s 600us/step - loss: 0.7938 - acc: 0.6531\n",
      "Epoch 20/160\n",
      "4263/4263 [==============================] - 3s 600us/step - loss: 0.7856 - acc: 0.6622\n",
      "Epoch 21/160\n",
      "4263/4263 [==============================] - 3s 602us/step - loss: 0.7827 - acc: 0.6592\n",
      "Epoch 22/160\n",
      "4263/4263 [==============================] - 3s 610us/step - loss: 0.7854 - acc: 0.6596\n",
      "Epoch 23/160\n",
      "4263/4263 [==============================] - 3s 601us/step - loss: 0.7775 - acc: 0.6622\n",
      "Epoch 24/160\n",
      "4263/4263 [==============================] - 3s 604us/step - loss: 0.7826 - acc: 0.6610\n",
      "Epoch 25/160\n",
      "4263/4263 [==============================] - 3s 596us/step - loss: 0.7799 - acc: 0.6615\n",
      "Epoch 26/160\n",
      "4263/4263 [==============================] - 2s 562us/step - loss: 0.7783 - acc: 0.6648\n",
      "Epoch 27/160\n",
      "4263/4263 [==============================] - 3s 601us/step - loss: 0.7778 - acc: 0.6610\n",
      "Epoch 28/160\n",
      "4263/4263 [==============================] - 3s 603us/step - loss: 0.7769 - acc: 0.6678\n",
      "Epoch 29/160\n",
      "4263/4263 [==============================] - 3s 655us/step - loss: 0.7755 - acc: 0.6707\n",
      "Epoch 30/160\n",
      "4263/4263 [==============================] - 3s 650us/step - loss: 0.7721 - acc: 0.6674\n",
      "Epoch 31/160\n",
      "4263/4263 [==============================] - 3s 804us/step - loss: 0.7728 - acc: 0.6683\n",
      "Epoch 32/160\n",
      "4263/4263 [==============================] - 3s 702us/step - loss: 0.7733 - acc: 0.6667\n",
      "Epoch 33/160\n",
      "4263/4263 [==============================] - 3s 778us/step - loss: 0.7725 - acc: 0.6610\n",
      "Epoch 34/160\n",
      "4263/4263 [==============================] - 3s 716us/step - loss: 0.7680 - acc: 0.6634\n",
      "Epoch 35/160\n",
      "4263/4263 [==============================] - 3s 637us/step - loss: 0.7809 - acc: 0.6599\n",
      "Epoch 36/160\n",
      "4263/4263 [==============================] - 3s 640us/step - loss: 0.7620 - acc: 0.6671\n",
      "Epoch 37/160\n",
      "4263/4263 [==============================] - 3s 629us/step - loss: 0.7695 - acc: 0.6622\n",
      "Epoch 38/160\n",
      "4263/4263 [==============================] - 3s 635us/step - loss: 0.7649 - acc: 0.6671\n",
      "Epoch 39/160\n",
      "4263/4263 [==============================] - 3s 610us/step - loss: 0.7631 - acc: 0.6702\n",
      "Epoch 40/160\n",
      "4263/4263 [==============================] - 3s 614us/step - loss: 0.7721 - acc: 0.6667\n",
      "Epoch 41/160\n",
      "4263/4263 [==============================] - 3s 612us/step - loss: 0.7635 - acc: 0.6763\n",
      "Epoch 42/160\n",
      "4263/4263 [==============================] - 3s 613us/step - loss: 0.7682 - acc: 0.6646\n",
      "Epoch 43/160\n",
      "4263/4263 [==============================] - 3s 614us/step - loss: 0.7628 - acc: 0.6695\n",
      "Epoch 44/160\n",
      "4263/4263 [==============================] - 3s 618us/step - loss: 0.7650 - acc: 0.6718\n",
      "Epoch 45/160\n",
      "4263/4263 [==============================] - 3s 606us/step - loss: 0.7593 - acc: 0.6667\n",
      "Epoch 46/160\n",
      "4263/4263 [==============================] - 3s 614us/step - loss: 0.7589 - acc: 0.6723\n",
      "Epoch 47/160\n",
      "4263/4263 [==============================] - 3s 606us/step - loss: 0.7587 - acc: 0.6671\n",
      "Epoch 48/160\n",
      "4263/4263 [==============================] - 3s 606us/step - loss: 0.7561 - acc: 0.6678\n",
      "Epoch 49/160\n",
      "4263/4263 [==============================] - 3s 608us/step - loss: 0.7613 - acc: 0.6690\n",
      "Epoch 50/160\n",
      "4263/4263 [==============================] - 3s 609us/step - loss: 0.7590 - acc: 0.6709\n",
      "Epoch 51/160\n",
      "4263/4263 [==============================] - 3s 610us/step - loss: 0.7578 - acc: 0.6711\n",
      "Epoch 52/160\n",
      "4263/4263 [==============================] - 3s 612us/step - loss: 0.7595 - acc: 0.6671\n",
      "Epoch 53/160\n",
      "4263/4263 [==============================] - 3s 609us/step - loss: 0.7575 - acc: 0.6732\n",
      "Epoch 54/160\n",
      "4263/4263 [==============================] - 3s 604us/step - loss: 0.7560 - acc: 0.6683\n",
      "Epoch 55/160\n",
      "4263/4263 [==============================] - 3s 615us/step - loss: 0.7641 - acc: 0.6685\n",
      "Epoch 56/160\n",
      "4263/4263 [==============================] - 3s 610us/step - loss: 0.7592 - acc: 0.6746\n",
      "Epoch 57/160\n",
      "4263/4263 [==============================] - 3s 604us/step - loss: 0.7601 - acc: 0.6657\n",
      "Epoch 58/160\n",
      "4263/4263 [==============================] - 3s 597us/step - loss: 0.7499 - acc: 0.6669\n",
      "Epoch 59/160\n",
      "4263/4263 [==============================] - 3s 632us/step - loss: 0.7506 - acc: 0.6707\n",
      "Epoch 60/160\n",
      "4263/4263 [==============================] - 3s 620us/step - loss: 0.7510 - acc: 0.6685\n",
      "Epoch 61/160\n",
      "4263/4263 [==============================] - 3s 617us/step - loss: 0.7543 - acc: 0.6742\n",
      "Epoch 62/160\n",
      "4263/4263 [==============================] - 3s 614us/step - loss: 0.7498 - acc: 0.6753\n",
      "Epoch 63/160\n",
      "4263/4263 [==============================] - 3s 617us/step - loss: 0.7508 - acc: 0.6779\n",
      "Epoch 64/160\n",
      "4263/4263 [==============================] - 3s 601us/step - loss: 0.7455 - acc: 0.6737\n",
      "Epoch 65/160\n",
      "4263/4263 [==============================] - 3s 610us/step - loss: 0.7483 - acc: 0.6765\n",
      "Epoch 66/160\n",
      "4263/4263 [==============================] - 3s 608us/step - loss: 0.7498 - acc: 0.6728\n",
      "Epoch 67/160\n",
      "4263/4263 [==============================] - 3s 607us/step - loss: 0.7490 - acc: 0.6714\n",
      "Epoch 68/160\n",
      "4263/4263 [==============================] - 3s 599us/step - loss: 0.7572 - acc: 0.6732\n",
      "Epoch 69/160\n",
      "4263/4263 [==============================] - 3s 625us/step - loss: 0.7423 - acc: 0.6716\n",
      "Epoch 70/160\n",
      "4263/4263 [==============================] - 3s 650us/step - loss: 0.7449 - acc: 0.6728\n",
      "Epoch 71/160\n",
      "4263/4263 [==============================] - 3s 612us/step - loss: 0.7437 - acc: 0.6756\n",
      "Epoch 72/160\n",
      "4263/4263 [==============================] - 3s 637us/step - loss: 0.7404 - acc: 0.6784\n",
      "Epoch 73/160\n",
      "4263/4263 [==============================] - 3s 598us/step - loss: 0.7493 - acc: 0.6707\n",
      "Epoch 74/160\n",
      "4263/4263 [==============================] - 3s 602us/step - loss: 0.7489 - acc: 0.6793\n",
      "Epoch 75/160\n",
      "4263/4263 [==============================] - 3s 663us/step - loss: 0.7467 - acc: 0.6805\n",
      "Epoch 76/160\n",
      "4263/4263 [==============================] - 3s 640us/step - loss: 0.7466 - acc: 0.6742\n",
      "Epoch 77/160\n",
      "4263/4263 [==============================] - 3s 610us/step - loss: 0.7420 - acc: 0.6760\n",
      "Epoch 78/160\n",
      "4263/4263 [==============================] - 3s 612us/step - loss: 0.7421 - acc: 0.6793\n",
      "Epoch 79/160\n",
      "4263/4263 [==============================] - 3s 601us/step - loss: 0.7427 - acc: 0.6777\n",
      "Epoch 80/160\n",
      "4263/4263 [==============================] - 3s 611us/step - loss: 0.7358 - acc: 0.6812\n",
      "Epoch 81/160\n",
      "4263/4263 [==============================] - 3s 604us/step - loss: 0.7418 - acc: 0.6782\n",
      "Epoch 82/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 3s 619us/step - loss: 0.7466 - acc: 0.6782\n",
      "Epoch 83/160\n",
      "4263/4263 [==============================] - 3s 604us/step - loss: 0.7406 - acc: 0.6800\n",
      "Epoch 84/160\n",
      "4263/4263 [==============================] - 3s 612us/step - loss: 0.7422 - acc: 0.6751\n",
      "Epoch 85/160\n",
      "4263/4263 [==============================] - 3s 607us/step - loss: 0.7331 - acc: 0.6749\n",
      "Epoch 86/160\n",
      "4263/4263 [==============================] - 3s 614us/step - loss: 0.7517 - acc: 0.6782\n",
      "Epoch 87/160\n",
      "4263/4263 [==============================] - 3s 601us/step - loss: 0.7463 - acc: 0.6775\n",
      "Epoch 88/160\n",
      "4263/4263 [==============================] - 3s 607us/step - loss: 0.7369 - acc: 0.6829\n",
      "Epoch 89/160\n",
      "4263/4263 [==============================] - 3s 613us/step - loss: 0.7357 - acc: 0.6782\n",
      "Epoch 90/160\n",
      "4263/4263 [==============================] - 3s 623us/step - loss: 0.7355 - acc: 0.6885\n",
      "Epoch 91/160\n",
      "4263/4263 [==============================] - 3s 614us/step - loss: 0.7353 - acc: 0.6833\n",
      "Epoch 92/160\n",
      "4263/4263 [==============================] - 3s 611us/step - loss: 0.7468 - acc: 0.6779\n",
      "Epoch 93/160\n",
      "4263/4263 [==============================] - 3s 618us/step - loss: 0.7319 - acc: 0.6840\n",
      "Epoch 94/160\n",
      "4263/4263 [==============================] - 3s 615us/step - loss: 0.7292 - acc: 0.6878\n",
      "Epoch 95/160\n",
      "4263/4263 [==============================] - 3s 616us/step - loss: 0.7274 - acc: 0.6852\n",
      "Epoch 96/160\n",
      "4263/4263 [==============================] - 3s 622us/step - loss: 0.7339 - acc: 0.6826\n",
      "Epoch 97/160\n",
      "4263/4263 [==============================] - 3s 617us/step - loss: 0.7283 - acc: 0.6859\n",
      "Epoch 98/160\n",
      "4263/4263 [==============================] - 3s 622us/step - loss: 0.7246 - acc: 0.6843\n",
      "Epoch 99/160\n",
      "4263/4263 [==============================] - 3s 615us/step - loss: 0.7340 - acc: 0.6880\n",
      "Epoch 100/160\n",
      "4263/4263 [==============================] - 3s 612us/step - loss: 0.7252 - acc: 0.6890\n",
      "Epoch 101/160\n",
      "4263/4263 [==============================] - 3s 612us/step - loss: 0.7310 - acc: 0.6772\n",
      "Epoch 102/160\n",
      "4263/4263 [==============================] - 3s 607us/step - loss: 0.7265 - acc: 0.6875\n",
      "Epoch 103/160\n",
      "4263/4263 [==============================] - 3s 616us/step - loss: 0.7291 - acc: 0.6861\n",
      "Epoch 104/160\n",
      "4263/4263 [==============================] - 3s 602us/step - loss: 0.7286 - acc: 0.6831\n",
      "Epoch 105/160\n",
      "4263/4263 [==============================] - 3s 609us/step - loss: 0.7235 - acc: 0.6934\n",
      "Epoch 106/160\n",
      "4263/4263 [==============================] - 3s 603us/step - loss: 0.7276 - acc: 0.6915\n",
      "Epoch 107/160\n",
      "4263/4263 [==============================] - 3s 609us/step - loss: 0.7329 - acc: 0.6847\n",
      "Epoch 108/160\n",
      "4263/4263 [==============================] - 3s 605us/step - loss: 0.7277 - acc: 0.6840\n",
      "Epoch 109/160\n",
      "4263/4263 [==============================] - 3s 611us/step - loss: 0.7263 - acc: 0.6875\n",
      "Epoch 110/160\n",
      "4263/4263 [==============================] - 3s 608us/step - loss: 0.7259 - acc: 0.6821\n",
      "Epoch 111/160\n",
      "4263/4263 [==============================] - 3s 615us/step - loss: 0.7306 - acc: 0.6868\n",
      "Epoch 112/160\n",
      "4263/4263 [==============================] - 3s 640us/step - loss: 0.7180 - acc: 0.6875\n",
      "Epoch 113/160\n",
      "4263/4263 [==============================] - 3s 629us/step - loss: 0.7219 - acc: 0.6833\n",
      "Epoch 114/160\n",
      "4263/4263 [==============================] - 3s 615us/step - loss: 0.7201 - acc: 0.6892\n",
      "Epoch 115/160\n",
      "4263/4263 [==============================] - 3s 615us/step - loss: 0.7258 - acc: 0.6843\n",
      "Epoch 116/160\n",
      "4263/4263 [==============================] - 3s 615us/step - loss: 0.7163 - acc: 0.6901\n",
      "Epoch 117/160\n",
      "4263/4263 [==============================] - 3s 623us/step - loss: 0.7237 - acc: 0.6880\n",
      "Epoch 118/160\n",
      "4263/4263 [==============================] - 3s 621us/step - loss: 0.7152 - acc: 0.6913\n",
      "Epoch 119/160\n",
      "4263/4263 [==============================] - 3s 607us/step - loss: 0.7163 - acc: 0.6920\n",
      "Epoch 120/160\n",
      "4263/4263 [==============================] - 3s 623us/step - loss: 0.7210 - acc: 0.6857\n",
      "Epoch 121/160\n",
      "4263/4263 [==============================] - 3s 609us/step - loss: 0.7153 - acc: 0.6929\n",
      "Epoch 122/160\n",
      "4263/4263 [==============================] - 3s 616us/step - loss: 0.7058 - acc: 0.6953\n",
      "Epoch 123/160\n",
      "4263/4263 [==============================] - 3s 610us/step - loss: 0.7174 - acc: 0.6840\n",
      "Epoch 124/160\n",
      "4263/4263 [==============================] - 3s 616us/step - loss: 0.7223 - acc: 0.6906\n",
      "Epoch 125/160\n",
      "4263/4263 [==============================] - 3s 604us/step - loss: 0.7135 - acc: 0.6960\n",
      "Epoch 126/160\n",
      "4263/4263 [==============================] - 3s 615us/step - loss: 0.7187 - acc: 0.6838\n",
      "Epoch 127/160\n",
      "4263/4263 [==============================] - 3s 610us/step - loss: 0.7172 - acc: 0.6861\n",
      "Epoch 128/160\n",
      "4263/4263 [==============================] - 3s 617us/step - loss: 0.7157 - acc: 0.6868\n",
      "Epoch 129/160\n",
      "4263/4263 [==============================] - 3s 625us/step - loss: 0.7078 - acc: 0.6922\n",
      "Epoch 130/160\n",
      "4263/4263 [==============================] - 3s 627us/step - loss: 0.7104 - acc: 0.6925\n",
      "Epoch 131/160\n",
      "4263/4263 [==============================] - 3s 615us/step - loss: 0.7190 - acc: 0.6943\n",
      "Epoch 132/160\n",
      "4263/4263 [==============================] - 3s 661us/step - loss: 0.7031 - acc: 0.6892\n",
      "Epoch 133/160\n",
      "4263/4263 [==============================] - 3s 720us/step - loss: 0.7164 - acc: 0.6880\n",
      "Epoch 134/160\n",
      "4263/4263 [==============================] - 3s 692us/step - loss: 0.7106 - acc: 0.6925\n",
      "Epoch 135/160\n",
      "4263/4263 [==============================] - 3s 675us/step - loss: 0.7125 - acc: 0.6904\n",
      "Epoch 136/160\n",
      "4263/4263 [==============================] - 3s 684us/step - loss: 0.7169 - acc: 0.6861\n",
      "Epoch 137/160\n",
      "4263/4263 [==============================] - 3s 688us/step - loss: 0.7118 - acc: 0.6901\n",
      "Epoch 138/160\n",
      "4263/4263 [==============================] - 3s 642us/step - loss: 0.7119 - acc: 0.6878\n",
      "Epoch 139/160\n",
      "4263/4263 [==============================] - 3s 593us/step - loss: 0.7181 - acc: 0.6929\n",
      "Epoch 140/160\n",
      "4263/4263 [==============================] - 3s 664us/step - loss: 0.7009 - acc: 0.6981\n",
      "Epoch 141/160\n",
      "4263/4263 [==============================] - 3s 674us/step - loss: 0.6998 - acc: 0.6943\n",
      "Epoch 142/160\n",
      "4263/4263 [==============================] - 3s 624us/step - loss: 0.6982 - acc: 0.6953\n",
      "Epoch 143/160\n",
      "4263/4263 [==============================] - 3s 611us/step - loss: 0.7000 - acc: 0.6981\n",
      "Epoch 144/160\n",
      "4263/4263 [==============================] - 3s 613us/step - loss: 0.7052 - acc: 0.6939\n",
      "Epoch 145/160\n",
      "4263/4263 [==============================] - 3s 601us/step - loss: 0.6986 - acc: 0.6941\n",
      "Epoch 146/160\n",
      "4263/4263 [==============================] - 3s 608us/step - loss: 0.7122 - acc: 0.6951\n",
      "Epoch 147/160\n",
      "4263/4263 [==============================] - 3s 605us/step - loss: 0.7063 - acc: 0.6918\n",
      "Epoch 148/160\n",
      "4263/4263 [==============================] - 3s 608us/step - loss: 0.7011 - acc: 0.6981\n",
      "Epoch 149/160\n",
      "4263/4263 [==============================] - 3s 600us/step - loss: 0.6884 - acc: 0.7080\n",
      "Epoch 150/160\n",
      "4263/4263 [==============================] - 3s 611us/step - loss: 0.7052 - acc: 0.6955\n",
      "Epoch 151/160\n",
      "4263/4263 [==============================] - 3s 608us/step - loss: 0.7000 - acc: 0.6932\n",
      "Epoch 152/160\n",
      "4263/4263 [==============================] - 3s 611us/step - loss: 0.6926 - acc: 0.7023\n",
      "Epoch 153/160\n",
      "4263/4263 [==============================] - 3s 607us/step - loss: 0.6983 - acc: 0.6955\n",
      "Epoch 154/160\n",
      "4263/4263 [==============================] - 3s 639us/step - loss: 0.6953 - acc: 0.7014\n",
      "Epoch 155/160\n",
      "4263/4263 [==============================] - 3s 618us/step - loss: 0.6867 - acc: 0.6960\n",
      "Epoch 156/160\n",
      "4263/4263 [==============================] - 3s 668us/step - loss: 0.6860 - acc: 0.7023\n",
      "Epoch 157/160\n",
      "4263/4263 [==============================] - 3s 616us/step - loss: 0.7142 - acc: 0.6894\n",
      "Epoch 158/160\n",
      "4263/4263 [==============================] - 3s 608us/step - loss: 0.6874 - acc: 0.7021\n",
      "Epoch 159/160\n",
      "4263/4263 [==============================] - 3s 616us/step - loss: 0.6970 - acc: 0.7009\n",
      "Epoch 160/160\n",
      "4263/4263 [==============================] - 3s 608us/step - loss: 0.6901 - acc: 0.6979\n",
      "1066/1066 [==============================] - 2s 2ms/step\n",
      "4263/4263 [==============================] - 1s 175us/step\n",
      "\n",
      "acc: 67.07%\n",
      "\n",
      "acc: 71.19%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df['Class'].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d,y_d,test_size=0.2,random_state=42)\n",
    "\n",
    "y_train_2=pd.get_dummies(y_train)\n",
    "y_test_2=pd.get_dummies(y_test)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(700, activation='relu'))\n",
    "#model.add(Dense(1000, activation='relu'))\n",
    "#model.add(Dense(1200, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=160, callbacks=[early_stopping_monitor],batch_size=25)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "scores_2 = model.evaluate(X_train, y_train_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_2[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/160\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 1.0399 - acc: 0.5632\n",
      "Epoch 2/160\n",
      "4263/4263 [==============================] - 1s 330us/step - loss: 0.9026 - acc: 0.6061\n",
      "Epoch 3/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.8773 - acc: 0.6132\n",
      "Epoch 4/160\n",
      "4263/4263 [==============================] - 1s 316us/step - loss: 0.8582 - acc: 0.6226\n",
      "Epoch 5/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.8494 - acc: 0.6345\n",
      "Epoch 6/160\n",
      "4263/4263 [==============================] - 1s 345us/step - loss: 0.8498 - acc: 0.6298\n",
      "Epoch 7/160\n",
      "4263/4263 [==============================] - 1s 334us/step - loss: 0.8324 - acc: 0.6406\n",
      "Epoch 8/160\n",
      "4263/4263 [==============================] - 1s 330us/step - loss: 0.8476 - acc: 0.6343\n",
      "Epoch 9/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.8302 - acc: 0.6364\n",
      "Epoch 10/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.8225 - acc: 0.6437\n",
      "Epoch 11/160\n",
      "4263/4263 [==============================] - 1s 327us/step - loss: 0.8245 - acc: 0.6404\n",
      "Epoch 12/160\n",
      "4263/4263 [==============================] - 1s 319us/step - loss: 0.8130 - acc: 0.6481\n",
      "Epoch 13/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.8124 - acc: 0.6493\n",
      "Epoch 14/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.8078 - acc: 0.6502\n",
      "Epoch 15/160\n",
      "4263/4263 [==============================] - 1s 333us/step - loss: 0.8110 - acc: 0.6540\n",
      "Epoch 16/160\n",
      "4263/4263 [==============================] - 1s 330us/step - loss: 0.8034 - acc: 0.6559\n",
      "Epoch 17/160\n",
      "4263/4263 [==============================] - 2s 363us/step - loss: 0.8027 - acc: 0.6517\n",
      "Epoch 18/160\n",
      "4263/4263 [==============================] - 1s 326us/step - loss: 0.7947 - acc: 0.6556\n",
      "Epoch 19/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.7981 - acc: 0.6554\n",
      "Epoch 20/160\n",
      "4263/4263 [==============================] - 1s 325us/step - loss: 0.7931 - acc: 0.6526\n",
      "Epoch 21/160\n",
      "4263/4263 [==============================] - 1s 343us/step - loss: 0.7831 - acc: 0.6578\n",
      "Epoch 22/160\n",
      "4263/4263 [==============================] - 1s 332us/step - loss: 0.7861 - acc: 0.6610\n",
      "Epoch 23/160\n",
      "4263/4263 [==============================] - 2s 363us/step - loss: 0.7854 - acc: 0.6617\n",
      "Epoch 24/160\n",
      "4263/4263 [==============================] - 1s 339us/step - loss: 0.7776 - acc: 0.6641\n",
      "Epoch 25/160\n",
      "4263/4263 [==============================] - 1s 326us/step - loss: 0.7826 - acc: 0.6631\n",
      "Epoch 26/160\n",
      "4263/4263 [==============================] - 2s 378us/step - loss: 0.7682 - acc: 0.6671\n",
      "Epoch 27/160\n",
      "4263/4263 [==============================] - 2s 356us/step - loss: 0.7819 - acc: 0.6606\n",
      "Epoch 28/160\n",
      "4263/4263 [==============================] - 2s 375us/step - loss: 0.7837 - acc: 0.6629\n",
      "Epoch 29/160\n",
      "4263/4263 [==============================] - 1s 327us/step - loss: 0.7757 - acc: 0.6580\n",
      "Epoch 30/160\n",
      "4263/4263 [==============================] - 1s 328us/step - loss: 0.7684 - acc: 0.6669\n",
      "Epoch 31/160\n",
      "4263/4263 [==============================] - 1s 329us/step - loss: 0.7747 - acc: 0.6627\n",
      "Epoch 32/160\n",
      "4263/4263 [==============================] - 1s 332us/step - loss: 0.7626 - acc: 0.6669\n",
      "Epoch 33/160\n",
      "4263/4263 [==============================] - 1s 331us/step - loss: 0.7736 - acc: 0.6589\n",
      "Epoch 34/160\n",
      "4263/4263 [==============================] - 1s 328us/step - loss: 0.7812 - acc: 0.6610\n",
      "Epoch 35/160\n",
      "4263/4263 [==============================] - 1s 335us/step - loss: 0.7686 - acc: 0.6690\n",
      "Epoch 36/160\n",
      "4263/4263 [==============================] - 1s 328us/step - loss: 0.7699 - acc: 0.6678\n",
      "Epoch 37/160\n",
      "4263/4263 [==============================] - 1s 332us/step - loss: 0.7681 - acc: 0.6566\n",
      "Epoch 38/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.7697 - acc: 0.6589\n",
      "Epoch 39/160\n",
      "4263/4263 [==============================] - 1s 327us/step - loss: 0.7627 - acc: 0.6692\n",
      "Epoch 40/160\n",
      "4263/4263 [==============================] - 1s 331us/step - loss: 0.7620 - acc: 0.6681\n",
      "Epoch 41/160\n",
      "4263/4263 [==============================] - 1s 331us/step - loss: 0.7616 - acc: 0.6646\n",
      "Epoch 42/160\n",
      "4263/4263 [==============================] - 1s 325us/step - loss: 0.7594 - acc: 0.6613\n",
      "Epoch 43/160\n",
      "4263/4263 [==============================] - 1s 327us/step - loss: 0.7633 - acc: 0.6662\n",
      "Epoch 44/160\n",
      "4263/4263 [==============================] - 1s 328us/step - loss: 0.7613 - acc: 0.6685\n",
      "Epoch 45/160\n",
      "4263/4263 [==============================] - 1s 325us/step - loss: 0.7520 - acc: 0.6735\n",
      "Epoch 46/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.7529 - acc: 0.6692\n",
      "Epoch 47/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.7524 - acc: 0.6760\n",
      "Epoch 48/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.7532 - acc: 0.6732\n",
      "Epoch 49/160\n",
      "4263/4263 [==============================] - 1s 329us/step - loss: 0.7530 - acc: 0.6725\n",
      "Epoch 50/160\n",
      "4263/4263 [==============================] - 1s 329us/step - loss: 0.7577 - acc: 0.6718\n",
      "Epoch 51/160\n",
      "4263/4263 [==============================] - 1s 326us/step - loss: 0.7624 - acc: 0.6603\n",
      "Epoch 52/160\n",
      "4263/4263 [==============================] - 1s 325us/step - loss: 0.7523 - acc: 0.6695\n",
      "Epoch 53/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.7568 - acc: 0.6791\n",
      "Epoch 54/160\n",
      "4263/4263 [==============================] - 1s 326us/step - loss: 0.7507 - acc: 0.6667\n",
      "Epoch 55/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.7608 - acc: 0.6749\n",
      "Epoch 56/160\n",
      "4263/4263 [==============================] - 1s 327us/step - loss: 0.7483 - acc: 0.6810\n",
      "Epoch 57/160\n",
      "4263/4263 [==============================] - 1s 322us/step - loss: 0.7501 - acc: 0.6718\n",
      "Epoch 58/160\n",
      "4263/4263 [==============================] - 1s 328us/step - loss: 0.7537 - acc: 0.6744\n",
      "Epoch 59/160\n",
      "4263/4263 [==============================] - 1s 330us/step - loss: 0.7469 - acc: 0.6784\n",
      "Epoch 60/160\n",
      "4263/4263 [==============================] - 1s 325us/step - loss: 0.7489 - acc: 0.6739\n",
      "Epoch 61/160\n",
      "4263/4263 [==============================] - 1s 329us/step - loss: 0.7585 - acc: 0.6671\n",
      "Epoch 62/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.7469 - acc: 0.6770\n",
      "Epoch 63/160\n",
      "4263/4263 [==============================] - 1s 327us/step - loss: 0.7453 - acc: 0.6742\n",
      "Epoch 64/160\n",
      "4263/4263 [==============================] - 1s 327us/step - loss: 0.7479 - acc: 0.6777\n",
      "Epoch 65/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.7455 - acc: 0.6739\n",
      "Epoch 66/160\n",
      "4263/4263 [==============================] - 1s 332us/step - loss: 0.7463 - acc: 0.6746\n",
      "Epoch 67/160\n",
      "4263/4263 [==============================] - 1s 346us/step - loss: 0.7449 - acc: 0.6770\n",
      "Epoch 68/160\n",
      "4263/4263 [==============================] - 1s 334us/step - loss: 0.7463 - acc: 0.6692\n",
      "Epoch 69/160\n",
      "4263/4263 [==============================] - 1s 327us/step - loss: 0.7405 - acc: 0.6730\n",
      "Epoch 70/160\n",
      "4263/4263 [==============================] - 1s 329us/step - loss: 0.7463 - acc: 0.6786\n",
      "Epoch 71/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.7424 - acc: 0.6772\n",
      "Epoch 72/160\n",
      "4263/4263 [==============================] - 1s 322us/step - loss: 0.7428 - acc: 0.6749\n",
      "Epoch 73/160\n",
      "4263/4263 [==============================] - 1s 334us/step - loss: 0.7516 - acc: 0.6749\n",
      "Epoch 74/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.7389 - acc: 0.6784\n",
      "Epoch 75/160\n",
      "4263/4263 [==============================] - 1s 327us/step - loss: 0.7373 - acc: 0.6793\n",
      "Epoch 76/160\n",
      "4263/4263 [==============================] - 1s 330us/step - loss: 0.7325 - acc: 0.6831\n",
      "Epoch 77/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.7328 - acc: 0.6791\n",
      "Epoch 78/160\n",
      "4263/4263 [==============================] - 1s 330us/step - loss: 0.7385 - acc: 0.6836\n",
      "Epoch 79/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.7362 - acc: 0.6758\n",
      "Epoch 80/160\n",
      "4263/4263 [==============================] - 1s 325us/step - loss: 0.7333 - acc: 0.6770\n",
      "Epoch 81/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.7428 - acc: 0.6786\n",
      "Epoch 82/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 1s 317us/step - loss: 0.7390 - acc: 0.6857\n",
      "Epoch 83/160\n",
      "4263/4263 [==============================] - 1s 317us/step - loss: 0.7403 - acc: 0.6821\n",
      "Epoch 84/160\n",
      "4263/4263 [==============================] - 1s 316us/step - loss: 0.7298 - acc: 0.6847\n",
      "Epoch 85/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.7307 - acc: 0.6866\n",
      "Epoch 86/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.7274 - acc: 0.6833\n",
      "Epoch 87/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.7278 - acc: 0.6866\n",
      "Epoch 88/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.7347 - acc: 0.6789\n",
      "Epoch 89/160\n",
      "4263/4263 [==============================] - 1s 317us/step - loss: 0.7271 - acc: 0.6875\n",
      "Epoch 90/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.7442 - acc: 0.6786\n",
      "Epoch 91/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.7279 - acc: 0.6922\n",
      "Epoch 92/160\n",
      "4263/4263 [==============================] - 1s 317us/step - loss: 0.7298 - acc: 0.6887\n",
      "Epoch 93/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.7330 - acc: 0.6770\n",
      "Epoch 94/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.7238 - acc: 0.6812\n",
      "Epoch 95/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.7232 - acc: 0.6894\n",
      "Epoch 96/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.7193 - acc: 0.6932\n",
      "Epoch 97/160\n",
      "4263/4263 [==============================] - 1s 317us/step - loss: 0.7151 - acc: 0.6875\n",
      "Epoch 98/160\n",
      "4263/4263 [==============================] - 1s 319us/step - loss: 0.7200 - acc: 0.6932\n",
      "Epoch 99/160\n",
      "4263/4263 [==============================] - 1s 316us/step - loss: 0.7244 - acc: 0.6812\n",
      "Epoch 100/160\n",
      "4263/4263 [==============================] - 1s 313us/step - loss: 0.7314 - acc: 0.6850\n",
      "Epoch 101/160\n",
      "4263/4263 [==============================] - 1s 314us/step - loss: 0.7205 - acc: 0.6861\n",
      "Epoch 102/160\n",
      "4263/4263 [==============================] - 1s 317us/step - loss: 0.7250 - acc: 0.6857\n",
      "Epoch 103/160\n",
      "4263/4263 [==============================] - 1s 315us/step - loss: 0.7161 - acc: 0.6925\n",
      "Epoch 104/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.7217 - acc: 0.6882\n",
      "Epoch 105/160\n",
      "4263/4263 [==============================] - 1s 312us/step - loss: 0.7182 - acc: 0.6887\n",
      "Epoch 106/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.7192 - acc: 0.6854\n",
      "Epoch 107/160\n",
      "4263/4263 [==============================] - 1s 316us/step - loss: 0.7123 - acc: 0.6939\n",
      "Epoch 108/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.7161 - acc: 0.6854\n",
      "Epoch 109/160\n",
      "4263/4263 [==============================] - 1s 319us/step - loss: 0.7328 - acc: 0.6786\n",
      "Epoch 110/160\n",
      "4263/4263 [==============================] - 1s 313us/step - loss: 0.7084 - acc: 0.6925\n",
      "Epoch 111/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.7180 - acc: 0.6866\n",
      "Epoch 112/160\n",
      "4263/4263 [==============================] - 1s 315us/step - loss: 0.7062 - acc: 0.6955\n",
      "Epoch 113/160\n",
      "4263/4263 [==============================] - 1s 319us/step - loss: 0.7085 - acc: 0.6875\n",
      "Epoch 114/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.7065 - acc: 0.6934\n",
      "Epoch 115/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.7130 - acc: 0.6958\n",
      "Epoch 116/160\n",
      "4263/4263 [==============================] - 1s 315us/step - loss: 0.7067 - acc: 0.6955\n",
      "Epoch 117/160\n",
      "4263/4263 [==============================] - 1s 317us/step - loss: 0.7029 - acc: 0.6969\n",
      "Epoch 118/160\n",
      "4263/4263 [==============================] - 1s 310us/step - loss: 0.6962 - acc: 0.6981\n",
      "Epoch 119/160\n",
      "4263/4263 [==============================] - 1s 315us/step - loss: 0.7069 - acc: 0.6927\n",
      "Epoch 120/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.7051 - acc: 0.6915\n",
      "Epoch 121/160\n",
      "4263/4263 [==============================] - 1s 315us/step - loss: 0.7173 - acc: 0.6798\n",
      "Epoch 122/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.7120 - acc: 0.6894\n",
      "Epoch 123/160\n",
      "4263/4263 [==============================] - 1s 310us/step - loss: 0.7100 - acc: 0.6929\n",
      "Epoch 124/160\n",
      "4263/4263 [==============================] - 1s 314us/step - loss: 0.7012 - acc: 0.6986\n",
      "Epoch 125/160\n",
      "4263/4263 [==============================] - 1s 314us/step - loss: 0.7025 - acc: 0.6981\n",
      "Epoch 126/160\n",
      "4263/4263 [==============================] - 1s 317us/step - loss: 0.7000 - acc: 0.6911\n",
      "Epoch 127/160\n",
      "4263/4263 [==============================] - 1s 319us/step - loss: 0.6959 - acc: 0.7021\n",
      "Epoch 128/160\n",
      "4263/4263 [==============================] - 1s 316us/step - loss: 0.6959 - acc: 0.7014\n",
      "Epoch 129/160\n",
      "4263/4263 [==============================] - 1s 315us/step - loss: 0.7037 - acc: 0.7023\n",
      "Epoch 130/160\n",
      "4263/4263 [==============================] - 1s 315us/step - loss: 0.6944 - acc: 0.6981\n",
      "Epoch 131/160\n",
      "4263/4263 [==============================] - 1s 315us/step - loss: 0.7051 - acc: 0.6962\n",
      "Epoch 132/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.7115 - acc: 0.6885\n",
      "Epoch 133/160\n",
      "4263/4263 [==============================] - 1s 315us/step - loss: 0.6924 - acc: 0.6958\n",
      "Epoch 134/160\n",
      "4263/4263 [==============================] - 1s 316us/step - loss: 0.6942 - acc: 0.6986\n",
      "Epoch 135/160\n",
      "4263/4263 [==============================] - 1s 314us/step - loss: 0.6944 - acc: 0.6958\n",
      "Epoch 136/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.6969 - acc: 0.6974\n",
      "Epoch 137/160\n",
      "4263/4263 [==============================] - 1s 314us/step - loss: 0.6907 - acc: 0.6988\n",
      "Epoch 138/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.6875 - acc: 0.7051\n",
      "Epoch 139/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.6910 - acc: 0.7000\n",
      "Epoch 140/160\n",
      "4263/4263 [==============================] - 1s 316us/step - loss: 0.6887 - acc: 0.7016\n",
      "Epoch 141/160\n",
      "4263/4263 [==============================] - 1s 312us/step - loss: 0.6856 - acc: 0.7044\n",
      "Epoch 142/160\n",
      "4263/4263 [==============================] - 1s 314us/step - loss: 0.6820 - acc: 0.7000\n",
      "Epoch 143/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.6931 - acc: 0.6990\n",
      "Epoch 144/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.6820 - acc: 0.7042\n",
      "Epoch 145/160\n",
      "4263/4263 [==============================] - 1s 319us/step - loss: 0.6748 - acc: 0.7122\n",
      "Epoch 146/160\n",
      "4263/4263 [==============================] - 1s 315us/step - loss: 0.6951 - acc: 0.6967\n",
      "Epoch 147/160\n",
      "4263/4263 [==============================] - 1s 319us/step - loss: 0.6756 - acc: 0.7084\n",
      "Epoch 148/160\n",
      "4263/4263 [==============================] - 1s 328us/step - loss: 0.6957 - acc: 0.7004\n",
      "Epoch 149/160\n",
      "4263/4263 [==============================] - 1s 338us/step - loss: 0.6837 - acc: 0.7080\n",
      "Epoch 150/160\n",
      "4263/4263 [==============================] - 1s 345us/step - loss: 0.6887 - acc: 0.6976\n",
      "Epoch 151/160\n",
      "4263/4263 [==============================] - 1s 335us/step - loss: 0.6906 - acc: 0.6967\n",
      "Epoch 152/160\n",
      "4263/4263 [==============================] - 1s 332us/step - loss: 0.6841 - acc: 0.7028\n",
      "Epoch 153/160\n",
      "4263/4263 [==============================] - 1s 326us/step - loss: 0.6746 - acc: 0.7108\n",
      "Epoch 154/160\n",
      "4263/4263 [==============================] - 1s 332us/step - loss: 0.6772 - acc: 0.7065\n",
      "Epoch 155/160\n",
      "4263/4263 [==============================] - 1s 339us/step - loss: 0.6721 - acc: 0.7136\n",
      "Epoch 156/160\n",
      "4263/4263 [==============================] - 1s 336us/step - loss: 0.6711 - acc: 0.7070\n",
      "Epoch 157/160\n",
      "4263/4263 [==============================] - 1s 330us/step - loss: 0.6956 - acc: 0.6986\n",
      "Epoch 158/160\n",
      "4263/4263 [==============================] - 1s 325us/step - loss: 0.6692 - acc: 0.7110\n",
      "Epoch 159/160\n",
      "4263/4263 [==============================] - 1s 330us/step - loss: 0.6692 - acc: 0.7096\n",
      "Epoch 160/160\n",
      "4263/4263 [==============================] - 1s 330us/step - loss: 0.6683 - acc: 0.7087\n",
      "1066/1066 [==============================] - 2s 2ms/step\n",
      "4263/4263 [==============================] - 1s 176us/step\n",
      "\n",
      "acc: 68.11%\n",
      "\n",
      "acc: 72.41%\n",
      "[[2.9637709e-01 6.5718114e-01 2.9870786e-02 1.6570909e-02]\n",
      " [1.0061311e-01 8.7958688e-01 1.9789351e-02 1.0608734e-05]\n",
      " [3.5630036e-02 9.4838524e-01 1.5982747e-02 1.9750025e-06]\n",
      " ...\n",
      " [2.6572853e-01 6.7697698e-01 5.3240057e-02 4.0543578e-03]\n",
      " [2.3154166e-01 7.4539021e-02 1.2809869e-05 6.9390649e-01]\n",
      " [3.9975783e-01 5.6086582e-01 2.5055222e-02 1.4321171e-02]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df['Class'].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d,y_d,test_size=0.2,random_state=42)\n",
    "\n",
    "y_train_2=pd.get_dummies(y_train)\n",
    "y_test_2=pd.get_dummies(y_test)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(700, activation='relu'))\n",
    "#model.add(Dense(1000, activation='relu'))\n",
    "#model.add(Dense(1200, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=160, callbacks=[early_stopping_monitor],batch_size=50)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "scores_2 = model.evaluate(X_train, y_train_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "predictions=model.predict(X_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_2[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/160\n",
      "4263/4263 [==============================] - 6s 1ms/step - loss: 1.0431 - acc: 0.5651\n",
      "Epoch 2/160\n",
      "4263/4263 [==============================] - 1s 330us/step - loss: 0.9117 - acc: 0.5921\n",
      "Epoch 3/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.8887 - acc: 0.6043\n",
      "Epoch 4/160\n",
      "4263/4263 [==============================] - 1s 325us/step - loss: 0.8727 - acc: 0.6195\n",
      "Epoch 5/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.8565 - acc: 0.6198\n",
      "Epoch 6/160\n",
      "4263/4263 [==============================] - 1s 340us/step - loss: 0.8396 - acc: 0.6308\n",
      "Epoch 7/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.8510 - acc: 0.6312\n",
      "Epoch 8/160\n",
      "4263/4263 [==============================] - 1s 331us/step - loss: 0.8386 - acc: 0.6406\n",
      "Epoch 9/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.8313 - acc: 0.6366\n",
      "Epoch 10/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.8244 - acc: 0.6411\n",
      "Epoch 11/160\n",
      "4263/4263 [==============================] - 2s 361us/step - loss: 0.8200 - acc: 0.6460\n",
      "Epoch 12/160\n",
      "4263/4263 [==============================] - 1s 322us/step - loss: 0.8168 - acc: 0.6465\n",
      "Epoch 13/160\n",
      "4263/4263 [==============================] - 1s 339us/step - loss: 0.8261 - acc: 0.6413\n",
      "Epoch 14/160\n",
      "4263/4263 [==============================] - 1s 340us/step - loss: 0.8138 - acc: 0.6463\n",
      "Epoch 15/160\n",
      "4263/4263 [==============================] - 2s 378us/step - loss: 0.8047 - acc: 0.6488\n",
      "Epoch 16/160\n",
      "4263/4263 [==============================] - 2s 368us/step - loss: 0.8054 - acc: 0.6573\n",
      "Epoch 17/160\n",
      "4263/4263 [==============================] - 1s 339us/step - loss: 0.8198 - acc: 0.6463\n",
      "Epoch 18/160\n",
      "4263/4263 [==============================] - 1s 344us/step - loss: 0.7904 - acc: 0.6573\n",
      "Epoch 19/160\n",
      "4263/4263 [==============================] - 1s 326us/step - loss: 0.8003 - acc: 0.6582\n",
      "Epoch 20/160\n",
      "4263/4263 [==============================] - 1s 322us/step - loss: 0.7916 - acc: 0.6622\n",
      "Epoch 21/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.7922 - acc: 0.6561\n",
      "Epoch 22/160\n",
      "4263/4263 [==============================] - 2s 367us/step - loss: 0.7855 - acc: 0.6554\n",
      "Epoch 23/160\n",
      "4263/4263 [==============================] - 1s 336us/step - loss: 0.7873 - acc: 0.6587\n",
      "Epoch 24/160\n",
      "4263/4263 [==============================] - 2s 377us/step - loss: 0.7858 - acc: 0.6603\n",
      "Epoch 25/160\n",
      "4263/4263 [==============================] - 1s 343us/step - loss: 0.7800 - acc: 0.6634\n",
      "Epoch 26/160\n",
      "4263/4263 [==============================] - 1s 329us/step - loss: 0.7756 - acc: 0.6641\n",
      "Epoch 27/160\n",
      "4263/4263 [==============================] - 1s 329us/step - loss: 0.7846 - acc: 0.6603\n",
      "Epoch 28/160\n",
      "4263/4263 [==============================] - 1s 325us/step - loss: 0.7839 - acc: 0.6578\n",
      "Epoch 29/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.7800 - acc: 0.6662\n",
      "Epoch 30/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.7714 - acc: 0.6639\n",
      "Epoch 31/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.7751 - acc: 0.6622\n",
      "Epoch 32/160\n",
      "4263/4263 [==============================] - 1s 322us/step - loss: 0.7637 - acc: 0.6707\n",
      "Epoch 33/160\n",
      "4263/4263 [==============================] - 1s 328us/step - loss: 0.7800 - acc: 0.6601\n",
      "Epoch 34/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.7714 - acc: 0.6627\n",
      "Epoch 35/160\n",
      "4263/4263 [==============================] - 1s 332us/step - loss: 0.7713 - acc: 0.6629\n",
      "Epoch 36/160\n",
      "4263/4263 [==============================] - 1s 326us/step - loss: 0.7716 - acc: 0.6653\n",
      "Epoch 37/160\n",
      "4263/4263 [==============================] - 1s 326us/step - loss: 0.7681 - acc: 0.6634\n",
      "Epoch 38/160\n",
      "4263/4263 [==============================] - 1s 333us/step - loss: 0.7676 - acc: 0.6664\n",
      "Epoch 39/160\n",
      "4263/4263 [==============================] - 1s 332us/step - loss: 0.7729 - acc: 0.6610\n",
      "Epoch 40/160\n",
      "4263/4263 [==============================] - 1s 328us/step - loss: 0.7655 - acc: 0.6662\n",
      "Epoch 41/160\n",
      "4263/4263 [==============================] - 1s 322us/step - loss: 0.7606 - acc: 0.6667\n",
      "Epoch 42/160\n",
      "4263/4263 [==============================] - 1s 325us/step - loss: 0.7660 - acc: 0.6650\n",
      "Epoch 43/160\n",
      "4263/4263 [==============================] - 1s 330us/step - loss: 0.7623 - acc: 0.6669\n",
      "Epoch 44/160\n",
      "4263/4263 [==============================] - 1s 331us/step - loss: 0.7613 - acc: 0.6634\n",
      "Epoch 45/160\n",
      "4263/4263 [==============================] - 1s 327us/step - loss: 0.7621 - acc: 0.6746\n",
      "Epoch 46/160\n",
      "4263/4263 [==============================] - 1s 331us/step - loss: 0.7592 - acc: 0.6746\n",
      "Epoch 47/160\n",
      "4263/4263 [==============================] - 1s 331us/step - loss: 0.7620 - acc: 0.6660\n",
      "Epoch 48/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.7636 - acc: 0.6582\n",
      "Epoch 49/160\n",
      "4263/4263 [==============================] - 1s 342us/step - loss: 0.7614 - acc: 0.6685\n",
      "Epoch 50/160\n",
      "4263/4263 [==============================] - 1s 334us/step - loss: 0.7575 - acc: 0.6709\n",
      "Epoch 51/160\n",
      "4263/4263 [==============================] - 1s 334us/step - loss: 0.7621 - acc: 0.6683\n",
      "Epoch 52/160\n",
      "4263/4263 [==============================] - 1s 322us/step - loss: 0.7552 - acc: 0.6660\n",
      "Epoch 53/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.7563 - acc: 0.6711\n",
      "Epoch 54/160\n",
      "4263/4263 [==============================] - 1s 328us/step - loss: 0.7614 - acc: 0.6685\n",
      "Epoch 55/160\n",
      "4263/4263 [==============================] - 1s 325us/step - loss: 0.7549 - acc: 0.6744\n",
      "Epoch 56/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.7685 - acc: 0.6578\n",
      "Epoch 57/160\n",
      "4263/4263 [==============================] - 1s 329us/step - loss: 0.7529 - acc: 0.6751\n",
      "Epoch 58/160\n",
      "4263/4263 [==============================] - 1s 329us/step - loss: 0.7520 - acc: 0.6671\n",
      "Epoch 59/160\n",
      "4263/4263 [==============================] - 1s 322us/step - loss: 0.7573 - acc: 0.6697\n",
      "Epoch 60/160\n",
      "4263/4263 [==============================] - 1s 330us/step - loss: 0.7607 - acc: 0.6725\n",
      "Epoch 61/160\n",
      "4263/4263 [==============================] - 1s 332us/step - loss: 0.7566 - acc: 0.6709\n",
      "Epoch 62/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.7545 - acc: 0.6695\n",
      "Epoch 63/160\n",
      "4263/4263 [==============================] - 1s 327us/step - loss: 0.7517 - acc: 0.6749\n",
      "Epoch 64/160\n",
      "4263/4263 [==============================] - 1s 328us/step - loss: 0.7534 - acc: 0.6746\n",
      "Epoch 65/160\n",
      "4263/4263 [==============================] - 1s 332us/step - loss: 0.7423 - acc: 0.6772\n",
      "Epoch 66/160\n",
      "4263/4263 [==============================] - 1s 325us/step - loss: 0.7486 - acc: 0.6786\n",
      "Epoch 67/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.7551 - acc: 0.6648\n",
      "Epoch 68/160\n",
      "4263/4263 [==============================] - 1s 325us/step - loss: 0.7491 - acc: 0.6765\n",
      "Epoch 69/160\n",
      "4263/4263 [==============================] - 1s 332us/step - loss: 0.7515 - acc: 0.6772\n",
      "Epoch 70/160\n",
      "4263/4263 [==============================] - 1s 327us/step - loss: 0.7515 - acc: 0.6700\n",
      "Epoch 71/160\n",
      "4263/4263 [==============================] - 1s 329us/step - loss: 0.7515 - acc: 0.6742\n",
      "Epoch 72/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.7433 - acc: 0.6784\n",
      "Epoch 73/160\n",
      "4263/4263 [==============================] - 1s 328us/step - loss: 0.7500 - acc: 0.6730\n",
      "Epoch 74/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.7405 - acc: 0.6824\n",
      "Epoch 75/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.7428 - acc: 0.6681\n",
      "Epoch 76/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.7418 - acc: 0.6866\n",
      "Epoch 77/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.7401 - acc: 0.6782\n",
      "Epoch 78/160\n",
      "4263/4263 [==============================] - 1s 326us/step - loss: 0.7423 - acc: 0.6760\n",
      "Epoch 79/160\n",
      "4263/4263 [==============================] - 1s 328us/step - loss: 0.7421 - acc: 0.6753\n",
      "Epoch 80/160\n",
      "4263/4263 [==============================] - 1s 325us/step - loss: 0.7406 - acc: 0.6772\n",
      "Epoch 81/160\n",
      "4263/4263 [==============================] - 1s 325us/step - loss: 0.7360 - acc: 0.6847\n",
      "Epoch 82/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.7359 - acc: 0.6779\n",
      "Epoch 83/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.7325 - acc: 0.6833\n",
      "Epoch 84/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.7397 - acc: 0.6768\n",
      "Epoch 85/160\n",
      "4263/4263 [==============================] - 1s 327us/step - loss: 0.7332 - acc: 0.6826\n",
      "Epoch 86/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.7351 - acc: 0.6789\n",
      "Epoch 87/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.7363 - acc: 0.6791\n",
      "Epoch 88/160\n",
      "4263/4263 [==============================] - 1s 322us/step - loss: 0.7320 - acc: 0.6730\n",
      "Epoch 89/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.7405 - acc: 0.6803\n",
      "Epoch 90/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.7304 - acc: 0.6821\n",
      "Epoch 91/160\n",
      "4263/4263 [==============================] - 1s 326us/step - loss: 0.7345 - acc: 0.6749\n",
      "Epoch 92/160\n",
      "4263/4263 [==============================] - 1s 329us/step - loss: 0.7303 - acc: 0.6836\n",
      "Epoch 93/160\n",
      "4263/4263 [==============================] - 1s 319us/step - loss: 0.7256 - acc: 0.6911\n",
      "Epoch 94/160\n",
      "4263/4263 [==============================] - 1s 326us/step - loss: 0.7240 - acc: 0.6875\n",
      "Epoch 95/160\n",
      "4263/4263 [==============================] - 1s 327us/step - loss: 0.7222 - acc: 0.6875\n",
      "Epoch 96/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.7338 - acc: 0.6833\n",
      "Epoch 97/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.7301 - acc: 0.6840\n",
      "Epoch 98/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.7294 - acc: 0.6821\n",
      "Epoch 99/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.7282 - acc: 0.6866\n",
      "Epoch 100/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.7202 - acc: 0.6920\n",
      "Epoch 101/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.7340 - acc: 0.6821\n",
      "Epoch 102/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.7191 - acc: 0.6906\n",
      "Epoch 103/160\n",
      "4263/4263 [==============================] - 1s 319us/step - loss: 0.7158 - acc: 0.6908\n",
      "Epoch 104/160\n",
      "4263/4263 [==============================] - 1s 326us/step - loss: 0.7278 - acc: 0.6854\n",
      "Epoch 105/160\n",
      "4263/4263 [==============================] - 1s 319us/step - loss: 0.7320 - acc: 0.6810\n",
      "Epoch 106/160\n",
      "4263/4263 [==============================] - 1s 313us/step - loss: 0.7263 - acc: 0.6873\n",
      "Epoch 107/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.7221 - acc: 0.6833\n",
      "Epoch 108/160\n",
      "4263/4263 [==============================] - 1s 325us/step - loss: 0.7149 - acc: 0.6908\n",
      "Epoch 109/160\n",
      "4263/4263 [==============================] - 1s 315us/step - loss: 0.7124 - acc: 0.6904\n",
      "Epoch 110/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.7193 - acc: 0.6854\n",
      "Epoch 111/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.7270 - acc: 0.6850\n",
      "Epoch 112/160\n",
      "4263/4263 [==============================] - 1s 332us/step - loss: 0.7139 - acc: 0.6920\n",
      "Epoch 113/160\n",
      "4263/4263 [==============================] - 1s 330us/step - loss: 0.7185 - acc: 0.6887\n",
      "Epoch 114/160\n",
      "4263/4263 [==============================] - 1s 322us/step - loss: 0.7130 - acc: 0.6906\n",
      "Epoch 115/160\n",
      "4263/4263 [==============================] - 1s 322us/step - loss: 0.7121 - acc: 0.6925\n",
      "Epoch 116/160\n",
      "4263/4263 [==============================] - 1s 319us/step - loss: 0.7213 - acc: 0.6833\n",
      "Epoch 117/160\n",
      "4263/4263 [==============================] - 1s 315us/step - loss: 0.7211 - acc: 0.6843\n",
      "Epoch 118/160\n",
      "4263/4263 [==============================] - 1s 322us/step - loss: 0.7078 - acc: 0.6976\n",
      "Epoch 119/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.7085 - acc: 0.6986\n",
      "Epoch 120/160\n",
      "4263/4263 [==============================] - 1s 322us/step - loss: 0.7049 - acc: 0.6953\n",
      "Epoch 121/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.7082 - acc: 0.6951\n",
      "Epoch 122/160\n",
      "4263/4263 [==============================] - 1s 317us/step - loss: 0.7091 - acc: 0.6920\n",
      "Epoch 123/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.7044 - acc: 0.6995\n",
      "Epoch 124/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.7050 - acc: 0.6986\n",
      "Epoch 125/160\n",
      "4263/4263 [==============================] - 1s 319us/step - loss: 0.7078 - acc: 0.6901\n",
      "Epoch 126/160\n",
      "4263/4263 [==============================] - 1s 319us/step - loss: 0.7101 - acc: 0.6925\n",
      "Epoch 127/160\n",
      "4263/4263 [==============================] - 1s 315us/step - loss: 0.7083 - acc: 0.6939\n",
      "Epoch 128/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.7036 - acc: 0.6983\n",
      "Epoch 129/160\n",
      "4263/4263 [==============================] - 1s 317us/step - loss: 0.7106 - acc: 0.6972\n",
      "Epoch 130/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.7038 - acc: 0.6990\n",
      "Epoch 131/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.6984 - acc: 0.6946\n",
      "Epoch 132/160\n",
      "4263/4263 [==============================] - 1s 322us/step - loss: 0.7047 - acc: 0.6958\n",
      "Epoch 133/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.6931 - acc: 0.7030\n",
      "Epoch 134/160\n",
      "4263/4263 [==============================] - 1s 315us/step - loss: 0.7023 - acc: 0.6934\n",
      "Epoch 135/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.6959 - acc: 0.6972\n",
      "Epoch 136/160\n",
      "4263/4263 [==============================] - 1s 315us/step - loss: 0.6996 - acc: 0.6986\n",
      "Epoch 137/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.7000 - acc: 0.6960\n",
      "Epoch 138/160\n",
      "4263/4263 [==============================] - 1s 319us/step - loss: 0.6985 - acc: 0.6927\n",
      "Epoch 139/160\n",
      "4263/4263 [==============================] - 1s 317us/step - loss: 0.6899 - acc: 0.6965\n",
      "Epoch 140/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.7048 - acc: 0.7035\n",
      "Epoch 141/160\n",
      "4263/4263 [==============================] - 1s 322us/step - loss: 0.6904 - acc: 0.7014\n",
      "Epoch 142/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.6923 - acc: 0.7007\n",
      "Epoch 143/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.6891 - acc: 0.6988\n",
      "Epoch 144/160\n",
      "4263/4263 [==============================] - 1s 314us/step - loss: 0.7073 - acc: 0.6918\n",
      "Epoch 145/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.6949 - acc: 0.6946\n",
      "Epoch 146/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.6870 - acc: 0.7014\n",
      "Epoch 147/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.6912 - acc: 0.6988\n",
      "Epoch 148/160\n",
      "4263/4263 [==============================] - 1s 317us/step - loss: 0.7073 - acc: 0.6920\n",
      "Epoch 149/160\n",
      "4263/4263 [==============================] - 1s 315us/step - loss: 0.6846 - acc: 0.7035\n",
      "Epoch 150/160\n",
      "4263/4263 [==============================] - 1s 317us/step - loss: 0.6805 - acc: 0.7047\n",
      "Epoch 151/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.6886 - acc: 0.6972\n",
      "Epoch 152/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.6845 - acc: 0.7068\n",
      "Epoch 153/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.6952 - acc: 0.7009\n",
      "Epoch 154/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.6920 - acc: 0.6979\n",
      "Epoch 155/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.6761 - acc: 0.7070\n",
      "Epoch 156/160\n",
      "4263/4263 [==============================] - 1s 317us/step - loss: 0.6789 - acc: 0.7047\n",
      "Epoch 157/160\n",
      "4263/4263 [==============================] - 1s 314us/step - loss: 0.6829 - acc: 0.7014\n",
      "Epoch 158/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.6780 - acc: 0.6995\n",
      "Epoch 159/160\n",
      "4263/4263 [==============================] - 1s 315us/step - loss: 0.6774 - acc: 0.7058\n",
      "Epoch 160/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.6822 - acc: 0.6988\n",
      "1066/1066 [==============================] - 2s 2ms/step\n",
      "4263/4263 [==============================] - 1s 161us/step\n",
      "\n",
      "acc: 68.01%\n",
      "\n",
      "acc: 71.10%\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df['Class'].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d,y_d,test_size=0.2,random_state=42)\n",
    "\n",
    "y_train_2=pd.get_dummies(y_train)\n",
    "y_test_2=pd.get_dummies(y_test)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(700, activation='relu'))\n",
    "#model.add(Dense(1000, activation='relu'))\n",
    "#model.add(Dense(1200, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=160, callbacks=[early_stopping_monitor],batch_size=50)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "scores_2 = model.evaluate(X_train, y_train_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "predictions=model.predict(X_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_2[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/160\n",
      "4263/4263 [==============================] - 7s 2ms/step - loss: 0.4566 - acc: 0.7862\n",
      "Epoch 2/160\n",
      "4263/4263 [==============================] - 2s 369us/step - loss: 0.4020 - acc: 0.8089\n",
      "Epoch 3/160\n",
      "4263/4263 [==============================] - 1s 328us/step - loss: 0.3874 - acc: 0.8101\n",
      "Epoch 4/160\n",
      "4263/4263 [==============================] - 1s 326us/step - loss: 0.3833 - acc: 0.8159\n",
      "Epoch 5/160\n",
      "4263/4263 [==============================] - 1s 332us/step - loss: 0.3793 - acc: 0.8172\n",
      "Epoch 6/160\n",
      "4263/4263 [==============================] - 1s 337us/step - loss: 0.3773 - acc: 0.8180\n",
      "Epoch 7/160\n",
      "4263/4263 [==============================] - 1s 346us/step - loss: 0.3742 - acc: 0.8212\n",
      "Epoch 8/160\n",
      "4263/4263 [==============================] - 1s 345us/step - loss: 0.3712 - acc: 0.8249\n",
      "Epoch 9/160\n",
      "4263/4263 [==============================] - 1s 331us/step - loss: 0.3717 - acc: 0.8227\n",
      "Epoch 10/160\n",
      "4263/4263 [==============================] - 2s 360us/step - loss: 0.3661 - acc: 0.8261\n",
      "Epoch 11/160\n",
      "4263/4263 [==============================] - 2s 366us/step - loss: 0.3661 - acc: 0.8298\n",
      "Epoch 12/160\n",
      "4263/4263 [==============================] - 2s 365us/step - loss: 0.3670 - acc: 0.8257\n",
      "Epoch 13/160\n",
      "4263/4263 [==============================] - 2s 359us/step - loss: 0.3628 - acc: 0.8300\n",
      "Epoch 14/160\n",
      "4263/4263 [==============================] - 1s 335us/step - loss: 0.3658 - acc: 0.8272\n",
      "Epoch 15/160\n",
      "4263/4263 [==============================] - 1s 351us/step - loss: 0.3595 - acc: 0.8298\n",
      "Epoch 16/160\n",
      "4263/4263 [==============================] - 2s 360us/step - loss: 0.3608 - acc: 0.8304\n",
      "Epoch 17/160\n",
      "4263/4263 [==============================] - 2s 371us/step - loss: 0.3579 - acc: 0.8300\n",
      "Epoch 18/160\n",
      "4263/4263 [==============================] - 2s 394us/step - loss: 0.3588 - acc: 0.8314\n",
      "Epoch 19/160\n",
      "4263/4263 [==============================] - 2s 373us/step - loss: 0.3556 - acc: 0.8327\n",
      "Epoch 20/160\n",
      "4263/4263 [==============================] - 1s 331us/step - loss: 0.3622 - acc: 0.8285\n",
      "Epoch 21/160\n",
      "4263/4263 [==============================] - 1s 333us/step - loss: 0.3545 - acc: 0.8330\n",
      "Epoch 22/160\n",
      "4263/4263 [==============================] - 1s 328us/step - loss: 0.3525 - acc: 0.8304\n",
      "Epoch 23/160\n",
      "4263/4263 [==============================] - 1s 340us/step - loss: 0.3537 - acc: 0.8340\n",
      "Epoch 24/160\n",
      "4263/4263 [==============================] - 1s 333us/step - loss: 0.3545 - acc: 0.8353\n",
      "Epoch 25/160\n",
      "4263/4263 [==============================] - 1s 331us/step - loss: 0.3532 - acc: 0.8329\n",
      "Epoch 26/160\n",
      "4263/4263 [==============================] - 2s 375us/step - loss: 0.3523 - acc: 0.8350\n",
      "Epoch 27/160\n",
      "4263/4263 [==============================] - 2s 420us/step - loss: 0.3513 - acc: 0.8355\n",
      "Epoch 28/160\n",
      "4263/4263 [==============================] - 2s 447us/step - loss: 0.3478 - acc: 0.8346\n",
      "Epoch 29/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.3500 - acc: 0.8347\n",
      "Epoch 30/160\n",
      "4263/4263 [==============================] - 2s 412us/step - loss: 0.3505 - acc: 0.8334\n",
      "Epoch 31/160\n",
      "4263/4263 [==============================] - 2s 388us/step - loss: 0.3488 - acc: 0.8362\n",
      "Epoch 32/160\n",
      "4263/4263 [==============================] - 2s 375us/step - loss: 0.3476 - acc: 0.8350\n",
      "Epoch 33/160\n",
      "4263/4263 [==============================] - 1s 347us/step - loss: 0.3503 - acc: 0.8344\n",
      "Epoch 34/160\n",
      "4263/4263 [==============================] - 1s 333us/step - loss: 0.3493 - acc: 0.8337\n",
      "Epoch 35/160\n",
      "4263/4263 [==============================] - 2s 359us/step - loss: 0.3471 - acc: 0.8349\n",
      "Epoch 36/160\n",
      "4263/4263 [==============================] - 1s 335us/step - loss: 0.3472 - acc: 0.8350\n",
      "Epoch 37/160\n",
      "4263/4263 [==============================] - 1s 345us/step - loss: 0.3491 - acc: 0.8346\n",
      "Epoch 38/160\n",
      "4263/4263 [==============================] - 1s 336us/step - loss: 0.3490 - acc: 0.8349\n",
      "Epoch 39/160\n",
      "4263/4263 [==============================] - 1s 344us/step - loss: 0.3525 - acc: 0.8316\n",
      "Epoch 40/160\n",
      "4263/4263 [==============================] - 1s 336us/step - loss: 0.3518 - acc: 0.8332\n",
      "Epoch 41/160\n",
      "4263/4263 [==============================] - 1s 342us/step - loss: 0.3454 - acc: 0.8370\n",
      "Epoch 42/160\n",
      "4263/4263 [==============================] - 2s 417us/step - loss: 0.3479 - acc: 0.8346\n",
      "Epoch 43/160\n",
      "4263/4263 [==============================] - 1s 342us/step - loss: 0.3455 - acc: 0.8373\n",
      "Epoch 44/160\n",
      "4263/4263 [==============================] - 1s 348us/step - loss: 0.3449 - acc: 0.8380\n",
      "Epoch 45/160\n",
      "4263/4263 [==============================] - 1s 312us/step - loss: 0.3482 - acc: 0.8340\n",
      "Epoch 46/160\n",
      "4263/4263 [==============================] - 2s 401us/step - loss: 0.3446 - acc: 0.8364\n",
      "Epoch 47/160\n",
      "4263/4263 [==============================] - 2s 372us/step - loss: 0.3466 - acc: 0.8357\n",
      "Epoch 48/160\n",
      "4263/4263 [==============================] - 2s 362us/step - loss: 0.3431 - acc: 0.8401\n",
      "Epoch 49/160\n",
      "4263/4263 [==============================] - 1s 334us/step - loss: 0.3426 - acc: 0.8390\n",
      "Epoch 50/160\n",
      "4263/4263 [==============================] - 1s 340us/step - loss: 0.3418 - acc: 0.8366\n",
      "Epoch 51/160\n",
      "4263/4263 [==============================] - 1s 338us/step - loss: 0.3418 - acc: 0.8389\n",
      "Epoch 52/160\n",
      "4263/4263 [==============================] - 1s 343us/step - loss: 0.3409 - acc: 0.8397\n",
      "Epoch 53/160\n",
      "4263/4263 [==============================] - 1s 334us/step - loss: 0.3476 - acc: 0.8345\n",
      "Epoch 54/160\n",
      "4263/4263 [==============================] - 2s 357us/step - loss: 0.3449 - acc: 0.8350\n",
      "Epoch 55/160\n",
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.3439 - acc: 0.8371\n",
      "Epoch 56/160\n",
      "4263/4263 [==============================] - 2s 374us/step - loss: 0.3426 - acc: 0.8377\n",
      "Epoch 57/160\n",
      "4263/4263 [==============================] - 2s 411us/step - loss: 0.3458 - acc: 0.8356\n",
      "Epoch 58/160\n",
      "4263/4263 [==============================] - 2s 363us/step - loss: 0.3392 - acc: 0.8399\n",
      "Epoch 59/160\n",
      "4263/4263 [==============================] - 2s 362us/step - loss: 0.3400 - acc: 0.8356\n",
      "Epoch 60/160\n",
      "4263/4263 [==============================] - 1s 345us/step - loss: 0.3426 - acc: 0.8378\n",
      "Epoch 61/160\n",
      "4263/4263 [==============================] - 1s 346us/step - loss: 0.3440 - acc: 0.8390\n",
      "Epoch 62/160\n",
      "4263/4263 [==============================] - 2s 353us/step - loss: 0.3405 - acc: 0.8408\n",
      "Epoch 63/160\n",
      "4263/4263 [==============================] - 1s 348us/step - loss: 0.3409 - acc: 0.8379\n",
      "Epoch 64/160\n",
      "4263/4263 [==============================] - 2s 357us/step - loss: 0.3381 - acc: 0.8394\n",
      "Epoch 65/160\n",
      "4263/4263 [==============================] - 1s 348us/step - loss: 0.3397 - acc: 0.8393\n",
      "Epoch 66/160\n",
      "4263/4263 [==============================] - 2s 378us/step - loss: 0.3421 - acc: 0.8391\n",
      "Epoch 67/160\n",
      "4263/4263 [==============================] - 2s 439us/step - loss: 0.3401 - acc: 0.8406\n",
      "Epoch 68/160\n",
      "4263/4263 [==============================] - 2s 504us/step - loss: 0.3421 - acc: 0.8404\n",
      "Epoch 69/160\n",
      "4263/4263 [==============================] - 2s 493us/step - loss: 0.3382 - acc: 0.8398\n",
      "Epoch 70/160\n",
      "4263/4263 [==============================] - 2s 451us/step - loss: 0.3406 - acc: 0.8389\n",
      "Epoch 71/160\n",
      "4263/4263 [==============================] - 2s 432us/step - loss: 0.3385 - acc: 0.8369\n",
      "Epoch 72/160\n",
      "4263/4263 [==============================] - 2s 421us/step - loss: 0.3367 - acc: 0.8406\n",
      "Epoch 73/160\n",
      "4263/4263 [==============================] - 2s 460us/step - loss: 0.3380 - acc: 0.8429\n",
      "Epoch 74/160\n",
      "4263/4263 [==============================] - 2s 429us/step - loss: 0.3384 - acc: 0.8394\n",
      "Epoch 75/160\n",
      "4263/4263 [==============================] - 2s 363us/step - loss: 0.3384 - acc: 0.8391\n",
      "Epoch 76/160\n",
      "4263/4263 [==============================] - 2s 414us/step - loss: 0.3384 - acc: 0.8417\n",
      "Epoch 77/160\n",
      "4263/4263 [==============================] - 1s 343us/step - loss: 0.3405 - acc: 0.8387\n",
      "Epoch 78/160\n",
      "4263/4263 [==============================] - 1s 336us/step - loss: 0.3374 - acc: 0.8392\n",
      "Epoch 79/160\n",
      "4263/4263 [==============================] - 1s 347us/step - loss: 0.3375 - acc: 0.8427\n",
      "Epoch 80/160\n",
      "4263/4263 [==============================] - 1s 343us/step - loss: 0.3394 - acc: 0.8403\n",
      "Epoch 81/160\n",
      "4263/4263 [==============================] - 1s 343us/step - loss: 0.3372 - acc: 0.8418\n",
      "Epoch 82/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 2s 414us/step - loss: 0.3363 - acc: 0.8428\n",
      "Epoch 83/160\n",
      "4263/4263 [==============================] - 2s 442us/step - loss: 0.3381 - acc: 0.8411\n",
      "Epoch 84/160\n",
      "4263/4263 [==============================] - 2s 451us/step - loss: 0.3387 - acc: 0.8400\n",
      "Epoch 85/160\n",
      "4263/4263 [==============================] - 2s 436us/step - loss: 0.3363 - acc: 0.8414\n",
      "Epoch 86/160\n",
      "4263/4263 [==============================] - 2s 430us/step - loss: 0.3340 - acc: 0.8433\n",
      "Epoch 87/160\n",
      "4263/4263 [==============================] - 2s 434us/step - loss: 0.3362 - acc: 0.8424\n",
      "Epoch 88/160\n",
      "4263/4263 [==============================] - 2s 542us/step - loss: 0.3366 - acc: 0.8398\n",
      "Epoch 89/160\n",
      "4263/4263 [==============================] - 2s 468us/step - loss: 0.3323 - acc: 0.8451\n",
      "Epoch 90/160\n",
      "4263/4263 [==============================] - 2s 446us/step - loss: 0.3392 - acc: 0.8384\n",
      "Epoch 91/160\n",
      "4263/4263 [==============================] - 2s 417us/step - loss: 0.3333 - acc: 0.8454\n",
      "Epoch 92/160\n",
      "4263/4263 [==============================] - 1s 334us/step - loss: 0.3338 - acc: 0.8437\n",
      "Epoch 93/160\n",
      "4263/4263 [==============================] - 2s 375us/step - loss: 0.3339 - acc: 0.8434\n",
      "Epoch 94/160\n",
      "4263/4263 [==============================] - 1s 331us/step - loss: 0.3307 - acc: 0.8477\n",
      "Epoch 95/160\n",
      "4263/4263 [==============================] - 1s 338us/step - loss: 0.3343 - acc: 0.8439\n",
      "Epoch 96/160\n",
      "4263/4263 [==============================] - 2s 421us/step - loss: 0.3353 - acc: 0.8421\n",
      "Epoch 97/160\n",
      "4263/4263 [==============================] - 1s 348us/step - loss: 0.3340 - acc: 0.8445\n",
      "Epoch 98/160\n",
      "4263/4263 [==============================] - 1s 317us/step - loss: 0.3346 - acc: 0.8430\n",
      "Epoch 99/160\n",
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.3329 - acc: 0.8430\n",
      "Epoch 100/160\n",
      "4263/4263 [==============================] - 2s 370us/step - loss: 0.3342 - acc: 0.8430\n",
      "Epoch 101/160\n",
      "4263/4263 [==============================] - 1s 348us/step - loss: 0.3293 - acc: 0.8457\n",
      "Epoch 102/160\n",
      "4263/4263 [==============================] - 2s 394us/step - loss: 0.3378 - acc: 0.8420\n",
      "Epoch 103/160\n",
      "4263/4263 [==============================] - 2s 352us/step - loss: 0.3337 - acc: 0.8435\n",
      "Epoch 104/160\n",
      "4263/4263 [==============================] - 2s 371us/step - loss: 0.3342 - acc: 0.8435\n",
      "Epoch 105/160\n",
      "4263/4263 [==============================] - 2s 358us/step - loss: 0.3325 - acc: 0.8449\n",
      "Epoch 106/160\n",
      "4263/4263 [==============================] - 1s 329us/step - loss: 0.3306 - acc: 0.8469\n",
      "Epoch 107/160\n",
      "4263/4263 [==============================] - 1s 326us/step - loss: 0.3347 - acc: 0.8435\n",
      "Epoch 108/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.3324 - acc: 0.8437\n",
      "Epoch 109/160\n",
      "4263/4263 [==============================] - 1s 326us/step - loss: 0.3295 - acc: 0.8465\n",
      "Epoch 110/160\n",
      "4263/4263 [==============================] - 1s 317us/step - loss: 0.3316 - acc: 0.8456\n",
      "Epoch 111/160\n",
      "4263/4263 [==============================] - 1s 331us/step - loss: 0.3294 - acc: 0.8458\n",
      "Epoch 112/160\n",
      "4263/4263 [==============================] - 1s 312us/step - loss: 0.3294 - acc: 0.8446\n",
      "Epoch 113/160\n",
      "4263/4263 [==============================] - 1s 315us/step - loss: 0.3348 - acc: 0.8448\n",
      "Epoch 114/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.3298 - acc: 0.8459\n",
      "Epoch 115/160\n",
      "4263/4263 [==============================] - 1s 313us/step - loss: 0.3303 - acc: 0.8462\n",
      "Epoch 116/160\n",
      "4263/4263 [==============================] - 1s 319us/step - loss: 0.3317 - acc: 0.8428\n",
      "Epoch 117/160\n",
      "4263/4263 [==============================] - 1s 314us/step - loss: 0.3286 - acc: 0.8482\n",
      "Epoch 118/160\n",
      "4263/4263 [==============================] - 1s 317us/step - loss: 0.3301 - acc: 0.8460\n",
      "Epoch 119/160\n",
      "4263/4263 [==============================] - 1s 322us/step - loss: 0.3276 - acc: 0.8473\n",
      "Epoch 120/160\n",
      "4263/4263 [==============================] - 2s 428us/step - loss: 0.3291 - acc: 0.8464\n",
      "Epoch 121/160\n",
      "4263/4263 [==============================] - 2s 427us/step - loss: 0.3274 - acc: 0.8469\n",
      "Epoch 122/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.3256 - acc: 0.8491\n",
      "Epoch 123/160\n",
      "4263/4263 [==============================] - 2s 400us/step - loss: 0.3290 - acc: 0.8469\n",
      "Epoch 124/160\n",
      "4263/4263 [==============================] - 1s 341us/step - loss: 0.3352 - acc: 0.8439\n",
      "Epoch 125/160\n",
      "4263/4263 [==============================] - 2s 435us/step - loss: 0.3262 - acc: 0.8481\n",
      "Epoch 126/160\n",
      "4263/4263 [==============================] - 2s 368us/step - loss: 0.3304 - acc: 0.8478\n",
      "Epoch 127/160\n",
      "4263/4263 [==============================] - 1s 330us/step - loss: 0.3264 - acc: 0.8472\n",
      "Epoch 128/160\n",
      "4263/4263 [==============================] - 1s 312us/step - loss: 0.3240 - acc: 0.8509\n",
      "Epoch 129/160\n",
      "4263/4263 [==============================] - 1s 314us/step - loss: 0.3355 - acc: 0.8420\n",
      "Epoch 130/160\n",
      "4263/4263 [==============================] - 1s 325us/step - loss: 0.3238 - acc: 0.8494\n",
      "Epoch 131/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.3261 - acc: 0.8485\n",
      "Epoch 132/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.3262 - acc: 0.8474\n",
      "Epoch 133/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.3281 - acc: 0.8459\n",
      "Epoch 134/160\n",
      "4263/4263 [==============================] - 1s 313us/step - loss: 0.3292 - acc: 0.8475\n",
      "Epoch 135/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.3256 - acc: 0.8487\n",
      "Epoch 136/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.3264 - acc: 0.8474\n",
      "Epoch 137/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.3268 - acc: 0.8473\n",
      "Epoch 138/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.3252 - acc: 0.8492\n",
      "Epoch 139/160\n",
      "4263/4263 [==============================] - 1s 316us/step - loss: 0.3250 - acc: 0.8489\n",
      "Epoch 140/160\n",
      "4263/4263 [==============================] - 1s 316us/step - loss: 0.3254 - acc: 0.8461\n",
      "Epoch 141/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.3245 - acc: 0.8499\n",
      "Epoch 142/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.3263 - acc: 0.8479\n",
      "Epoch 143/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.3235 - acc: 0.8492\n",
      "Epoch 144/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.3247 - acc: 0.8497\n",
      "Epoch 145/160\n",
      "4263/4263 [==============================] - 1s 325us/step - loss: 0.3238 - acc: 0.8488\n",
      "Epoch 146/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.3233 - acc: 0.8499\n",
      "Epoch 147/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.3284 - acc: 0.8456\n",
      "Epoch 148/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.3210 - acc: 0.8515\n",
      "Epoch 149/160\n",
      "4263/4263 [==============================] - 1s 313us/step - loss: 0.3234 - acc: 0.8491\n",
      "Epoch 150/160\n",
      "4263/4263 [==============================] - 1s 319us/step - loss: 0.3217 - acc: 0.8506\n",
      "Epoch 151/160\n",
      "4263/4263 [==============================] - 1s 322us/step - loss: 0.3195 - acc: 0.8506\n",
      "Epoch 152/160\n",
      "4263/4263 [==============================] - 1s 316us/step - loss: 0.3215 - acc: 0.8495\n",
      "Epoch 153/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.3213 - acc: 0.8480\n",
      "Epoch 154/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.3223 - acc: 0.8494\n",
      "Epoch 155/160\n",
      "4263/4263 [==============================] - 1s 322us/step - loss: 0.3188 - acc: 0.8513\n",
      "Epoch 156/160\n",
      "4263/4263 [==============================] - 1s 324us/step - loss: 0.3166 - acc: 0.8530\n",
      "Epoch 157/160\n",
      "4263/4263 [==============================] - 2s 375us/step - loss: 0.3224 - acc: 0.8481\n",
      "Epoch 158/160\n",
      "4263/4263 [==============================] - 2s 429us/step - loss: 0.3330 - acc: 0.8439\n",
      "Epoch 159/160\n",
      "4263/4263 [==============================] - 2s 372us/step - loss: 0.3231 - acc: 0.8488\n",
      "Epoch 160/160\n",
      "4263/4263 [==============================] - 1s 336us/step - loss: 0.3197 - acc: 0.8519\n",
      "1066/1066 [==============================] - 3s 3ms/step\n",
      "4263/4263 [==============================] - 1s 179us/step\n",
      "\n",
      "acc: 83.33%\n",
      "\n",
      "acc: 85.37%\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df['Class'].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d,y_d,test_size=0.2,random_state=42)\n",
    "\n",
    "y_train_2=pd.get_dummies(y_train)\n",
    "y_test_2=pd.get_dummies(y_test)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(700, activation='relu'))\n",
    "#model.add(Dense(1000, activation='relu'))\n",
    "#model.add(Dense(1200, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=160, callbacks=[early_stopping_monitor],batch_size=50)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "scores_2 = model.evaluate(X_train, y_train_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "predictions=model.predict(X_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_2[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/160\n",
      "4263/4263 [==============================] - 7s 2ms/step - loss: 0.4448 - acc: 0.7950\n",
      "Epoch 2/160\n",
      "4263/4263 [==============================] - 1s 312us/step - loss: 0.3966 - acc: 0.8108\n",
      "Epoch 3/160\n",
      "4263/4263 [==============================] - 1s 310us/step - loss: 0.3905 - acc: 0.8123\n",
      "Epoch 4/160\n",
      "4263/4263 [==============================] - 1s 323us/step - loss: 0.3876 - acc: 0.8135\n",
      "Epoch 5/160\n",
      "4263/4263 [==============================] - 1s 321us/step - loss: 0.3841 - acc: 0.8139\n",
      "Epoch 6/160\n",
      "4263/4263 [==============================] - 1s 337us/step - loss: 0.3809 - acc: 0.8175\n",
      "Epoch 7/160\n",
      "4263/4263 [==============================] - 1s 329us/step - loss: 0.3732 - acc: 0.8204\n",
      "Epoch 8/160\n",
      "4263/4263 [==============================] - 1s 331us/step - loss: 0.3725 - acc: 0.8259\n",
      "Epoch 9/160\n",
      "4263/4263 [==============================] - 1s 347us/step - loss: 0.3736 - acc: 0.8237\n",
      "Epoch 10/160\n",
      "4263/4263 [==============================] - 2s 353us/step - loss: 0.3705 - acc: 0.8215\n",
      "Epoch 11/160\n",
      "4263/4263 [==============================] - 2s 363us/step - loss: 0.3709 - acc: 0.8273\n",
      "Epoch 12/160\n",
      "4263/4263 [==============================] - 1s 341us/step - loss: 0.3698 - acc: 0.8271\n",
      "Epoch 13/160\n",
      "4263/4263 [==============================] - 2s 354us/step - loss: 0.3723 - acc: 0.8247\n",
      "Epoch 14/160\n",
      "4263/4263 [==============================] - 1s 306us/step - loss: 0.3667 - acc: 0.8277\n",
      "Epoch 15/160\n",
      "4263/4263 [==============================] - 1s 338us/step - loss: 0.3656 - acc: 0.8265\n",
      "Epoch 16/160\n",
      "4263/4263 [==============================] - 1s 309us/step - loss: 0.3634 - acc: 0.8315\n",
      "Epoch 17/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.3626 - acc: 0.8281\n",
      "Epoch 18/160\n",
      "4263/4263 [==============================] - 2s 357us/step - loss: 0.3619 - acc: 0.8303\n",
      "Epoch 19/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.3593 - acc: 0.8326\n",
      "Epoch 20/160\n",
      "4263/4263 [==============================] - 1s 320us/step - loss: 0.3584 - acc: 0.8329\n",
      "Epoch 21/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.3572 - acc: 0.8325\n",
      "Epoch 22/160\n",
      "4263/4263 [==============================] - 2s 365us/step - loss: 0.3558 - acc: 0.8349\n",
      "Epoch 23/160\n",
      "4263/4263 [==============================] - 1s 326us/step - loss: 0.3544 - acc: 0.8316\n",
      "Epoch 24/160\n",
      "4263/4263 [==============================] - 1s 315us/step - loss: 0.3574 - acc: 0.8290\n",
      "Epoch 25/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.3555 - acc: 0.8336\n",
      "Epoch 26/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.3550 - acc: 0.8309\n",
      "Epoch 27/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.3533 - acc: 0.8338\n",
      "Epoch 28/160\n",
      "4263/4263 [==============================] - 1s 313us/step - loss: 0.3535 - acc: 0.8356\n",
      "Epoch 29/160\n",
      "4263/4263 [==============================] - 1s 311us/step - loss: 0.3544 - acc: 0.8313\n",
      "Epoch 30/160\n",
      "4263/4263 [==============================] - 1s 313us/step - loss: 0.3495 - acc: 0.8336\n",
      "Epoch 31/160\n",
      "4263/4263 [==============================] - 1s 314us/step - loss: 0.3526 - acc: 0.8322\n",
      "Epoch 32/160\n",
      "4263/4263 [==============================] - 1s 317us/step - loss: 0.3563 - acc: 0.8303\n",
      "Epoch 33/160\n",
      "4263/4263 [==============================] - 1s 316us/step - loss: 0.3503 - acc: 0.8326\n",
      "Epoch 34/160\n",
      "4263/4263 [==============================] - 1s 315us/step - loss: 0.3493 - acc: 0.8332\n",
      "Epoch 35/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.3469 - acc: 0.8346\n",
      "Epoch 36/160\n",
      "4263/4263 [==============================] - 1s 318us/step - loss: 0.3530 - acc: 0.8325\n",
      "Epoch 37/160\n",
      "4263/4263 [==============================] - 1s 316us/step - loss: 0.3477 - acc: 0.8365\n",
      "Epoch 38/160\n",
      "4263/4263 [==============================] - 1s 343us/step - loss: 0.3492 - acc: 0.8332\n",
      "Epoch 39/160\n",
      "4263/4263 [==============================] - 2s 369us/step - loss: 0.3499 - acc: 0.8343\n",
      "Epoch 40/160\n",
      "4263/4263 [==============================] - 1s 341us/step - loss: 0.3497 - acc: 0.8353\n",
      "Epoch 41/160\n",
      "4263/4263 [==============================] - 2s 407us/step - loss: 0.3449 - acc: 0.8357\n",
      "Epoch 42/160\n",
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.3467 - acc: 0.8347\n",
      "Epoch 43/160\n",
      "4263/4263 [==============================] - 2s 480us/step - loss: 0.3449 - acc: 0.8354\n",
      "Epoch 44/160\n",
      "4263/4263 [==============================] - 2s 428us/step - loss: 0.3464 - acc: 0.8359\n",
      "Epoch 45/160\n",
      "4263/4263 [==============================] - 2s 457us/step - loss: 0.3449 - acc: 0.8373\n",
      "Epoch 46/160\n",
      "4263/4263 [==============================] - 2s 475us/step - loss: 0.3457 - acc: 0.8373\n",
      "Epoch 47/160\n",
      "4263/4263 [==============================] - 2s 545us/step - loss: 0.3479 - acc: 0.8360\n",
      "Epoch 48/160\n",
      "4263/4263 [==============================] - 2s 428us/step - loss: 0.3454 - acc: 0.8370\n",
      "Epoch 49/160\n",
      "4263/4263 [==============================] - 2s 412us/step - loss: 0.3454 - acc: 0.8366\n",
      "Epoch 50/160\n",
      "4263/4263 [==============================] - 2s 557us/step - loss: 0.3451 - acc: 0.8352\n",
      "Epoch 51/160\n",
      "4263/4263 [==============================] - 2s 500us/step - loss: 0.3441 - acc: 0.8343\n",
      "Epoch 52/160\n",
      "4263/4263 [==============================] - 2s 498us/step - loss: 0.3449 - acc: 0.8361\n",
      "Epoch 53/160\n",
      "4263/4263 [==============================] - 2s 416us/step - loss: 0.3420 - acc: 0.8376\n",
      "Epoch 54/160\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.3437 - acc: 0.8350\n",
      "Epoch 55/160\n",
      "4263/4263 [==============================] - 2s 416us/step - loss: 0.3437 - acc: 0.8401\n",
      "Epoch 56/160\n",
      "4263/4263 [==============================] - 2s 536us/step - loss: 0.3415 - acc: 0.8374\n",
      "Epoch 57/160\n",
      "4263/4263 [==============================] - 2s 465us/step - loss: 0.3429 - acc: 0.8371\n",
      "Epoch 58/160\n",
      "4263/4263 [==============================] - 2s 449us/step - loss: 0.3420 - acc: 0.8375\n",
      "Epoch 59/160\n",
      "4263/4263 [==============================] - 2s 560us/step - loss: 0.3430 - acc: 0.8383\n",
      "Epoch 60/160\n",
      "4263/4263 [==============================] - 2s 443us/step - loss: 0.3411 - acc: 0.8378\n",
      "Epoch 61/160\n",
      "4263/4263 [==============================] - 2s 482us/step - loss: 0.3452 - acc: 0.8352\n",
      "Epoch 62/160\n",
      "4263/4263 [==============================] - 2s 454us/step - loss: 0.3389 - acc: 0.8373\n",
      "Epoch 63/160\n",
      "4263/4263 [==============================] - 2s 492us/step - loss: 0.3393 - acc: 0.8405\n",
      "Epoch 64/160\n",
      "4263/4263 [==============================] - 2s 458us/step - loss: 0.3410 - acc: 0.8367\n",
      "Epoch 65/160\n",
      "4263/4263 [==============================] - ETA: 0s - loss: 0.3415 - acc: 0.837 - 2s 527us/step - loss: 0.3415 - acc: 0.8378\n",
      "Epoch 66/160\n",
      "4263/4263 [==============================] - 2s 507us/step - loss: 0.3407 - acc: 0.8404\n",
      "Epoch 67/160\n",
      "4263/4263 [==============================] - 2s 455us/step - loss: 0.3393 - acc: 0.8386\n",
      "Epoch 68/160\n",
      "4263/4263 [==============================] - 2s 453us/step - loss: 0.3383 - acc: 0.8421\n",
      "Epoch 69/160\n",
      "4263/4263 [==============================] - 2s 419us/step - loss: 0.3406 - acc: 0.8381\n",
      "Epoch 70/160\n",
      "4263/4263 [==============================] - 2s 405us/step - loss: 0.3391 - acc: 0.8404\n",
      "Epoch 71/160\n",
      "4263/4263 [==============================] - 2s 487us/step - loss: 0.3427 - acc: 0.8357\n",
      "Epoch 72/160\n",
      "4263/4263 [==============================] - 2s 418us/step - loss: 0.3388 - acc: 0.8385\n",
      "Epoch 73/160\n",
      "4263/4263 [==============================] - 2s 432us/step - loss: 0.3435 - acc: 0.8386\n",
      "Epoch 74/160\n",
      "4263/4263 [==============================] - 2s 397us/step - loss: 0.3386 - acc: 0.8405\n",
      "Epoch 75/160\n",
      "4263/4263 [==============================] - 2s 400us/step - loss: 0.3388 - acc: 0.8417\n",
      "Epoch 76/160\n",
      "4263/4263 [==============================] - 2s 461us/step - loss: 0.3412 - acc: 0.8401\n",
      "Epoch 77/160\n",
      "4263/4263 [==============================] - 2s 419us/step - loss: 0.3407 - acc: 0.8399\n",
      "Epoch 78/160\n",
      "4263/4263 [==============================] - 2s 429us/step - loss: 0.3394 - acc: 0.8395\n",
      "Epoch 79/160\n",
      "4263/4263 [==============================] - 2s 435us/step - loss: 0.3396 - acc: 0.8424\n",
      "Epoch 80/160\n",
      "4263/4263 [==============================] - 2s 376us/step - loss: 0.3372 - acc: 0.8399\n",
      "Epoch 81/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 2s 437us/step - loss: 0.3390 - acc: 0.8420\n",
      "Epoch 82/160\n",
      "4263/4263 [==============================] - 2s 416us/step - loss: 0.3402 - acc: 0.8383\n",
      "Epoch 83/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.3386 - acc: 0.8404\n",
      "Epoch 84/160\n",
      "4263/4263 [==============================] - 2s 389us/step - loss: 0.3391 - acc: 0.8392\n",
      "Epoch 85/160\n",
      "4263/4263 [==============================] - 2s 375us/step - loss: 0.3346 - acc: 0.8427\n",
      "Epoch 86/160\n",
      "4263/4263 [==============================] - 2s 380us/step - loss: 0.3371 - acc: 0.8417\n",
      "Epoch 87/160\n",
      "4263/4263 [==============================] - 2s 382us/step - loss: 0.3338 - acc: 0.8429\n",
      "Epoch 88/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.3380 - acc: 0.8432\n",
      "Epoch 89/160\n",
      "4263/4263 [==============================] - 2s 378us/step - loss: 0.3368 - acc: 0.8430\n",
      "Epoch 90/160\n",
      "4263/4263 [==============================] - 2s 386us/step - loss: 0.3357 - acc: 0.8436\n",
      "Epoch 91/160\n",
      "4263/4263 [==============================] - 2s 379us/step - loss: 0.3377 - acc: 0.8445\n",
      "Epoch 92/160\n",
      "4263/4263 [==============================] - 2s 365us/step - loss: 0.3355 - acc: 0.8431\n",
      "Epoch 93/160\n",
      "4263/4263 [==============================] - 2s 450us/step - loss: 0.3339 - acc: 0.8422\n",
      "Epoch 94/160\n",
      "4263/4263 [==============================] - 2s 388us/step - loss: 0.3334 - acc: 0.8432\n",
      "Epoch 95/160\n",
      "4263/4263 [==============================] - 2s 380us/step - loss: 0.3383 - acc: 0.8395\n",
      "Epoch 96/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.3351 - acc: 0.8445\n",
      "Epoch 97/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.3329 - acc: 0.8432\n",
      "Epoch 98/160\n",
      "4263/4263 [==============================] - 2s 379us/step - loss: 0.3313 - acc: 0.8449\n",
      "Epoch 99/160\n",
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.3348 - acc: 0.8425\n",
      "Epoch 100/160\n",
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.3312 - acc: 0.8455\n",
      "Epoch 101/160\n",
      "4263/4263 [==============================] - 2s 394us/step - loss: 0.3333 - acc: 0.8415\n",
      "Epoch 102/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.3368 - acc: 0.8449\n",
      "Epoch 103/160\n",
      "4263/4263 [==============================] - 2s 384us/step - loss: 0.3339 - acc: 0.8413\n",
      "Epoch 104/160\n",
      "4263/4263 [==============================] - 2s 408us/step - loss: 0.3299 - acc: 0.8448\n",
      "Epoch 105/160\n",
      "4263/4263 [==============================] - 2s 395us/step - loss: 0.3287 - acc: 0.8465\n",
      "Epoch 106/160\n",
      "4263/4263 [==============================] - 2s 396us/step - loss: 0.3290 - acc: 0.8473\n",
      "Epoch 107/160\n",
      "4263/4263 [==============================] - 2s 399us/step - loss: 0.3274 - acc: 0.8461\n",
      "Epoch 108/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.3291 - acc: 0.8478\n",
      "Epoch 109/160\n",
      "4263/4263 [==============================] - 2s 413us/step - loss: 0.3282 - acc: 0.8465\n",
      "Epoch 110/160\n",
      "4263/4263 [==============================] - 2s 406us/step - loss: 0.3304 - acc: 0.8445\n",
      "Epoch 111/160\n",
      "4263/4263 [==============================] - 3s 592us/step - loss: 0.3293 - acc: 0.8458\n",
      "Epoch 112/160\n",
      "4263/4263 [==============================] - 2s 556us/step - loss: 0.3309 - acc: 0.8437\n",
      "Epoch 113/160\n",
      "4263/4263 [==============================] - 2s 469us/step - loss: 0.3255 - acc: 0.8485\n",
      "Epoch 114/160\n",
      "4263/4263 [==============================] - 2s 424us/step - loss: 0.3250 - acc: 0.8482\n",
      "Epoch 115/160\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.3247 - acc: 0.8460\n",
      "Epoch 116/160\n",
      "4263/4263 [==============================] - 2s 449us/step - loss: 0.3273 - acc: 0.8473\n",
      "Epoch 117/160\n",
      "4263/4263 [==============================] - 2s 402us/step - loss: 0.3278 - acc: 0.8470\n",
      "Epoch 118/160\n",
      "4263/4263 [==============================] - 2s 396us/step - loss: 0.3257 - acc: 0.8475\n",
      "Epoch 119/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.3256 - acc: 0.8478\n",
      "Epoch 120/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.3314 - acc: 0.8469\n",
      "Epoch 121/160\n",
      "4263/4263 [==============================] - 2s 491us/step - loss: 0.3333 - acc: 0.8441\n",
      "Epoch 122/160\n",
      "4263/4263 [==============================] - 2s 470us/step - loss: 0.3255 - acc: 0.8466\n",
      "Epoch 123/160\n",
      "4263/4263 [==============================] - 2s 394us/step - loss: 0.3238 - acc: 0.8450\n",
      "Epoch 124/160\n",
      "4263/4263 [==============================] - 1s 311us/step - loss: 0.3239 - acc: 0.8500\n",
      "Epoch 125/160\n",
      "4263/4263 [==============================] - 2s 445us/step - loss: 0.3263 - acc: 0.8455\n",
      "Epoch 126/160\n",
      "4263/4263 [==============================] - 1s 342us/step - loss: 0.3249 - acc: 0.8472\n",
      "Epoch 127/160\n",
      "4263/4263 [==============================] - 1s 312us/step - loss: 0.3213 - acc: 0.8500\n",
      "Epoch 128/160\n",
      "4263/4263 [==============================] - 2s 379us/step - loss: 0.3291 - acc: 0.8449\n",
      "Epoch 129/160\n",
      "4263/4263 [==============================] - 2s 511us/step - loss: 0.3245 - acc: 0.8489\n",
      "Epoch 130/160\n",
      "4263/4263 [==============================] - 2s 453us/step - loss: 0.3218 - acc: 0.8506\n",
      "Epoch 131/160\n",
      "4263/4263 [==============================] - 2s 471us/step - loss: 0.3215 - acc: 0.8495\n",
      "Epoch 132/160\n",
      "4263/4263 [==============================] - 2s 562us/step - loss: 0.3191 - acc: 0.8513\n",
      "Epoch 133/160\n",
      "4263/4263 [==============================] - 2s 478us/step - loss: 0.3204 - acc: 0.8506\n",
      "Epoch 134/160\n",
      "4263/4263 [==============================] - 2s 470us/step - loss: 0.3243 - acc: 0.8485\n",
      "Epoch 135/160\n",
      "4263/4263 [==============================] - 2s 461us/step - loss: 0.3207 - acc: 0.8515\n",
      "Epoch 136/160\n",
      "4263/4263 [==============================] - 2s 379us/step - loss: 0.3201 - acc: 0.8485\n",
      "Epoch 137/160\n",
      "4263/4263 [==============================] - 2s 376us/step - loss: 0.3192 - acc: 0.8529\n",
      "Epoch 138/160\n",
      "4263/4263 [==============================] - 2s 507us/step - loss: 0.3254 - acc: 0.8469\n",
      "Epoch 139/160\n",
      "4263/4263 [==============================] - 2s 475us/step - loss: 0.3186 - acc: 0.8528\n",
      "Epoch 140/160\n",
      "4263/4263 [==============================] - 2s 524us/step - loss: 0.3180 - acc: 0.8499\n",
      "Epoch 141/160\n",
      "4263/4263 [==============================] - 2s 497us/step - loss: 0.3191 - acc: 0.8515 1s - loss\n",
      "Epoch 142/160\n",
      "4263/4263 [==============================] - 2s 430us/step - loss: 0.3166 - acc: 0.8503\n",
      "Epoch 143/160\n",
      "4263/4263 [==============================] - 2s 406us/step - loss: 0.3153 - acc: 0.8517\n",
      "Epoch 144/160\n",
      "4263/4263 [==============================] - 2s 396us/step - loss: 0.3150 - acc: 0.8529\n",
      "Epoch 145/160\n",
      "4263/4263 [==============================] - 2s 410us/step - loss: 0.3190 - acc: 0.8513\n",
      "Epoch 146/160\n",
      "4263/4263 [==============================] - 2s 407us/step - loss: 0.3155 - acc: 0.8526\n",
      "Epoch 147/160\n",
      "4263/4263 [==============================] - 2s 406us/step - loss: 0.3196 - acc: 0.8510\n",
      "Epoch 148/160\n",
      "4263/4263 [==============================] - 2s 410us/step - loss: 0.3170 - acc: 0.8535\n",
      "Epoch 149/160\n",
      "4263/4263 [==============================] - 2s 397us/step - loss: 0.3169 - acc: 0.8549\n",
      "Epoch 150/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.3175 - acc: 0.8513\n",
      "Epoch 151/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.3151 - acc: 0.8499\n",
      "Epoch 152/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.3160 - acc: 0.8502\n",
      "Epoch 153/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.3177 - acc: 0.8502\n",
      "Epoch 154/160\n",
      "4263/4263 [==============================] - 2s 413us/step - loss: 0.3114 - acc: 0.8529\n",
      "Epoch 155/160\n",
      "4263/4263 [==============================] - 2s 405us/step - loss: 0.3135 - acc: 0.8542\n",
      "Epoch 156/160\n",
      "4263/4263 [==============================] - 2s 402us/step - loss: 0.3167 - acc: 0.8521\n",
      "Epoch 157/160\n",
      "4263/4263 [==============================] - 2s 406us/step - loss: 0.3141 - acc: 0.8518\n",
      "Epoch 158/160\n",
      "4263/4263 [==============================] - 2s 402us/step - loss: 0.3171 - acc: 0.8526\n",
      "Epoch 159/160\n",
      "4263/4263 [==============================] - 2s 390us/step - loss: 0.3092 - acc: 0.8538\n",
      "Epoch 160/160\n",
      "4263/4263 [==============================] - 2s 395us/step - loss: 0.3112 - acc: 0.8548\n",
      "1066/1066 [==============================] - 3s 2ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 1s 203us/step\n",
      "\n",
      "acc: 84.10%\n",
      "\n",
      "acc: 86.11%\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df['Class'].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d,y_d,test_size=0.2,random_state=42)\n",
    "\n",
    "y_train_2=pd.get_dummies(y_train)\n",
    "y_test_2=pd.get_dummies(y_test)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(600, activation='relu'))\n",
    "#model.add(Dense(1000, activation='relu'))\n",
    "#model.add(Dense(1200, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=160, callbacks=[early_stopping_monitor],batch_size=50)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "scores_2 = model.evaluate(X_train, y_train_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "predictions=model.predict(X_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_2[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/170\n",
      "4263/4263 [==============================] - 10s 2ms/step - loss: 0.4385 - acc: 0.7981\n",
      "Epoch 2/170\n",
      "4263/4263 [==============================] - 2s 508us/step - loss: 0.3962 - acc: 0.8099\n",
      "Epoch 3/170\n",
      "4263/4263 [==============================] - 2s 501us/step - loss: 0.3882 - acc: 0.8134\n",
      "Epoch 4/170\n",
      "4263/4263 [==============================] - 2s 580us/step - loss: 0.3844 - acc: 0.8142\n",
      "Epoch 5/170\n",
      "4263/4263 [==============================] - 2s 529us/step - loss: 0.3771 - acc: 0.8162\n",
      "Epoch 6/170\n",
      "4263/4263 [==============================] - 2s 508us/step - loss: 0.3707 - acc: 0.8241\n",
      "Epoch 7/170\n",
      "4263/4263 [==============================] - 2s 434us/step - loss: 0.3708 - acc: 0.8241\n",
      "Epoch 8/170\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.3671 - acc: 0.8259\n",
      "Epoch 9/170\n",
      "4263/4263 [==============================] - 2s 421us/step - loss: 0.3679 - acc: 0.8246\n",
      "Epoch 10/170\n",
      "4263/4263 [==============================] - 2s 421us/step - loss: 0.3645 - acc: 0.8320\n",
      "Epoch 11/170\n",
      "4263/4263 [==============================] - 2s 416us/step - loss: 0.3651 - acc: 0.8280\n",
      "Epoch 12/170\n",
      "4263/4263 [==============================] - 2s 410us/step - loss: 0.3599 - acc: 0.8293\n",
      "Epoch 13/170\n",
      "4263/4263 [==============================] - 2s 412us/step - loss: 0.3595 - acc: 0.8261\n",
      "Epoch 14/170\n",
      "4263/4263 [==============================] - 2s 408us/step - loss: 0.3602 - acc: 0.8281\n",
      "Epoch 15/170\n",
      "4263/4263 [==============================] - 2s 406us/step - loss: 0.3575 - acc: 0.8296\n",
      "Epoch 16/170\n",
      "4263/4263 [==============================] - 2s 420us/step - loss: 0.3603 - acc: 0.8279\n",
      "Epoch 17/170\n",
      "4263/4263 [==============================] - 2s 430us/step - loss: 0.3580 - acc: 0.8332\n",
      "Epoch 18/170\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.3559 - acc: 0.8329\n",
      "Epoch 19/170\n",
      "4263/4263 [==============================] - 2s 424us/step - loss: 0.3530 - acc: 0.8336\n",
      "Epoch 20/170\n",
      "4263/4263 [==============================] - 2s 426us/step - loss: 0.3544 - acc: 0.8346\n",
      "Epoch 21/170\n",
      "4263/4263 [==============================] - 2s 410us/step - loss: 0.3562 - acc: 0.8315\n",
      "Epoch 22/170\n",
      "4263/4263 [==============================] - 2s 453us/step - loss: 0.3517 - acc: 0.8336 1s - loss\n",
      "Epoch 23/170\n",
      "4263/4263 [==============================] - 2s 423us/step - loss: 0.3554 - acc: 0.8280\n",
      "Epoch 24/170\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.3499 - acc: 0.8324\n",
      "Epoch 25/170\n",
      "4263/4263 [==============================] - 2s 424us/step - loss: 0.3520 - acc: 0.8330\n",
      "Epoch 26/170\n",
      "4263/4263 [==============================] - 2s 410us/step - loss: 0.3521 - acc: 0.8327\n",
      "Epoch 27/170\n",
      "4263/4263 [==============================] - 2s 412us/step - loss: 0.3525 - acc: 0.8309\n",
      "Epoch 28/170\n",
      "4263/4263 [==============================] - 2s 408us/step - loss: 0.3540 - acc: 0.8300\n",
      "Epoch 29/170\n",
      "4263/4263 [==============================] - 2s 407us/step - loss: 0.3546 - acc: 0.8295\n",
      "Epoch 30/170\n",
      "4263/4263 [==============================] - 2s 434us/step - loss: 0.3553 - acc: 0.8320\n",
      "Epoch 31/170\n",
      "4263/4263 [==============================] - 2s 465us/step - loss: 0.3537 - acc: 0.8309\n",
      "Epoch 32/170\n",
      "4263/4263 [==============================] - 2s 412us/step - loss: 0.3480 - acc: 0.8373\n",
      "Epoch 33/170\n",
      "4263/4263 [==============================] - 2s 415us/step - loss: 0.3488 - acc: 0.8353\n",
      "Epoch 34/170\n",
      "4263/4263 [==============================] - 2s 416us/step - loss: 0.3471 - acc: 0.8364\n",
      "Epoch 35/170\n",
      "4263/4263 [==============================] - 2s 414us/step - loss: 0.3462 - acc: 0.8343\n",
      "Epoch 36/170\n",
      "4263/4263 [==============================] - 2s 413us/step - loss: 0.3505 - acc: 0.8313\n",
      "Epoch 37/170\n",
      "4263/4263 [==============================] - 2s 423us/step - loss: 0.3468 - acc: 0.8367\n",
      "Epoch 38/170\n",
      "4263/4263 [==============================] - 2s 421us/step - loss: 0.3489 - acc: 0.8316\n",
      "Epoch 39/170\n",
      "4263/4263 [==============================] - 2s 416us/step - loss: 0.3496 - acc: 0.8340\n",
      "Epoch 40/170\n",
      "4263/4263 [==============================] - 2s 421us/step - loss: 0.3467 - acc: 0.8381\n",
      "Epoch 41/170\n",
      "4263/4263 [==============================] - 2s 417us/step - loss: 0.3449 - acc: 0.8344\n",
      "Epoch 42/170\n",
      "4263/4263 [==============================] - 2s 412us/step - loss: 0.3472 - acc: 0.8342\n",
      "Epoch 43/170\n",
      "4263/4263 [==============================] - 2s 412us/step - loss: 0.3444 - acc: 0.8359\n",
      "Epoch 44/170\n",
      "4263/4263 [==============================] - 2s 424us/step - loss: 0.3492 - acc: 0.8357\n",
      "Epoch 45/170\n",
      "4263/4263 [==============================] - 2s 405us/step - loss: 0.3477 - acc: 0.8346\n",
      "Epoch 46/170\n",
      "4263/4263 [==============================] - 2s 413us/step - loss: 0.3471 - acc: 0.8359\n",
      "Epoch 47/170\n",
      "4263/4263 [==============================] - 2s 432us/step - loss: 0.3427 - acc: 0.8370\n",
      "Epoch 48/170\n",
      "4263/4263 [==============================] - 2s 414us/step - loss: 0.3437 - acc: 0.8401\n",
      "Epoch 49/170\n",
      "4263/4263 [==============================] - 2s 419us/step - loss: 0.3461 - acc: 0.8375\n",
      "Epoch 50/170\n",
      "4263/4263 [==============================] - 2s 419us/step - loss: 0.3483 - acc: 0.8343\n",
      "Epoch 51/170\n",
      "4263/4263 [==============================] - 2s 417us/step - loss: 0.3410 - acc: 0.8400\n",
      "Epoch 52/170\n",
      "4263/4263 [==============================] - 2s 414us/step - loss: 0.3437 - acc: 0.8380\n",
      "Epoch 53/170\n",
      "4263/4263 [==============================] - 2s 408us/step - loss: 0.3444 - acc: 0.8395\n",
      "Epoch 54/170\n",
      "4263/4263 [==============================] - 2s 426us/step - loss: 0.3447 - acc: 0.8375\n",
      "Epoch 55/170\n",
      "4263/4263 [==============================] - 2s 410us/step - loss: 0.3409 - acc: 0.8397\n",
      "Epoch 56/170\n",
      "4263/4263 [==============================] - 2s 412us/step - loss: 0.3405 - acc: 0.8366\n",
      "Epoch 57/170\n",
      "4263/4263 [==============================] - 2s 412us/step - loss: 0.3409 - acc: 0.8388\n",
      "Epoch 58/170\n",
      "4263/4263 [==============================] - 2s 421us/step - loss: 0.3398 - acc: 0.8391\n",
      "Epoch 59/170\n",
      "4263/4263 [==============================] - 2s 412us/step - loss: 0.3437 - acc: 0.8383\n",
      "Epoch 60/170\n",
      "4263/4263 [==============================] - 2s 418us/step - loss: 0.3402 - acc: 0.8397\n",
      "Epoch 61/170\n",
      "4263/4263 [==============================] - 2s 419us/step - loss: 0.3388 - acc: 0.8413\n",
      "Epoch 62/170\n",
      "4263/4263 [==============================] - 2s 415us/step - loss: 0.3395 - acc: 0.8392\n",
      "Epoch 63/170\n",
      "4263/4263 [==============================] - 2s 419us/step - loss: 0.3397 - acc: 0.8404\n",
      "Epoch 64/170\n",
      "4263/4263 [==============================] - 2s 423us/step - loss: 0.3410 - acc: 0.8384\n",
      "Epoch 65/170\n",
      "4263/4263 [==============================] - 2s 412us/step - loss: 0.3396 - acc: 0.8398\n",
      "Epoch 66/170\n",
      "4263/4263 [==============================] - 2s 417us/step - loss: 0.3419 - acc: 0.8404\n",
      "Epoch 67/170\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.3380 - acc: 0.8420\n",
      "Epoch 68/170\n",
      "4263/4263 [==============================] - 2s 414us/step - loss: 0.3391 - acc: 0.8385\n",
      "Epoch 69/170\n",
      "4263/4263 [==============================] - 2s 411us/step - loss: 0.3399 - acc: 0.8404\n",
      "Epoch 70/170\n",
      "4263/4263 [==============================] - 2s 416us/step - loss: 0.3362 - acc: 0.8435\n",
      "Epoch 71/170\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.3419 - acc: 0.8374\n",
      "Epoch 72/170\n",
      "4263/4263 [==============================] - 2s 407us/step - loss: 0.3398 - acc: 0.8386\n",
      "Epoch 73/170\n",
      "4263/4263 [==============================] - 2s 416us/step - loss: 0.3397 - acc: 0.8381\n",
      "Epoch 74/170\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.3370 - acc: 0.8397\n",
      "Epoch 75/170\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.3379 - acc: 0.8427\n",
      "Epoch 76/170\n",
      "4263/4263 [==============================] - 2s 503us/step - loss: 0.3383 - acc: 0.8422\n",
      "Epoch 77/170\n",
      "4263/4263 [==============================] - 2s 433us/step - loss: 0.3373 - acc: 0.8434\n",
      "Epoch 78/170\n",
      "4263/4263 [==============================] - 2s 436us/step - loss: 0.3358 - acc: 0.8405\n",
      "Epoch 79/170\n",
      "4263/4263 [==============================] - 2s 443us/step - loss: 0.3359 - acc: 0.8439\n",
      "Epoch 80/170\n",
      "4263/4263 [==============================] - 2s 444us/step - loss: 0.3348 - acc: 0.8447\n",
      "Epoch 81/170\n",
      "4263/4263 [==============================] - 2s 446us/step - loss: 0.3392 - acc: 0.8396\n",
      "Epoch 82/170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 2s 437us/step - loss: 0.3373 - acc: 0.8444\n",
      "Epoch 83/170\n",
      "4263/4263 [==============================] - 2s 441us/step - loss: 0.3332 - acc: 0.8460\n",
      "Epoch 84/170\n",
      "4263/4263 [==============================] - 2s 448us/step - loss: 0.3327 - acc: 0.8451\n",
      "Epoch 85/170\n",
      "4263/4263 [==============================] - 2s 434us/step - loss: 0.3332 - acc: 0.8439\n",
      "Epoch 86/170\n",
      "4263/4263 [==============================] - 2s 436us/step - loss: 0.3335 - acc: 0.8437\n",
      "Epoch 87/170\n",
      "4263/4263 [==============================] - 2s 444us/step - loss: 0.3331 - acc: 0.8429\n",
      "Epoch 88/170\n",
      "4263/4263 [==============================] - 2s 433us/step - loss: 0.3402 - acc: 0.8396\n",
      "Epoch 89/170\n",
      "4263/4263 [==============================] - 2s 435us/step - loss: 0.3343 - acc: 0.8426\n",
      "Epoch 90/170\n",
      "4263/4263 [==============================] - 2s 448us/step - loss: 0.3341 - acc: 0.8421\n",
      "Epoch 91/170\n",
      "4263/4263 [==============================] - 2s 436us/step - loss: 0.3325 - acc: 0.8443\n",
      "Epoch 92/170\n",
      "4263/4263 [==============================] - 2s 431us/step - loss: 0.3356 - acc: 0.8408\n",
      "Epoch 93/170\n",
      "4263/4263 [==============================] - 2s 435us/step - loss: 0.3297 - acc: 0.8442\n",
      "Epoch 94/170\n",
      "4263/4263 [==============================] - 2s 432us/step - loss: 0.3354 - acc: 0.8417\n",
      "Epoch 95/170\n",
      "4263/4263 [==============================] - 2s 449us/step - loss: 0.3379 - acc: 0.8420\n",
      "Epoch 96/170\n",
      "4263/4263 [==============================] - 2s 443us/step - loss: 0.3309 - acc: 0.8458\n",
      "Epoch 97/170\n",
      "4263/4263 [==============================] - 2s 446us/step - loss: 0.3311 - acc: 0.8462\n",
      "Epoch 98/170\n",
      "4263/4263 [==============================] - 2s 440us/step - loss: 0.3313 - acc: 0.8466\n",
      "Epoch 99/170\n",
      "4263/4263 [==============================] - 2s 447us/step - loss: 0.3312 - acc: 0.8437\n",
      "Epoch 100/170\n",
      "4263/4263 [==============================] - 2s 439us/step - loss: 0.3280 - acc: 0.8471\n",
      "Epoch 101/170\n",
      "4263/4263 [==============================] - 2s 443us/step - loss: 0.3309 - acc: 0.8455\n",
      "Epoch 102/170\n",
      "4263/4263 [==============================] - 2s 519us/step - loss: 0.3341 - acc: 0.8429\n",
      "Epoch 103/170\n",
      "4263/4263 [==============================] - 2s 552us/step - loss: 0.3285 - acc: 0.8429\n",
      "Epoch 104/170\n",
      "4263/4263 [==============================] - 2s 583us/step - loss: 0.3304 - acc: 0.8444\n",
      "Epoch 105/170\n",
      "4263/4263 [==============================] - 2s 505us/step - loss: 0.3301 - acc: 0.8423\n",
      "Epoch 106/170\n",
      "4263/4263 [==============================] - 2s 431us/step - loss: 0.3267 - acc: 0.8480\n",
      "Epoch 107/170\n",
      "4263/4263 [==============================] - 2s 436us/step - loss: 0.3325 - acc: 0.8461\n",
      "Epoch 108/170\n",
      "4263/4263 [==============================] - 2s 421us/step - loss: 0.3288 - acc: 0.8452\n",
      "Epoch 109/170\n",
      "4263/4263 [==============================] - 2s 426us/step - loss: 0.3330 - acc: 0.8434\n",
      "Epoch 110/170\n",
      "4263/4263 [==============================] - 2s 423us/step - loss: 0.3294 - acc: 0.8453\n",
      "Epoch 111/170\n",
      "4263/4263 [==============================] - 2s 419us/step - loss: 0.3318 - acc: 0.8446\n",
      "Epoch 112/170\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.3275 - acc: 0.8475\n",
      "Epoch 113/170\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.3296 - acc: 0.8468\n",
      "Epoch 114/170\n",
      "4263/4263 [==============================] - 2s 415us/step - loss: 0.3279 - acc: 0.8466\n",
      "Epoch 115/170\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.3251 - acc: 0.8478\n",
      "Epoch 116/170\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.3241 - acc: 0.8482\n",
      "Epoch 117/170\n",
      "4263/4263 [==============================] - 2s 428us/step - loss: 0.3234 - acc: 0.8486\n",
      "Epoch 118/170\n",
      "4263/4263 [==============================] - 2s 430us/step - loss: 0.3325 - acc: 0.8459\n",
      "Epoch 119/170\n",
      "4263/4263 [==============================] - 2s 441us/step - loss: 0.3270 - acc: 0.8486\n",
      "Epoch 120/170\n",
      "4263/4263 [==============================] - 2s 424us/step - loss: 0.3253 - acc: 0.8467\n",
      "Epoch 121/170\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.3244 - acc: 0.8476\n",
      "Epoch 122/170\n",
      "4263/4263 [==============================] - 2s 430us/step - loss: 0.3253 - acc: 0.8480\n",
      "Epoch 123/170\n",
      "4263/4263 [==============================] - 2s 433us/step - loss: 0.3236 - acc: 0.8492\n",
      "Epoch 124/170\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.3269 - acc: 0.8459\n",
      "Epoch 125/170\n",
      "4263/4263 [==============================] - 2s 435us/step - loss: 0.3235 - acc: 0.8460\n",
      "Epoch 126/170\n",
      "4263/4263 [==============================] - 2s 433us/step - loss: 0.3241 - acc: 0.8458\n",
      "Epoch 127/170\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.3242 - acc: 0.8495\n",
      "Epoch 128/170\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.3246 - acc: 0.8464\n",
      "Epoch 129/170\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.3228 - acc: 0.8491\n",
      "Epoch 130/170\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.3248 - acc: 0.8478\n",
      "Epoch 131/170\n",
      "4263/4263 [==============================] - 2s 415us/step - loss: 0.3267 - acc: 0.8457\n",
      "Epoch 132/170\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.3260 - acc: 0.8466\n",
      "Epoch 133/170\n",
      "4263/4263 [==============================] - 2s 421us/step - loss: 0.3190 - acc: 0.8516\n",
      "Epoch 134/170\n",
      "4263/4263 [==============================] - 2s 426us/step - loss: 0.3253 - acc: 0.8467\n",
      "Epoch 135/170\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.3181 - acc: 0.8520\n",
      "Epoch 136/170\n",
      "4263/4263 [==============================] - 2s 420us/step - loss: 0.3246 - acc: 0.8495\n",
      "Epoch 137/170\n",
      "4263/4263 [==============================] - 2s 527us/step - loss: 0.3189 - acc: 0.8516\n",
      "Epoch 138/170\n",
      "4263/4263 [==============================] - 2s 509us/step - loss: 0.3230 - acc: 0.8476\n",
      "Epoch 139/170\n",
      "4263/4263 [==============================] - 2s 443us/step - loss: 0.3184 - acc: 0.8506\n",
      "Epoch 140/170\n",
      "4263/4263 [==============================] - 2s 423us/step - loss: 0.3205 - acc: 0.8503\n",
      "Epoch 141/170\n",
      "4263/4263 [==============================] - 2s 426us/step - loss: 0.3219 - acc: 0.8461\n",
      "Epoch 142/170\n",
      "4263/4263 [==============================] - 2s 423us/step - loss: 0.3197 - acc: 0.8492\n",
      "Epoch 143/170\n",
      "4263/4263 [==============================] - 2s 451us/step - loss: 0.3188 - acc: 0.8511\n",
      "Epoch 144/170\n",
      "4263/4263 [==============================] - 2s 436us/step - loss: 0.3223 - acc: 0.8495\n",
      "Epoch 145/170\n",
      "4263/4263 [==============================] - 2s 429us/step - loss: 0.3164 - acc: 0.8534\n",
      "Epoch 146/170\n",
      "4263/4263 [==============================] - 2s 423us/step - loss: 0.3192 - acc: 0.8510\n",
      "Epoch 147/170\n",
      "4263/4263 [==============================] - 2s 418us/step - loss: 0.3174 - acc: 0.8512\n",
      "Epoch 148/170\n",
      "4263/4263 [==============================] - 2s 432us/step - loss: 0.3195 - acc: 0.8482\n",
      "Epoch 149/170\n",
      "4263/4263 [==============================] - 2s 423us/step - loss: 0.3138 - acc: 0.8533\n",
      "Epoch 150/170\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.3195 - acc: 0.8505\n",
      "Epoch 151/170\n",
      "4263/4263 [==============================] - 2s 421us/step - loss: 0.3169 - acc: 0.8516\n",
      "Epoch 152/170\n",
      "4263/4263 [==============================] - 2s 427us/step - loss: 0.3158 - acc: 0.8505\n",
      "Epoch 153/170\n",
      "4263/4263 [==============================] - 2s 424us/step - loss: 0.3133 - acc: 0.8528\n",
      "Epoch 154/170\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.3192 - acc: 0.8502\n",
      "Epoch 155/170\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.3155 - acc: 0.8533\n",
      "Epoch 156/170\n",
      "4263/4263 [==============================] - 2s 435us/step - loss: 0.3189 - acc: 0.8491\n",
      "Epoch 157/170\n",
      "4263/4263 [==============================] - 2s 442us/step - loss: 0.3191 - acc: 0.8504\n",
      "Epoch 158/170\n",
      "4263/4263 [==============================] - 2s 442us/step - loss: 0.3160 - acc: 0.8505\n",
      "Epoch 159/170\n",
      "4263/4263 [==============================] - 2s 419us/step - loss: 0.3126 - acc: 0.8555\n",
      "Epoch 160/170\n",
      "4263/4263 [==============================] - 2s 421us/step - loss: 0.3144 - acc: 0.8540\n",
      "Epoch 161/170\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.3138 - acc: 0.8517\n",
      "Epoch 162/170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 2s 420us/step - loss: 0.3132 - acc: 0.8524\n",
      "Epoch 163/170\n",
      "4263/4263 [==============================] - 2s 419us/step - loss: 0.3132 - acc: 0.8551\n",
      "Epoch 164/170\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.3110 - acc: 0.8550\n",
      "Epoch 165/170\n",
      "4263/4263 [==============================] - 2s 436us/step - loss: 0.3140 - acc: 0.8523\n",
      "Epoch 166/170\n",
      "4263/4263 [==============================] - 2s 421us/step - loss: 0.3132 - acc: 0.8532\n",
      "Epoch 167/170\n",
      "4263/4263 [==============================] - 2s 431us/step - loss: 0.3097 - acc: 0.8570\n",
      "Epoch 168/170\n",
      "4263/4263 [==============================] - 2s 437us/step - loss: 0.3119 - acc: 0.8516\n",
      "Epoch 169/170\n",
      "4263/4263 [==============================] - 2s 423us/step - loss: 0.3057 - acc: 0.8547\n",
      "Epoch 170/170\n",
      "4263/4263 [==============================] - 2s 426us/step - loss: 0.3110 - acc: 0.8523\n",
      "1066/1066 [==============================] - 3s 2ms/step\n",
      "4263/4263 [==============================] - 1s 231us/step\n",
      "\n",
      "acc: 83.51%\n",
      "\n",
      "acc: 85.37%\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df['Class'].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d,y_d,test_size=0.2,random_state=42)\n",
    "\n",
    "y_train_2=pd.get_dummies(y_train)\n",
    "y_test_2=pd.get_dummies(y_test)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(700, activation='relu'))\n",
    "#model.add(Dense(1000, activation='relu'))\n",
    "#model.add(Dense(1200, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=170, callbacks=[early_stopping_monitor],batch_size=50)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "scores_2 = model.evaluate(X_train, y_train_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "predictions=model.predict(X_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_2[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/160\n",
      "4263/4263 [==============================] - 9s 2ms/step - loss: 0.4585 - acc: 0.7900\n",
      "Epoch 2/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.4025 - acc: 0.8110\n",
      "Epoch 3/160\n",
      "4263/4263 [==============================] - 2s 396us/step - loss: 0.3920 - acc: 0.8154\n",
      "Epoch 4/160\n",
      "4263/4263 [==============================] - 2s 390us/step - loss: 0.3891 - acc: 0.8132\n",
      "Epoch 5/160\n",
      "4263/4263 [==============================] - 2s 396us/step - loss: 0.3847 - acc: 0.8158\n",
      "Epoch 6/160\n",
      "4263/4263 [==============================] - 2s 400us/step - loss: 0.3778 - acc: 0.8181\n",
      "Epoch 7/160\n",
      "4263/4263 [==============================] - 2s 503us/step - loss: 0.3748 - acc: 0.8195\n",
      "Epoch 8/160\n",
      "4263/4263 [==============================] - 2s 478us/step - loss: 0.3711 - acc: 0.8217\n",
      "Epoch 9/160\n",
      "4263/4263 [==============================] - 2s 397us/step - loss: 0.3713 - acc: 0.8194\n",
      "Epoch 10/160\n",
      "4263/4263 [==============================] - 2s 399us/step - loss: 0.3716 - acc: 0.8210\n",
      "Epoch 11/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.3692 - acc: 0.8248\n",
      "Epoch 12/160\n",
      "4263/4263 [==============================] - 2s 388us/step - loss: 0.3695 - acc: 0.8231\n",
      "Epoch 13/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.3662 - acc: 0.8264\n",
      "Epoch 14/160\n",
      "4263/4263 [==============================] - 2s 394us/step - loss: 0.3661 - acc: 0.8268\n",
      "Epoch 15/160\n",
      "4263/4263 [==============================] - 2s 398us/step - loss: 0.3628 - acc: 0.8300\n",
      "Epoch 16/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.3621 - acc: 0.8280\n",
      "Epoch 17/160\n",
      "4263/4263 [==============================] - 2s 491us/step - loss: 0.3586 - acc: 0.8306\n",
      "Epoch 18/160\n",
      "4263/4263 [==============================] - 2s 470us/step - loss: 0.3583 - acc: 0.8303\n",
      "Epoch 19/160\n",
      "4263/4263 [==============================] - 2s 395us/step - loss: 0.3569 - acc: 0.8308\n",
      "Epoch 20/160\n",
      "4263/4263 [==============================] - 2s 396us/step - loss: 0.3573 - acc: 0.8305\n",
      "Epoch 21/160\n",
      "4263/4263 [==============================] - 2s 390us/step - loss: 0.3547 - acc: 0.8329\n",
      "Epoch 22/160\n",
      "4263/4263 [==============================] - 2s 388us/step - loss: 0.3540 - acc: 0.8332\n",
      "Epoch 23/160\n",
      "4263/4263 [==============================] - 2s 386us/step - loss: 0.3556 - acc: 0.8341\n",
      "Epoch 24/160\n",
      "4263/4263 [==============================] - 2s 379us/step - loss: 0.3549 - acc: 0.8319\n",
      "Epoch 25/160\n",
      "4263/4263 [==============================] - 2s 382us/step - loss: 0.3524 - acc: 0.8339\n",
      "Epoch 26/160\n",
      "4263/4263 [==============================] - 2s 382us/step - loss: 0.3523 - acc: 0.8329\n",
      "Epoch 27/160\n",
      "4263/4263 [==============================] - 2s 378us/step - loss: 0.3540 - acc: 0.8317\n",
      "Epoch 28/160\n",
      "4263/4263 [==============================] - 2s 374us/step - loss: 0.3531 - acc: 0.8342\n",
      "Epoch 29/160\n",
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.3530 - acc: 0.8312\n",
      "Epoch 30/160\n",
      "4263/4263 [==============================] - 2s 384us/step - loss: 0.3518 - acc: 0.8324\n",
      "Epoch 31/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.3479 - acc: 0.8362\n",
      "Epoch 32/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.3527 - acc: 0.8327\n",
      "Epoch 33/160\n",
      "4263/4263 [==============================] - 2s 386us/step - loss: 0.3480 - acc: 0.8347\n",
      "Epoch 34/160\n",
      "4263/4263 [==============================] - 2s 388us/step - loss: 0.3474 - acc: 0.8391\n",
      "Epoch 35/160\n",
      "4263/4263 [==============================] - 2s 403us/step - loss: 0.3500 - acc: 0.8357\n",
      "Epoch 36/160\n",
      "4263/4263 [==============================] - 2s 402us/step - loss: 0.3475 - acc: 0.8332\n",
      "Epoch 37/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.3509 - acc: 0.8326\n",
      "Epoch 38/160\n",
      "4263/4263 [==============================] - 2s 398us/step - loss: 0.3477 - acc: 0.8360\n",
      "Epoch 39/160\n",
      "4263/4263 [==============================] - 2s 379us/step - loss: 0.3472 - acc: 0.8360\n",
      "Epoch 40/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.3477 - acc: 0.8334\n",
      "Epoch 41/160\n",
      "4263/4263 [==============================] - 2s 405us/step - loss: 0.3450 - acc: 0.8361\n",
      "Epoch 42/160\n",
      "4263/4263 [==============================] - 2s 382us/step - loss: 0.3464 - acc: 0.8353\n",
      "Epoch 43/160\n",
      "4263/4263 [==============================] - 2s 388us/step - loss: 0.3463 - acc: 0.8359\n",
      "Epoch 44/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.3443 - acc: 0.8376\n",
      "Epoch 45/160\n",
      "4263/4263 [==============================] - 2s 396us/step - loss: 0.3458 - acc: 0.8350\n",
      "Epoch 46/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.3438 - acc: 0.8402\n",
      "Epoch 47/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.3439 - acc: 0.8363\n",
      "Epoch 48/160\n",
      "4263/4263 [==============================] - 2s 454us/step - loss: 0.3447 - acc: 0.8377\n",
      "Epoch 49/160\n",
      "4263/4263 [==============================] - 2s 500us/step - loss: 0.3478 - acc: 0.8344\n",
      "Epoch 50/160\n",
      "4263/4263 [==============================] - 2s 396us/step - loss: 0.3417 - acc: 0.8364\n",
      "Epoch 51/160\n",
      "4263/4263 [==============================] - 2s 394us/step - loss: 0.3448 - acc: 0.8360\n",
      "Epoch 52/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.3417 - acc: 0.8384\n",
      "Epoch 53/160\n",
      "4263/4263 [==============================] - 2s 394us/step - loss: 0.3437 - acc: 0.8369\n",
      "Epoch 54/160\n",
      "4263/4263 [==============================] - 2s 395us/step - loss: 0.3457 - acc: 0.8359\n",
      "Epoch 55/160\n",
      "4263/4263 [==============================] - 2s 390us/step - loss: 0.3429 - acc: 0.8353\n",
      "Epoch 56/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.3418 - acc: 0.8395\n",
      "Epoch 57/160\n",
      "4263/4263 [==============================] - 2s 446us/step - loss: 0.3420 - acc: 0.8398\n",
      "Epoch 58/160\n",
      "4263/4263 [==============================] - 2s 517us/step - loss: 0.3490 - acc: 0.8359\n",
      "Epoch 59/160\n",
      "4263/4263 [==============================] - 2s 399us/step - loss: 0.3443 - acc: 0.8388\n",
      "Epoch 60/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.3395 - acc: 0.8411\n",
      "Epoch 61/160\n",
      "4263/4263 [==============================] - 2s 389us/step - loss: 0.3433 - acc: 0.8397\n",
      "Epoch 62/160\n",
      "4263/4263 [==============================] - 2s 389us/step - loss: 0.3434 - acc: 0.8382\n",
      "Epoch 63/160\n",
      "4263/4263 [==============================] - 2s 532us/step - loss: 0.3418 - acc: 0.8377\n",
      "Epoch 64/160\n",
      "4263/4263 [==============================] - 2s 418us/step - loss: 0.3386 - acc: 0.8403\n",
      "Epoch 65/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.3388 - acc: 0.8410\n",
      "Epoch 66/160\n",
      "4263/4263 [==============================] - 2s 389us/step - loss: 0.3396 - acc: 0.8398\n",
      "Epoch 67/160\n",
      "4263/4263 [==============================] - 2s 388us/step - loss: 0.3371 - acc: 0.8426\n",
      "Epoch 68/160\n",
      "4263/4263 [==============================] - 2s 389us/step - loss: 0.3409 - acc: 0.8396\n",
      "Epoch 69/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.3408 - acc: 0.8394\n",
      "Epoch 70/160\n",
      "4263/4263 [==============================] - 2s 385us/step - loss: 0.3372 - acc: 0.8419\n",
      "Epoch 71/160\n",
      "4263/4263 [==============================] - 2s 397us/step - loss: 0.3398 - acc: 0.8388\n",
      "Epoch 72/160\n",
      "4263/4263 [==============================] - 2s 390us/step - loss: 0.3379 - acc: 0.8429\n",
      "Epoch 73/160\n",
      "4263/4263 [==============================] - 2s 395us/step - loss: 0.3369 - acc: 0.8420\n",
      "Epoch 74/160\n",
      "4263/4263 [==============================] - 2s 477us/step - loss: 0.3365 - acc: 0.8428\n",
      "Epoch 75/160\n",
      "4263/4263 [==============================] - 2s 467us/step - loss: 0.3396 - acc: 0.8400\n",
      "Epoch 76/160\n",
      "4263/4263 [==============================] - 2s 482us/step - loss: 0.3380 - acc: 0.8410\n",
      "Epoch 77/160\n",
      "4263/4263 [==============================] - 2s 471us/step - loss: 0.3356 - acc: 0.8429\n",
      "Epoch 78/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.3401 - acc: 0.8403\n",
      "Epoch 79/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.3373 - acc: 0.8423\n",
      "Epoch 80/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.3368 - acc: 0.8419\n",
      "Epoch 81/160\n",
      "4263/4263 [==============================] - 2s 385us/step - loss: 0.3338 - acc: 0.8442\n",
      "Epoch 82/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.3369 - acc: 0.8413\n",
      "Epoch 83/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.3374 - acc: 0.8424\n",
      "Epoch 84/160\n",
      "4263/4263 [==============================] - 2s 386us/step - loss: 0.3387 - acc: 0.8430\n",
      "Epoch 85/160\n",
      "4263/4263 [==============================] - 2s 388us/step - loss: 0.3359 - acc: 0.8450\n",
      "Epoch 86/160\n",
      "4263/4263 [==============================] - 2s 381us/step - loss: 0.3344 - acc: 0.8416\n",
      "Epoch 87/160\n",
      "4263/4263 [==============================] - 2s 390us/step - loss: 0.3343 - acc: 0.8419\n",
      "Epoch 88/160\n",
      "4263/4263 [==============================] - 2s 502us/step - loss: 0.3327 - acc: 0.8470\n",
      "Epoch 89/160\n",
      "4263/4263 [==============================] - 2s 456us/step - loss: 0.3342 - acc: 0.8437\n",
      "Epoch 90/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.3365 - acc: 0.8434\n",
      "Epoch 91/160\n",
      "4263/4263 [==============================] - 2s 388us/step - loss: 0.3338 - acc: 0.8423\n",
      "Epoch 92/160\n",
      "4263/4263 [==============================] - 2s 394us/step - loss: 0.3365 - acc: 0.8423\n",
      "Epoch 93/160\n",
      "4263/4263 [==============================] - 2s 385us/step - loss: 0.3353 - acc: 0.8444\n",
      "Epoch 94/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.3314 - acc: 0.8448\n",
      "Epoch 95/160\n",
      "4263/4263 [==============================] - 2s 390us/step - loss: 0.3331 - acc: 0.8448\n",
      "Epoch 96/160\n",
      "4263/4263 [==============================] - 2s 414us/step - loss: 0.3320 - acc: 0.8448\n",
      "Epoch 97/160\n",
      "4263/4263 [==============================] - 2s 430us/step - loss: 0.3305 - acc: 0.8468\n",
      "Epoch 98/160\n",
      "4263/4263 [==============================] - 2s 424us/step - loss: 0.3331 - acc: 0.8437\n",
      "Epoch 99/160\n",
      "4263/4263 [==============================] - 2s 417us/step - loss: 0.3327 - acc: 0.8447\n",
      "Epoch 100/160\n",
      "4263/4263 [==============================] - 2s 438us/step - loss: 0.3308 - acc: 0.8435\n",
      "Epoch 101/160\n",
      "4263/4263 [==============================] - 2s 424us/step - loss: 0.3297 - acc: 0.8452\n",
      "Epoch 102/160\n",
      "4263/4263 [==============================] - 2s 415us/step - loss: 0.3303 - acc: 0.8455\n",
      "Epoch 103/160\n",
      "4263/4263 [==============================] - 2s 420us/step - loss: 0.3313 - acc: 0.8441\n",
      "Epoch 104/160\n",
      "4263/4263 [==============================] - 2s 519us/step - loss: 0.3298 - acc: 0.8454\n",
      "Epoch 105/160\n",
      "4263/4263 [==============================] - 2s 508us/step - loss: 0.3343 - acc: 0.8424\n",
      "Epoch 106/160\n",
      "4263/4263 [==============================] - 2s 470us/step - loss: 0.3314 - acc: 0.8453\n",
      "Epoch 107/160\n",
      "4263/4263 [==============================] - 2s 405us/step - loss: 0.3304 - acc: 0.8454\n",
      "Epoch 108/160\n",
      "4263/4263 [==============================] - 2s 400us/step - loss: 0.3287 - acc: 0.8464\n",
      "Epoch 109/160\n",
      "4263/4263 [==============================] - 2s 400us/step - loss: 0.3281 - acc: 0.8464\n",
      "Epoch 110/160\n",
      "4263/4263 [==============================] - 2s 403us/step - loss: 0.3281 - acc: 0.8489\n",
      "Epoch 111/160\n",
      "4263/4263 [==============================] - 2s 390us/step - loss: 0.3301 - acc: 0.8475\n",
      "Epoch 112/160\n",
      "4263/4263 [==============================] - 2s 394us/step - loss: 0.3273 - acc: 0.8478\n",
      "Epoch 113/160\n",
      "4263/4263 [==============================] - 2s 407us/step - loss: 0.3280 - acc: 0.8472\n",
      "Epoch 114/160\n",
      "4263/4263 [==============================] - 2s 397us/step - loss: 0.3277 - acc: 0.8470\n",
      "Epoch 115/160\n",
      "4263/4263 [==============================] - 2s 398us/step - loss: 0.3299 - acc: 0.8449\n",
      "Epoch 116/160\n",
      "4263/4263 [==============================] - 2s 399us/step - loss: 0.3266 - acc: 0.8481\n",
      "Epoch 117/160\n",
      "4263/4263 [==============================] - 2s 408us/step - loss: 0.3261 - acc: 0.8474\n",
      "Epoch 118/160\n",
      "4263/4263 [==============================] - 2s 396us/step - loss: 0.3276 - acc: 0.8496\n",
      "Epoch 119/160\n",
      "4263/4263 [==============================] - 2s 396us/step - loss: 0.3270 - acc: 0.8494\n",
      "Epoch 120/160\n",
      "4263/4263 [==============================] - 2s 404us/step - loss: 0.3282 - acc: 0.8456\n",
      "Epoch 121/160\n",
      "4263/4263 [==============================] - 2s 399us/step - loss: 0.3311 - acc: 0.8439\n",
      "Epoch 122/160\n",
      "4263/4263 [==============================] - 2s 401us/step - loss: 0.3279 - acc: 0.8453\n",
      "Epoch 123/160\n",
      "4263/4263 [==============================] - 2s 396us/step - loss: 0.3261 - acc: 0.8484\n",
      "Epoch 124/160\n",
      "4263/4263 [==============================] - 2s 401us/step - loss: 0.3279 - acc: 0.8464\n",
      "Epoch 125/160\n",
      "4263/4263 [==============================] - 2s 399us/step - loss: 0.3313 - acc: 0.8465\n",
      "Epoch 126/160\n",
      "4263/4263 [==============================] - 2s 392us/step - loss: 0.3262 - acc: 0.8471\n",
      "Epoch 127/160\n",
      "4263/4263 [==============================] - 2s 402us/step - loss: 0.3241 - acc: 0.8493\n",
      "Epoch 128/160\n",
      "4263/4263 [==============================] - 2s 400us/step - loss: 0.3224 - acc: 0.8497\n",
      "Epoch 129/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.3237 - acc: 0.8489\n",
      "Epoch 130/160\n",
      "4263/4263 [==============================] - 2s 396us/step - loss: 0.3267 - acc: 0.8489\n",
      "Epoch 131/160\n",
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.3244 - acc: 0.8490\n",
      "Epoch 132/160\n",
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.3217 - acc: 0.8500\n",
      "Epoch 133/160\n",
      "4263/4263 [==============================] - 2s 381us/step - loss: 0.3249 - acc: 0.8489\n",
      "Epoch 134/160\n",
      "4263/4263 [==============================] - 2s 377us/step - loss: 0.3244 - acc: 0.8493\n",
      "Epoch 135/160\n",
      "4263/4263 [==============================] - 2s 397us/step - loss: 0.3230 - acc: 0.8506\n",
      "Epoch 136/160\n",
      "4263/4263 [==============================] - 2s 386us/step - loss: 0.3246 - acc: 0.8492\n",
      "Epoch 137/160\n",
      "4263/4263 [==============================] - 2s 382us/step - loss: 0.3219 - acc: 0.8527\n",
      "Epoch 138/160\n",
      "4263/4263 [==============================] - 2s 390us/step - loss: 0.3202 - acc: 0.8526\n",
      "Epoch 139/160\n",
      "4263/4263 [==============================] - 2s 384us/step - loss: 0.3213 - acc: 0.8508\n",
      "Epoch 140/160\n",
      "4263/4263 [==============================] - 2s 386us/step - loss: 0.3248 - acc: 0.8491\n",
      "Epoch 141/160\n",
      "4263/4263 [==============================] - 2s 388us/step - loss: 0.3201 - acc: 0.8509\n",
      "Epoch 142/160\n",
      "4263/4263 [==============================] - 2s 390us/step - loss: 0.3174 - acc: 0.8538\n",
      "Epoch 143/160\n",
      "4263/4263 [==============================] - 2s 385us/step - loss: 0.3257 - acc: 0.8485\n",
      "Epoch 144/160\n",
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.3195 - acc: 0.8521\n",
      "Epoch 145/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.3214 - acc: 0.8508\n",
      "Epoch 146/160\n",
      "4263/4263 [==============================] - 2s 385us/step - loss: 0.3183 - acc: 0.8533\n",
      "Epoch 147/160\n",
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.3186 - acc: 0.8502\n",
      "Epoch 148/160\n",
      "4263/4263 [==============================] - 2s 386us/step - loss: 0.3189 - acc: 0.8509\n",
      "Epoch 149/160\n",
      "4263/4263 [==============================] - 2s 387us/step - loss: 0.3190 - acc: 0.8508\n",
      "Epoch 150/160\n",
      "4263/4263 [==============================] - 2s 383us/step - loss: 0.3206 - acc: 0.8508\n",
      "Epoch 151/160\n",
      "4263/4263 [==============================] - 2s 379us/step - loss: 0.3179 - acc: 0.8526\n",
      "Epoch 152/160\n",
      "4263/4263 [==============================] - 2s 376us/step - loss: 0.3188 - acc: 0.8512\n",
      "Epoch 153/160\n",
      "4263/4263 [==============================] - 2s 389us/step - loss: 0.3191 - acc: 0.8516\n",
      "Epoch 154/160\n",
      "4263/4263 [==============================] - 2s 385us/step - loss: 0.3210 - acc: 0.8504\n",
      "Epoch 155/160\n",
      "4263/4263 [==============================] - 2s 381us/step - loss: 0.3169 - acc: 0.8523\n",
      "Epoch 156/160\n",
      "4263/4263 [==============================] - 2s 391us/step - loss: 0.3207 - acc: 0.8507\n",
      "Epoch 157/160\n",
      "4263/4263 [==============================] - 2s 401us/step - loss: 0.3195 - acc: 0.8512\n",
      "Epoch 158/160\n",
      "4263/4263 [==============================] - 2s 396us/step - loss: 0.3196 - acc: 0.8500\n",
      "Epoch 159/160\n",
      "4263/4263 [==============================] - 2s 398us/step - loss: 0.3157 - acc: 0.8539\n",
      "Epoch 160/160\n",
      "4263/4263 [==============================] - 2s 401us/step - loss: 0.3128 - acc: 0.8527\n",
      "1066/1066 [==============================] - 3s 3ms/step\n",
      "4263/4263 [==============================] - 1s 215us/step\n",
      "\n",
      "acc: 83.51%\n",
      "\n",
      "acc: 85.77%\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df['Class'].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d,y_d,test_size=0.2,random_state=42)\n",
    "\n",
    "y_train_2=pd.get_dummies(y_train)\n",
    "y_test_2=pd.get_dummies(y_test)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(600, activation='relu'))\n",
    "#model.add(Dense(1000, activation='relu'))\n",
    "#model.add(Dense(1200, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=160, callbacks=[early_stopping_monitor],batch_size=50)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "scores_2 = model.evaluate(X_train, y_train_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "predictions=model.predict(X_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_2[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1066\n",
      "1066\n"
     ]
    }
   ],
   "source": [
    "print(len(y_test_2))\n",
    "print(len(rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0_Fast  0_Normal  0_Slow  0_Very Fast\n",
      "1323       0         1       0            0\n",
      "1839       0         1       0            0\n",
      "798        0         1       0            0\n",
      "3855       0         1       0            0\n",
      "4552       0         1       0            0\n",
      "856        0         1       0            0\n",
      "2333       0         0       0            1\n",
      "2499       0         1       0            0\n",
      "5010       0         1       0            0\n",
      "4379       0         1       0            0\n",
      "4733       0         1       0            0\n",
      "655        0         1       0            0\n",
      "2101       0         0       0            1\n",
      "893        0         1       0            0\n",
      "373        0         1       0            0\n",
      "297        0         1       0            0\n",
      "3817       0         0       1            0\n",
      "1808       0         1       0            0\n",
      "4392       0         1       0            0\n",
      "3585       0         0       1            0\n",
      "240        0         1       0            0\n",
      "4650       0         1       0            0\n",
      "2951       0         1       0            0\n",
      "2860       1         0       0            0\n",
      "465        0         1       0            0\n",
      "3685       1         0       0            0\n",
      "4316       0         1       0            0\n",
      "84         0         1       0            0\n",
      "1498       0         1       0            0\n",
      "803        0         1       0            0\n",
      "...      ...       ...     ...          ...\n",
      "1700       1         0       0            0\n",
      "745        0         1       0            0\n",
      "5074       0         0       0            1\n",
      "1815       0         1       0            0\n",
      "1611       0         1       0            0\n",
      "195        0         1       0            0\n",
      "4898       0         1       0            0\n",
      "3039       0         1       0            0\n",
      "802        0         1       0            0\n",
      "3437       0         0       1            0\n",
      "1717       1         0       0            0\n",
      "1103       0         1       0            0\n",
      "3463       0         1       0            0\n",
      "3358       0         1       0            0\n",
      "3326       1         0       0            0\n",
      "1747       0         1       0            0\n",
      "1740       1         0       0            0\n",
      "644        1         0       0            0\n",
      "4145       1         0       0            0\n",
      "3916       0         0       1            0\n",
      "120        0         1       0            0\n",
      "3196       0         1       0            0\n",
      "1881       0         1       0            0\n",
      "3999       0         1       0            0\n",
      "4293       0         1       0            0\n",
      "2841       1         0       0            0\n",
      "5207       0         0       1            0\n",
      "1965       0         0       1            0\n",
      "4537       0         0       0            1\n",
      "134        0         1       0            0\n",
      "\n",
      "[1066 rows x 4 columns]\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(y_test_2)\n",
    "for x in predictions:\n",
    "    #print(round(x[0])+\" \"+round(x[1])+\" \"+round(x[2])+round(x[3]))\n",
    "    print(round(x[0]))\n",
    "    print(round(x[1]))\n",
    "   # print(round(x[0]).type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n",
      "k\n"
     ]
    }
   ],
   "source": [
    "y_test_l1=y_test_2['0_Fast'].tolist()\n",
    "y_test_l2=y_test_2['0_Normal'].tolist()\n",
    "y_test_l3=y_test_2['0_Slow'].tolist()\n",
    "y_test_l4=y_test_2['0_Very Fast'].tolist()\n",
    "for x in predictions:\n",
    "    #print(round(x[0])+\" \"+round(x[1])+\" \"+round(x[2])+round(x[3]))\n",
    "    k_1=round(x[0])\n",
    "    k_1_i=int(k_1)\n",
    "    k_2=round(x[0])\n",
    "    k_1_i=int(k_1)\n",
    "    k_1=round(x[0])\n",
    "    k_1_i=int(k_1)\n",
    "    k_1=round(x[0])\n",
    "    k_1_i=int(k_1)\n",
    "   # print(\"k\")\n",
    "    #print(round(x[1]))\n",
    "   # print(round(x[0]).type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Slow', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Fast', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Normal', 'Slow', 'Fast', 'Fast', 'Very Fast', 'Very Fast', 'Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Slow', 'Very Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Very Fast', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Normal', 'Normal', 'Slow', 'Slow', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Slow', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Fast', 'Normal', 'Slow', 'Very Fast', 'Normal', 'Fast', 'Slow', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Slow', 'Normal', 'Fast', 'Normal', 'Slow', 'Normal', 'Normal', 'Very Fast', 'Very Fast', 'Fast', 'Normal', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Very Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Very Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Very Fast', 'Normal', 'Normal', 'Fast', 'Fast', 'Fast', 'Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Very Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Slow', 'Normal', 'Fast', 'Fast', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Fast', 'Fast', 'Fast', 'Normal', 'Very Fast', 'Slow', 'Normal', 'Slow', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Slow', 'Very Fast', 'Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Very Fast', 'Slow', 'Fast', 'Slow', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Fast', 'Very Fast', 'Normal', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Fast', 'Very Fast', 'Normal', 'Fast', 'Very Fast', 'Fast', 'Normal', 'Slow', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Slow', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Slow', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Fast', 'Fast', 'Fast', 'Fast', 'Normal', 'Fast', 'Fast', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Very Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Fast', 'Fast', 'Normal', 'Slow', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Fast', 'Fast', 'Fast', 'Fast', 'Very Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Slow', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Fast', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Fast', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Fast', 'Slow', 'Fast', 'Fast', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Fast', 'Very Fast', 'Slow', 'Slow', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Fast', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Very Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Slow', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Slow', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Very Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Slow', 'Normal', 'Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Very Fast', 'Slow', 'Slow', 'Normal', 'Slow', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Fast', 'Normal', 'Very Fast', 'Normal', 'Fast', 'Fast', 'Fast', 'Normal', 'Slow', 'Normal', 'Very Fast', 'Slow', 'Normal', 'Slow', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Very Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Very Fast', 'Very Fast', 'Fast', 'Very Fast', 'Normal', 'Slow', 'Very Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Very Fast', 'Normal', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Fast', 'Fast', 'Fast', 'Fast', 'Fast', 'Normal', 'Normal', 'Fast', 'Fast', 'Fast', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Slow', 'Normal', 'Fast', 'Slow', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Very Fast', 'Slow', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Fast', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Slow', 'Very Fast', 'Normal', 'Slow', 'Normal', 'Normal', 'Slow', 'Normal', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Slow', 'Normal', 'Very Fast', 'Slow', 'Normal', 'Very Fast', 'Normal', 'Fast', 'Slow', 'Normal', 'Fast', 'Very Fast', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Slow', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Slow', 'Fast', 'Normal', 'Fast', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Very Fast', 'Very Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Very Fast', 'Fast', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Fast', 'Fast', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Slow', 'Very Fast', 'Normal']\n"
     ]
    }
   ],
   "source": [
    "y_test_l=[]\n",
    "\n",
    "l=len(y_test_2)\n",
    "i=0\n",
    "while i<l:\n",
    "    if y_test_2.iloc[i]['0_Fast']==1:\n",
    "        y_test_l.append('Fast')\n",
    "    if y_test_2.iloc[i]['0_Normal']==1:\n",
    "        y_test_l.append('Normal')\n",
    "    if y_test_2.iloc[i]['0_Slow']==1:\n",
    "        y_test_l.append('Slow')\n",
    "    if y_test_2.iloc[i]['0_Very Fast']==1:\n",
    "        y_test_l.append('Very Fast')\n",
    "    i=i+1\n",
    "    #print(i)\n",
    "\n",
    "print(y_test_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 1 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 1 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "[10, 21, 23, 38, 42, 75, 79, 81, 84, 107, 117, 126, 135, 137, 138, 153, 170, 178, 181, 186, 195, 220, 222, 237, 242, 246, 247, 255, 271, 276, 278, 288, 292, 311, 320, 332, 334, 337, 339, 364, 366, 382, 393, 395, 396, 399, 402, 407, 416, 434, 443, 454, 457, 461, 470, 472, 484, 486, 488, 503, 505, 506, 527, 540, 545, 548, 550, 564, 571, 588, 589, 594, 600, 611, 622, 626, 629, 653, 660, 670, 672, 685, 688, 700, 706, 711, 716, 725, 732, 742, 747, 751, 752, 754, 756, 762, 764, 766, 769, 773, 786, 803, 816, 822, 833, 856, 866, 870, 877, 884, 900, 901, 902, 909, 910, 911, 919, 924, 928, 931, 947, 965, 968, 970, 974, 977, 983, 984, 1001, 1013, 1025, 1030, 1038]\n",
      "133\n",
      "933\n"
     ]
    }
   ],
   "source": [
    "prediction_l=[]\n",
    "count=0\n",
    "all_zero=[]\n",
    "#print(len(predictions))\n",
    "for x in predictions:\n",
    "    k_1=round(x[0])\n",
    "    k_1_i=int(k_1)\n",
    "    k_1_s=str(k_1_i)\n",
    "    k_2=round(x[1])\n",
    "    k_2_i=int(k_2)\n",
    "    k_2_s=str(k_2_i)\n",
    "    k_3=round(x[2])\n",
    "    k_3_i=int(k_3)\n",
    "    k_3_s=str(k_3_i)\n",
    "    k_4=round(x[3])\n",
    "    k_4_i=int(k_4)\n",
    "    k_4_s=str(k_4_i)\n",
    "    if k_1_i==0 and k_2_i==0 and k_3_i==0 and k_4_i==0:\n",
    "        all_zero.append(count)\n",
    "    print(k_1_s+' '+k_2_s+' '+k_3_s+' '+k_4_s)\n",
    "    if k_1_i==1:\n",
    "        prediction_l.append('Fast')\n",
    "    if k_2_i==1:\n",
    "        prediction_l.append('Normal')\n",
    "    if k_3_i==1:\n",
    "        prediction_l.append('Slow')\n",
    "    if k_4_i==1:\n",
    "        prediction_l.append('Very Fast')\n",
    "    count=count+1\n",
    "    \n",
    "print(all_zero)\n",
    "print(len(all_zero))\n",
    "#print(prediction_l)\n",
    "print(len(prediction_l))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1066\n",
      "933\n"
     ]
    }
   ],
   "source": [
    "print(len(y_test_l))\n",
    "print(len(prediction_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'0_Fast'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4380\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4381\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlibindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4382\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_box\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_at\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/util.pxd\u001b[0m in \u001b[0;36mpandas._libs.util.get_value_at\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/util.pxd\u001b[0m in \u001b[0;36mpandas._libs.util.validate_indexer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-5a9644d27b2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_zero\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0my_test_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'0_Fast'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0my_test_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fast'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_test_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'0_Normal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4387\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4388\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4389\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4390\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4391\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4374\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 4375\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   4376\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4377\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '0_Fast'"
     ]
    }
   ],
   "source": [
    "y_test_l=[]\n",
    "\n",
    "l=len(y_test_2)\n",
    "i=0\n",
    "while i<l:\n",
    "    if i not in all_zero:\n",
    "        if y_test_2.iloc[i]['0_Fast']==1:\n",
    "            y_test_l.append('Fast')\n",
    "        if y_test_2.iloc[i]['0_Normal']==1:\n",
    "            y_test_l.append('Normal')\n",
    "        if y_test_2.iloc[i]['0_Slow']==1:\n",
    "            y_test_l.append('Slow')\n",
    "        if y_test_2.iloc[i]['0_Very Fast']==1:\n",
    "            y_test_l.append('Very Fast')\n",
    "    i=i+1\n",
    "    #print(i)\n",
    "print(len(y_test_l))\n",
    "print(y_test_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/160\n",
      "4263/4263 [==============================] - 11s 2ms/step - loss: 1.0366 - acc: 0.5639\n",
      "Epoch 2/160\n",
      "4263/4263 [==============================] - 2s 438us/step - loss: 0.9068 - acc: 0.5935\n",
      "Epoch 3/160\n",
      "4263/4263 [==============================] - 2s 433us/step - loss: 0.8916 - acc: 0.6115\n",
      "Epoch 4/160\n",
      "4263/4263 [==============================] - 2s 426us/step - loss: 0.8726 - acc: 0.6280\n",
      "Epoch 5/160\n",
      "4263/4263 [==============================] - 2s 428us/step - loss: 0.8461 - acc: 0.6350\n",
      "Epoch 6/160\n",
      "4263/4263 [==============================] - 2s 433us/step - loss: 0.8481 - acc: 0.6261\n",
      "Epoch 7/160\n",
      "4263/4263 [==============================] - 2s 468us/step - loss: 0.8397 - acc: 0.6399\n",
      "Epoch 8/160\n",
      "4263/4263 [==============================] - 2s 434us/step - loss: 0.8375 - acc: 0.6376\n",
      "Epoch 9/160\n",
      "4263/4263 [==============================] - 2s 448us/step - loss: 0.8387 - acc: 0.6355\n",
      "Epoch 10/160\n",
      "4263/4263 [==============================] - 2s 432us/step - loss: 0.8242 - acc: 0.6395\n",
      "Epoch 11/160\n",
      "4263/4263 [==============================] - 2s 433us/step - loss: 0.8310 - acc: 0.6350\n",
      "Epoch 12/160\n",
      "4263/4263 [==============================] - 2s 415us/step - loss: 0.8261 - acc: 0.6395\n",
      "Epoch 13/160\n",
      "4263/4263 [==============================] - 2s 518us/step - loss: 0.8214 - acc: 0.6427\n",
      "Epoch 14/160\n",
      "4263/4263 [==============================] - 2s 547us/step - loss: 0.8103 - acc: 0.6477\n",
      "Epoch 15/160\n",
      "4263/4263 [==============================] - 2s 438us/step - loss: 0.8060 - acc: 0.6449\n",
      "Epoch 16/160\n",
      "4263/4263 [==============================] - 2s 423us/step - loss: 0.8046 - acc: 0.6568\n",
      "Epoch 17/160\n",
      "4263/4263 [==============================] - 2s 440us/step - loss: 0.8075 - acc: 0.6535\n",
      "Epoch 18/160\n",
      "4263/4263 [==============================] - 2s 441us/step - loss: 0.7991 - acc: 0.6554\n",
      "Epoch 19/160\n",
      "4263/4263 [==============================] - 2s 431us/step - loss: 0.8108 - acc: 0.6491\n",
      "Epoch 20/160\n",
      "4263/4263 [==============================] - 2s 486us/step - loss: 0.7952 - acc: 0.6587\n",
      "Epoch 21/160\n",
      "4263/4263 [==============================] - 2s 413us/step - loss: 0.7925 - acc: 0.6617\n",
      "Epoch 22/160\n",
      "4263/4263 [==============================] - 2s 441us/step - loss: 0.7861 - acc: 0.6622\n",
      "Epoch 23/160\n",
      "4263/4263 [==============================] - 2s 421us/step - loss: 0.7908 - acc: 0.6580\n",
      "Epoch 24/160\n",
      "4263/4263 [==============================] - 2s 435us/step - loss: 0.7848 - acc: 0.6552\n",
      "Epoch 25/160\n",
      "4263/4263 [==============================] - 2s 427us/step - loss: 0.7824 - acc: 0.6613\n",
      "Epoch 26/160\n",
      "4263/4263 [==============================] - 2s 432us/step - loss: 0.7842 - acc: 0.6575\n",
      "Epoch 27/160\n",
      "4263/4263 [==============================] - 2s 479us/step - loss: 0.7832 - acc: 0.6552\n",
      "Epoch 28/160\n",
      "4263/4263 [==============================] - 2s 424us/step - loss: 0.7806 - acc: 0.6669\n",
      "Epoch 29/160\n",
      "4263/4263 [==============================] - 2s 439us/step - loss: 0.7760 - acc: 0.6676\n",
      "Epoch 30/160\n",
      "4263/4263 [==============================] - 2s 446us/step - loss: 0.7827 - acc: 0.6563\n",
      "Epoch 31/160\n",
      "4263/4263 [==============================] - 2s 470us/step - loss: 0.7825 - acc: 0.6587\n",
      "Epoch 32/160\n",
      "4263/4263 [==============================] - 2s 445us/step - loss: 0.7691 - acc: 0.6601\n",
      "Epoch 33/160\n",
      "4263/4263 [==============================] - 2s 431us/step - loss: 0.7799 - acc: 0.6639\n",
      "Epoch 34/160\n",
      "4263/4263 [==============================] - 2s 435us/step - loss: 0.7776 - acc: 0.6650\n",
      "Epoch 35/160\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.7664 - acc: 0.6671\n",
      "Epoch 36/160\n",
      "4263/4263 [==============================] - 2s 516us/step - loss: 0.7675 - acc: 0.6631\n",
      "Epoch 37/160\n",
      "4263/4263 [==============================] - 2s 430us/step - loss: 0.7737 - acc: 0.6624\n",
      "Epoch 38/160\n",
      "4263/4263 [==============================] - 2s 432us/step - loss: 0.7850 - acc: 0.6610\n",
      "Epoch 39/160\n",
      "4263/4263 [==============================] - 2s 427us/step - loss: 0.7665 - acc: 0.6685\n",
      "Epoch 40/160\n",
      "4263/4263 [==============================] - 2s 427us/step - loss: 0.7716 - acc: 0.6594\n",
      "Epoch 41/160\n",
      "4263/4263 [==============================] - 2s 435us/step - loss: 0.7704 - acc: 0.6643\n",
      "Epoch 42/160\n",
      "4263/4263 [==============================] - 2s 435us/step - loss: 0.7618 - acc: 0.6653\n",
      "Epoch 43/160\n",
      "4263/4263 [==============================] - 2s 433us/step - loss: 0.7694 - acc: 0.6594\n",
      "Epoch 44/160\n",
      "4263/4263 [==============================] - 2s 436us/step - loss: 0.7605 - acc: 0.6737\n",
      "Epoch 45/160\n",
      "4263/4263 [==============================] - 2s 493us/step - loss: 0.7677 - acc: 0.6596\n",
      "Epoch 46/160\n",
      "4263/4263 [==============================] - 3s 624us/step - loss: 0.7629 - acc: 0.6749\n",
      "Epoch 47/160\n",
      "4263/4263 [==============================] - 2s 454us/step - loss: 0.7681 - acc: 0.6655\n",
      "Epoch 48/160\n",
      "4263/4263 [==============================] - 2s 440us/step - loss: 0.7545 - acc: 0.6735\n",
      "Epoch 49/160\n",
      "4263/4263 [==============================] - 2s 432us/step - loss: 0.7594 - acc: 0.6657\n",
      "Epoch 50/160\n",
      "4263/4263 [==============================] - 2s 427us/step - loss: 0.7552 - acc: 0.6608\n",
      "Epoch 51/160\n",
      "4263/4263 [==============================] - 2s 457us/step - loss: 0.7509 - acc: 0.6695\n",
      "Epoch 52/160\n",
      "4263/4263 [==============================] - 2s 435us/step - loss: 0.7592 - acc: 0.6700\n",
      "Epoch 53/160\n",
      "4263/4263 [==============================] - 2s 428us/step - loss: 0.7557 - acc: 0.6690\n",
      "Epoch 54/160\n",
      "4263/4263 [==============================] - 2s 427us/step - loss: 0.7571 - acc: 0.6753\n",
      "Epoch 55/160\n",
      "4263/4263 [==============================] - 2s 431us/step - loss: 0.7517 - acc: 0.6735\n",
      "Epoch 56/160\n",
      "4263/4263 [==============================] - 2s 431us/step - loss: 0.7606 - acc: 0.6700\n",
      "Epoch 57/160\n",
      "4263/4263 [==============================] - 2s 456us/step - loss: 0.7510 - acc: 0.6732\n",
      "Epoch 58/160\n",
      "4263/4263 [==============================] - 2s 423us/step - loss: 0.7563 - acc: 0.6685\n",
      "Epoch 59/160\n",
      "4263/4263 [==============================] - 2s 433us/step - loss: 0.7490 - acc: 0.6690\n",
      "Epoch 60/160\n",
      "4263/4263 [==============================] - 2s 426us/step - loss: 0.7485 - acc: 0.6657\n",
      "Epoch 61/160\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.7463 - acc: 0.6725\n",
      "Epoch 62/160\n",
      "4263/4263 [==============================] - 2s 446us/step - loss: 0.7530 - acc: 0.6758\n",
      "Epoch 63/160\n",
      "4263/4263 [==============================] - 2s 427us/step - loss: 0.7527 - acc: 0.6756\n",
      "Epoch 64/160\n",
      "4263/4263 [==============================] - 2s 442us/step - loss: 0.7524 - acc: 0.6697\n",
      "Epoch 65/160\n",
      "4263/4263 [==============================] - 2s 435us/step - loss: 0.7560 - acc: 0.6702\n",
      "Epoch 66/160\n",
      "4263/4263 [==============================] - 2s 440us/step - loss: 0.7394 - acc: 0.6779\n",
      "Epoch 67/160\n",
      "4263/4263 [==============================] - 2s 433us/step - loss: 0.7515 - acc: 0.6692\n",
      "Epoch 68/160\n",
      "4263/4263 [==============================] - 2s 441us/step - loss: 0.7541 - acc: 0.6714\n",
      "Epoch 69/160\n",
      "4263/4263 [==============================] - 2s 430us/step - loss: 0.7460 - acc: 0.6775\n",
      "Epoch 70/160\n",
      "4263/4263 [==============================] - 2s 429us/step - loss: 0.7479 - acc: 0.6737\n",
      "Epoch 71/160\n",
      "4263/4263 [==============================] - 2s 432us/step - loss: 0.7430 - acc: 0.6756\n",
      "Epoch 72/160\n",
      "4263/4263 [==============================] - 2s 467us/step - loss: 0.7455 - acc: 0.6716\n",
      "Epoch 73/160\n",
      "4263/4263 [==============================] - 3s 607us/step - loss: 0.7453 - acc: 0.6777\n",
      "Epoch 74/160\n",
      "4263/4263 [==============================] - 2s 459us/step - loss: 0.7416 - acc: 0.6777\n",
      "Epoch 75/160\n",
      "4263/4263 [==============================] - 2s 430us/step - loss: 0.7455 - acc: 0.6782\n",
      "Epoch 76/160\n",
      "4263/4263 [==============================] - 2s 443us/step - loss: 0.7491 - acc: 0.6777\n",
      "Epoch 77/160\n",
      "4263/4263 [==============================] - 2s 427us/step - loss: 0.7451 - acc: 0.6756\n",
      "Epoch 78/160\n",
      "4263/4263 [==============================] - 2s 437us/step - loss: 0.7384 - acc: 0.6791\n",
      "Epoch 79/160\n",
      "4263/4263 [==============================] - 2s 436us/step - loss: 0.7413 - acc: 0.6739\n",
      "Epoch 80/160\n",
      "4263/4263 [==============================] - 2s 421us/step - loss: 0.7431 - acc: 0.6739\n",
      "Epoch 81/160\n",
      "4263/4263 [==============================] - 2s 428us/step - loss: 0.7364 - acc: 0.6798\n",
      "Epoch 82/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 2s 419us/step - loss: 0.7386 - acc: 0.6880\n",
      "Epoch 83/160\n",
      "4263/4263 [==============================] - 2s 417us/step - loss: 0.7450 - acc: 0.6744\n",
      "Epoch 84/160\n",
      "4263/4263 [==============================] - 2s 421us/step - loss: 0.7352 - acc: 0.6847\n",
      "Epoch 85/160\n",
      "4263/4263 [==============================] - 2s 439us/step - loss: 0.7398 - acc: 0.6716\n",
      "Epoch 86/160\n",
      "4263/4263 [==============================] - 2s 420us/step - loss: 0.7352 - acc: 0.6824\n",
      "Epoch 87/160\n",
      "4263/4263 [==============================] - 2s 435us/step - loss: 0.7375 - acc: 0.6753\n",
      "Epoch 88/160\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.7333 - acc: 0.6845\n",
      "Epoch 89/160\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.7351 - acc: 0.6770\n",
      "Epoch 90/160\n",
      "4263/4263 [==============================] - 2s 436us/step - loss: 0.7309 - acc: 0.6819\n",
      "Epoch 91/160\n",
      "4263/4263 [==============================] - 2s 420us/step - loss: 0.7348 - acc: 0.6847\n",
      "Epoch 92/160\n",
      "4263/4263 [==============================] - 2s 420us/step - loss: 0.7321 - acc: 0.6852\n",
      "Epoch 93/160\n",
      "4263/4263 [==============================] - 2s 447us/step - loss: 0.7324 - acc: 0.6805\n",
      "Epoch 94/160\n",
      "4263/4263 [==============================] - 2s 469us/step - loss: 0.7286 - acc: 0.6906\n",
      "Epoch 95/160\n",
      "4263/4263 [==============================] - 2s 429us/step - loss: 0.7340 - acc: 0.6852\n",
      "Epoch 96/160\n",
      "4263/4263 [==============================] - 2s 417us/step - loss: 0.7345 - acc: 0.6824\n",
      "Epoch 97/160\n",
      "4263/4263 [==============================] - 2s 438us/step - loss: 0.7324 - acc: 0.6821\n",
      "Epoch 98/160\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.7356 - acc: 0.6789\n",
      "Epoch 99/160\n",
      "4263/4263 [==============================] - 2s 428us/step - loss: 0.7348 - acc: 0.6789\n",
      "Epoch 100/160\n",
      "4263/4263 [==============================] - 2s 445us/step - loss: 0.7216 - acc: 0.6847\n",
      "Epoch 101/160\n",
      "4263/4263 [==============================] - 2s 424us/step - loss: 0.7301 - acc: 0.6852\n",
      "Epoch 102/160\n",
      "4263/4263 [==============================] - 2s 437us/step - loss: 0.7260 - acc: 0.6880\n",
      "Epoch 103/160\n",
      "4263/4263 [==============================] - 2s 419us/step - loss: 0.7342 - acc: 0.6821\n",
      "Epoch 104/160\n",
      "4263/4263 [==============================] - 2s 437us/step - loss: 0.7223 - acc: 0.6840\n",
      "Epoch 105/160\n",
      "4263/4263 [==============================] - 2s 379us/step - loss: 0.7286 - acc: 0.6878\n",
      "Epoch 106/160\n",
      "4263/4263 [==============================] - 2s 513us/step - loss: 0.7336 - acc: 0.6725\n",
      "Epoch 107/160\n",
      "4263/4263 [==============================] - 2s 443us/step - loss: 0.7257 - acc: 0.6836\n",
      "Epoch 108/160\n",
      "4263/4263 [==============================] - 2s 473us/step - loss: 0.7208 - acc: 0.6880\n",
      "Epoch 109/160\n",
      "4263/4263 [==============================] - 2s 437us/step - loss: 0.7255 - acc: 0.6871\n",
      "Epoch 110/160\n",
      "4263/4263 [==============================] - 2s 435us/step - loss: 0.7140 - acc: 0.6882\n",
      "Epoch 111/160\n",
      "4263/4263 [==============================] - 2s 439us/step - loss: 0.7290 - acc: 0.6807\n",
      "Epoch 112/160\n",
      "4263/4263 [==============================] - 2s 486us/step - loss: 0.7199 - acc: 0.6927\n",
      "Epoch 113/160\n",
      "4263/4263 [==============================] - 2s 497us/step - loss: 0.7185 - acc: 0.6922\n",
      "Epoch 114/160\n",
      "4263/4263 [==============================] - 2s 584us/step - loss: 0.7268 - acc: 0.6892\n",
      "Epoch 115/160\n",
      "4263/4263 [==============================] - 2s 511us/step - loss: 0.7185 - acc: 0.6887\n",
      "Epoch 116/160\n",
      "4263/4263 [==============================] - 2s 520us/step - loss: 0.7228 - acc: 0.6897\n",
      "Epoch 117/160\n",
      "4263/4263 [==============================] - 2s 544us/step - loss: 0.7130 - acc: 0.6941\n",
      "Epoch 118/160\n",
      "4263/4263 [==============================] - 2s 537us/step - loss: 0.7168 - acc: 0.6892\n",
      "Epoch 119/160\n",
      "4263/4263 [==============================] - 2s 512us/step - loss: 0.7193 - acc: 0.6943\n",
      "Epoch 120/160\n",
      "4263/4263 [==============================] - 2s 552us/step - loss: 0.7118 - acc: 0.6934\n",
      "Epoch 121/160\n",
      "4263/4263 [==============================] - 2s 520us/step - loss: 0.7200 - acc: 0.6873\n",
      "Epoch 122/160\n",
      "4263/4263 [==============================] - 2s 510us/step - loss: 0.7175 - acc: 0.6934\n",
      "Epoch 123/160\n",
      "4263/4263 [==============================] - 2s 441us/step - loss: 0.7199 - acc: 0.6911\n",
      "Epoch 124/160\n",
      "4263/4263 [==============================] - 2s 468us/step - loss: 0.7104 - acc: 0.6965\n",
      "Epoch 125/160\n",
      "4263/4263 [==============================] - 2s 442us/step - loss: 0.7110 - acc: 0.6908\n",
      "Epoch 126/160\n",
      "4263/4263 [==============================] - 2s 428us/step - loss: 0.7083 - acc: 0.6951\n",
      "Epoch 127/160\n",
      "4263/4263 [==============================] - 2s 497us/step - loss: 0.7226 - acc: 0.6864\n",
      "Epoch 128/160\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.7139 - acc: 0.6906\n",
      "Epoch 129/160\n",
      "4263/4263 [==============================] - 2s 438us/step - loss: 0.7125 - acc: 0.6925\n",
      "Epoch 130/160\n",
      "4263/4263 [==============================] - 2s 449us/step - loss: 0.7206 - acc: 0.6911\n",
      "Epoch 131/160\n",
      "4263/4263 [==============================] - 2s 491us/step - loss: 0.7024 - acc: 0.6908\n",
      "Epoch 132/160\n",
      "4263/4263 [==============================] - 2s 499us/step - loss: 0.7083 - acc: 0.6941\n",
      "Epoch 133/160\n",
      "4263/4263 [==============================] - 3s 605us/step - loss: 0.7148 - acc: 0.6915\n",
      "Epoch 134/160\n",
      "4263/4263 [==============================] - 2s 442us/step - loss: 0.7165 - acc: 0.6967\n",
      "Epoch 135/160\n",
      "4263/4263 [==============================] - 2s 518us/step - loss: 0.7079 - acc: 0.6936\n",
      "Epoch 136/160\n",
      "4263/4263 [==============================] - 2s 502us/step - loss: 0.7113 - acc: 0.6967\n",
      "Epoch 137/160\n",
      "4263/4263 [==============================] - 2s 512us/step - loss: 0.7109 - acc: 0.6925\n",
      "Epoch 138/160\n",
      "4263/4263 [==============================] - 2s 524us/step - loss: 0.7039 - acc: 0.6936\n",
      "Epoch 139/160\n",
      "4263/4263 [==============================] - 2s 463us/step - loss: 0.7099 - acc: 0.6913\n",
      "Epoch 140/160\n",
      "4263/4263 [==============================] - 2s 450us/step - loss: 0.7015 - acc: 0.6962\n",
      "Epoch 141/160\n",
      "4263/4263 [==============================] - 2s 500us/step - loss: 0.7049 - acc: 0.6934\n",
      "Epoch 142/160\n",
      "4263/4263 [==============================] - 3s 602us/step - loss: 0.7060 - acc: 0.6934\n",
      "Epoch 143/160\n",
      "4263/4263 [==============================] - 3s 600us/step - loss: 0.7105 - acc: 0.6962\n",
      "Epoch 144/160\n",
      "4263/4263 [==============================] - 2s 583us/step - loss: 0.7089 - acc: 0.6955\n",
      "Epoch 145/160\n",
      "4263/4263 [==============================] - 2s 521us/step - loss: 0.7014 - acc: 0.6936\n",
      "Epoch 146/160\n",
      "4263/4263 [==============================] - 2s 416us/step - loss: 0.7145 - acc: 0.6920\n",
      "Epoch 147/160\n",
      "4263/4263 [==============================] - 2s 461us/step - loss: 0.6963 - acc: 0.7002\n",
      "Epoch 148/160\n",
      "4263/4263 [==============================] - 2s 438us/step - loss: 0.7044 - acc: 0.6918\n",
      "Epoch 149/160\n",
      "4263/4263 [==============================] - 2s 429us/step - loss: 0.7091 - acc: 0.6920\n",
      "Epoch 150/160\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.6956 - acc: 0.6997\n",
      "Epoch 151/160\n",
      "4263/4263 [==============================] - 2s 445us/step - loss: 0.7097 - acc: 0.6885\n",
      "Epoch 152/160\n",
      "4263/4263 [==============================] - 2s 421us/step - loss: 0.6979 - acc: 0.6958\n",
      "Epoch 153/160\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.7067 - acc: 0.6951\n",
      "Epoch 154/160\n",
      "4263/4263 [==============================] - 2s 482us/step - loss: 0.6930 - acc: 0.6995\n",
      "Epoch 155/160\n",
      "4263/4263 [==============================] - 2s 467us/step - loss: 0.7003 - acc: 0.6946\n",
      "Epoch 156/160\n",
      "4263/4263 [==============================] - 2s 434us/step - loss: 0.7071 - acc: 0.6965\n",
      "Epoch 157/160\n",
      "4263/4263 [==============================] - 2s 424us/step - loss: 0.7126 - acc: 0.6890\n",
      "Epoch 158/160\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.6939 - acc: 0.7051\n",
      "Epoch 159/160\n",
      "4263/4263 [==============================] - 2s 473us/step - loss: 0.6924 - acc: 0.6962\n",
      "Epoch 160/160\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.6846 - acc: 0.7002\n",
      "1066/1066 [==============================] - 3s 3ms/step\n",
      "4263/4263 [==============================] - 1s 267us/step\n",
      "\n",
      "acc: 67.92%\n",
      "\n",
      "acc: 71.26%\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 1 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 1 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "[10, 21, 23, 38, 42, 75, 79, 81, 84, 107, 117, 126, 135, 137, 138, 153, 170, 178, 181, 186, 195, 220, 222, 237, 242, 246, 247, 255, 271, 276, 278, 288, 292, 311, 320, 332, 334, 337, 339, 364, 366, 382, 393, 395, 396, 399, 402, 407, 416, 434, 443, 454, 457, 461, 470, 472, 484, 486, 488, 503, 505, 506, 527, 540, 545, 548, 550, 564, 571, 588, 589, 594, 600, 611, 622, 626, 629, 653, 660, 670, 672, 685, 688, 700, 706, 711, 716, 725, 732, 742, 747, 751, 752, 754, 756, 762, 764, 766, 769, 773, 786, 803, 816, 822, 833, 856, 866, 870, 877, 884, 900, 901, 902, 909, 910, 911, 919, 924, 928, 931, 947, 965, 968, 970, 974, 977, 983, 984, 1001, 1013, 1025, 1030, 1038]\n",
      "133\n",
      "933\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'0_Fast'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4380\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4381\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlibindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4382\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_box\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_at\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/util.pxd\u001b[0m in \u001b[0;36mpandas._libs.util.get_value_at\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/util.pxd\u001b[0m in \u001b[0;36mpandas._libs.util.validate_indexer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-2ce476db5d0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_zero\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0my_test_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'0_Fast'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0my_test_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fast'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_test_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'0_Normal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4387\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4388\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4389\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4390\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4391\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4374\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 4375\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   4376\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4377\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '0_Fast'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df[['Class','Mean_speed_kmph']].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test_k = train_test_split(X_d,y_d,test_size=0.2,random_state=42)\n",
    "new_y_2=y_train[0].copy()\n",
    "new_y_d_2=pd.DataFrame(new_y_2)\n",
    "new_y=y_test_k[0].copy()\n",
    "\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "y_test=pd.DataFrame(new_y)\n",
    "\n",
    "y_train_2=pd.get_dummies(new_y_d_2)\n",
    "y_test_2=pd.get_dummies(new_y)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(700, activation='relu'))\n",
    "#model.add(Dense(1000, activation='relu'))\n",
    "#model.add(Dense(1200, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=160,batch_size=50)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "scores_2 = model.evaluate(X_train, y_train_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_2[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "prediction_l=[]\n",
    "count=0\n",
    "all_zero=[]\n",
    "#print(len(predictions))\n",
    "for x in predictions:\n",
    "    k_1=round(x[0])\n",
    "    k_1_i=int(k_1)\n",
    "    k_1_s=str(k_1_i)\n",
    "    k_2=round(x[1])\n",
    "    k_2_i=int(k_2)\n",
    "    k_2_s=str(k_2_i)\n",
    "    k_3=round(x[2])\n",
    "    k_3_i=int(k_3)\n",
    "    k_3_s=str(k_3_i)\n",
    "    k_4=round(x[3])\n",
    "    k_4_i=int(k_4)\n",
    "    k_4_s=str(k_4_i)\n",
    "    if k_1_i==0 and k_2_i==0 and k_3_i==0 and k_4_i==0:\n",
    "        all_zero.append(count)\n",
    "    print(k_1_s+' '+k_2_s+' '+k_3_s+' '+k_4_s)\n",
    "    if k_1_i==1:\n",
    "        prediction_l.append('Fast')\n",
    "    if k_2_i==1:\n",
    "        prediction_l.append('Normal')\n",
    "    if k_3_i==1:\n",
    "        prediction_l.append('Slow')\n",
    "    if k_4_i==1:\n",
    "        prediction_l.append('Very Fast')\n",
    "    count=count+1\n",
    "    \n",
    "print(all_zero)\n",
    "print(len(all_zero))\n",
    "#print(prediction_l)\n",
    "print(len(prediction_l))\n",
    "\n",
    "y_test_l=[]\n",
    "\n",
    "l=len(y_test_2)\n",
    "i=0\n",
    "while i<l:\n",
    "    if i not in all_zero:\n",
    "        if y_test_2.iloc[i]['0_Fast']==1:\n",
    "            y_test_l.append('Fast')\n",
    "        if y_test_2.iloc[i]['0_Normal']==1:\n",
    "            y_test_l.append('Normal')\n",
    "        if y_test_2.iloc[i]['0_Slow']==1:\n",
    "            y_test_l.append('Slow')\n",
    "        if y_test_2.iloc[i]['0_Very Fast']==1:\n",
    "            y_test_l.append('Very Fast')\n",
    "    i=i+1\n",
    "    #print(i)\n",
    "print(len(y_test_l))\n",
    "print(y_test_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Epoch 1/160\n",
      "4263/4263 [==============================] - 11s 3ms/step - loss: 0.4538 - acc: 0.7925\n",
      "Epoch 2/160\n",
      "4263/4263 [==============================] - 2s 428us/step - loss: 0.4013 - acc: 0.8076\n",
      "Epoch 3/160\n",
      "4263/4263 [==============================] - 2s 523us/step - loss: 0.3903 - acc: 0.8132\n",
      "Epoch 4/160\n",
      "4263/4263 [==============================] - 2s 551us/step - loss: 0.3818 - acc: 0.8163\n",
      "Epoch 5/160\n",
      "4263/4263 [==============================] - 2s 430us/step - loss: 0.3798 - acc: 0.8198\n",
      "Epoch 6/160\n",
      "4263/4263 [==============================] - 2s 407us/step - loss: 0.3747 - acc: 0.8200\n",
      "Epoch 7/160\n",
      "4263/4263 [==============================] - 2s 402us/step - loss: 0.3716 - acc: 0.8208\n",
      "Epoch 8/160\n",
      "4263/4263 [==============================] - 2s 431us/step - loss: 0.3740 - acc: 0.8217\n",
      "Epoch 9/160\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.3700 - acc: 0.8251\n",
      "Epoch 10/160\n",
      "4263/4263 [==============================] - 2s 412us/step - loss: 0.3648 - acc: 0.8261\n",
      "Epoch 11/160\n",
      "4263/4263 [==============================] - 2s 404us/step - loss: 0.3649 - acc: 0.8259\n",
      "Epoch 12/160\n",
      "4263/4263 [==============================] - 2s 412us/step - loss: 0.3648 - acc: 0.8275\n",
      "Epoch 13/160\n",
      "4263/4263 [==============================] - 2s 396us/step - loss: 0.3639 - acc: 0.8281\n",
      "Epoch 14/160\n",
      "4263/4263 [==============================] - 2s 401us/step - loss: 0.3618 - acc: 0.8286\n",
      "Epoch 15/160\n",
      "4263/4263 [==============================] - 2s 409us/step - loss: 0.3628 - acc: 0.8268\n",
      "Epoch 16/160\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.3605 - acc: 0.8295\n",
      "Epoch 17/160\n",
      "4263/4263 [==============================] - 2s 423us/step - loss: 0.3578 - acc: 0.8328\n",
      "Epoch 18/160\n",
      "4263/4263 [==============================] - 2s 408us/step - loss: 0.3541 - acc: 0.8325\n",
      "Epoch 19/160\n",
      "4263/4263 [==============================] - 2s 408us/step - loss: 0.3535 - acc: 0.8326\n",
      "Epoch 20/160\n",
      "4263/4263 [==============================] - 2s 402us/step - loss: 0.3522 - acc: 0.8335\n",
      "Epoch 21/160\n",
      "4263/4263 [==============================] - 2s 434us/step - loss: 0.3524 - acc: 0.8349\n",
      "Epoch 22/160\n",
      "4263/4263 [==============================] - 2s 446us/step - loss: 0.3567 - acc: 0.8284\n",
      "Epoch 23/160\n",
      "4263/4263 [==============================] - 2s 403us/step - loss: 0.3521 - acc: 0.8352\n",
      "Epoch 24/160\n",
      "4263/4263 [==============================] - 2s 402us/step - loss: 0.3567 - acc: 0.8323\n",
      "Epoch 25/160\n",
      "4263/4263 [==============================] - 2s 404us/step - loss: 0.3517 - acc: 0.8360\n",
      "Epoch 26/160\n",
      "4263/4263 [==============================] - 2s 413us/step - loss: 0.3536 - acc: 0.8303\n",
      "Epoch 27/160\n",
      "4263/4263 [==============================] - 2s 403us/step - loss: 0.3536 - acc: 0.8319\n",
      "Epoch 28/160\n",
      "4263/4263 [==============================] - 2s 414us/step - loss: 0.3490 - acc: 0.8388\n",
      "Epoch 29/160\n",
      "4263/4263 [==============================] - 2s 402us/step - loss: 0.3512 - acc: 0.8310\n",
      "Epoch 30/160\n",
      "4263/4263 [==============================] - 2s 412us/step - loss: 0.3511 - acc: 0.8344\n",
      "Epoch 31/160\n",
      "4263/4263 [==============================] - 2s 401us/step - loss: 0.3474 - acc: 0.8363\n",
      "Epoch 32/160\n",
      "4263/4263 [==============================] - 2s 403us/step - loss: 0.3493 - acc: 0.8349\n",
      "Epoch 33/160\n",
      "4263/4263 [==============================] - 2s 420us/step - loss: 0.3480 - acc: 0.8349\n",
      "Epoch 34/160\n",
      "4263/4263 [==============================] - 2s 406us/step - loss: 0.3497 - acc: 0.8334\n",
      "Epoch 35/160\n",
      "4263/4263 [==============================] - 2s 404us/step - loss: 0.3453 - acc: 0.8365\n",
      "Epoch 36/160\n",
      "4263/4263 [==============================] - 2s 438us/step - loss: 0.3473 - acc: 0.8350\n",
      "Epoch 37/160\n",
      "4263/4263 [==============================] - 2s 436us/step - loss: 0.3450 - acc: 0.8366\n",
      "Epoch 38/160\n",
      "4263/4263 [==============================] - 2s 409us/step - loss: 0.3458 - acc: 0.8333\n",
      "Epoch 39/160\n",
      "4263/4263 [==============================] - 2s 408us/step - loss: 0.3463 - acc: 0.8391\n",
      "Epoch 40/160\n",
      "4263/4263 [==============================] - 2s 412us/step - loss: 0.3477 - acc: 0.8337\n",
      "Epoch 41/160\n",
      "4263/4263 [==============================] - 2s 427us/step - loss: 0.3439 - acc: 0.8353\n",
      "Epoch 42/160\n",
      "4263/4263 [==============================] - 2s 419us/step - loss: 0.3467 - acc: 0.8357\n",
      "Epoch 43/160\n",
      "4263/4263 [==============================] - 2s 410us/step - loss: 0.3438 - acc: 0.8382\n",
      "Epoch 44/160\n",
      "4263/4263 [==============================] - 2s 409us/step - loss: 0.3460 - acc: 0.8370\n",
      "Epoch 45/160\n",
      "4263/4263 [==============================] - 2s 405us/step - loss: 0.3447 - acc: 0.8383\n",
      "Epoch 46/160\n",
      "4263/4263 [==============================] - 2s 402us/step - loss: 0.3438 - acc: 0.8386\n",
      "Epoch 47/160\n",
      "4263/4263 [==============================] - 2s 401us/step - loss: 0.3438 - acc: 0.8397\n",
      "Epoch 48/160\n",
      "4263/4263 [==============================] - 2s 420us/step - loss: 0.3420 - acc: 0.8404\n",
      "Epoch 49/160\n",
      "4263/4263 [==============================] - 2s 417us/step - loss: 0.3424 - acc: 0.8373\n",
      "Epoch 50/160\n",
      "4263/4263 [==============================] - 2s 416us/step - loss: 0.3421 - acc: 0.8402\n",
      "Epoch 51/160\n",
      "4263/4263 [==============================] - 2s 399us/step - loss: 0.3433 - acc: 0.8370\n",
      "Epoch 52/160\n",
      "4263/4263 [==============================] - 2s 411us/step - loss: 0.3457 - acc: 0.8359\n",
      "Epoch 53/160\n",
      "4263/4263 [==============================] - 2s 412us/step - loss: 0.3413 - acc: 0.8405\n",
      "Epoch 54/160\n",
      "4263/4263 [==============================] - 2s 403us/step - loss: 0.3419 - acc: 0.8373\n",
      "Epoch 55/160\n",
      "4263/4263 [==============================] - 2s 409us/step - loss: 0.3442 - acc: 0.8365\n",
      "Epoch 56/160\n",
      "4263/4263 [==============================] - 2s 400us/step - loss: 0.3407 - acc: 0.8388\n",
      "Epoch 57/160\n",
      "4263/4263 [==============================] - 2s 407us/step - loss: 0.3414 - acc: 0.8386\n",
      "Epoch 58/160\n",
      "4263/4263 [==============================] - 2s 416us/step - loss: 0.3426 - acc: 0.8371\n",
      "Epoch 59/160\n",
      "4263/4263 [==============================] - 2s 404us/step - loss: 0.3433 - acc: 0.8366\n",
      "Epoch 60/160\n",
      "4263/4263 [==============================] - 2s 408us/step - loss: 0.3397 - acc: 0.8401\n",
      "Epoch 61/160\n",
      "4263/4263 [==============================] - 2s 405us/step - loss: 0.3414 - acc: 0.8386\n",
      "Epoch 62/160\n",
      "4263/4263 [==============================] - 2s 417us/step - loss: 0.3421 - acc: 0.8377\n",
      "Epoch 63/160\n",
      "4263/4263 [==============================] - 2s 401us/step - loss: 0.3385 - acc: 0.8391\n",
      "Epoch 64/160\n",
      "4263/4263 [==============================] - 2s 415us/step - loss: 0.3394 - acc: 0.8403\n",
      "Epoch 65/160\n",
      "4263/4263 [==============================] - 2s 407us/step - loss: 0.3389 - acc: 0.8405\n",
      "Epoch 66/160\n",
      "4263/4263 [==============================] - 2s 402us/step - loss: 0.3380 - acc: 0.8407\n",
      "Epoch 67/160\n",
      "4263/4263 [==============================] - 2s 397us/step - loss: 0.3411 - acc: 0.8399\n",
      "Epoch 68/160\n",
      "4263/4263 [==============================] - 2s 408us/step - loss: 0.3375 - acc: 0.8422\n",
      "Epoch 69/160\n",
      "4263/4263 [==============================] - 2s 406us/step - loss: 0.3393 - acc: 0.8413\n",
      "Epoch 70/160\n",
      "4263/4263 [==============================] - 2s 405us/step - loss: 0.3409 - acc: 0.8404\n",
      "Epoch 71/160\n",
      "4263/4263 [==============================] - 2s 415us/step - loss: 0.3412 - acc: 0.8426\n",
      "Epoch 72/160\n",
      "4263/4263 [==============================] - 2s 406us/step - loss: 0.3365 - acc: 0.8419\n",
      "Epoch 73/160\n",
      "4263/4263 [==============================] - 2s 445us/step - loss: 0.3363 - acc: 0.8413\n",
      "Epoch 74/160\n",
      "4263/4263 [==============================] - 2s 489us/step - loss: 0.3404 - acc: 0.8402\n",
      "Epoch 75/160\n",
      "4263/4263 [==============================] - 2s 438us/step - loss: 0.3392 - acc: 0.8395\n",
      "Epoch 76/160\n",
      "4263/4263 [==============================] - 2s 408us/step - loss: 0.3398 - acc: 0.8387\n",
      "Epoch 77/160\n",
      "4263/4263 [==============================] - 2s 413us/step - loss: 0.3372 - acc: 0.8410\n",
      "Epoch 78/160\n",
      "4263/4263 [==============================] - 2s 449us/step - loss: 0.3432 - acc: 0.8396\n",
      "Epoch 79/160\n",
      "4263/4263 [==============================] - 3s 596us/step - loss: 0.3377 - acc: 0.8413\n",
      "Epoch 80/160\n",
      "4263/4263 [==============================] - 2s 586us/step - loss: 0.3384 - acc: 0.8418\n",
      "Epoch 81/160\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.3347 - acc: 0.8457\n",
      "Epoch 82/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 2s 401us/step - loss: 0.3347 - acc: 0.8436\n",
      "Epoch 83/160\n",
      "4263/4263 [==============================] - 2s 404us/step - loss: 0.3363 - acc: 0.8421\n",
      "Epoch 84/160\n",
      "4263/4263 [==============================] - 2s 416us/step - loss: 0.3367 - acc: 0.8407\n",
      "Epoch 85/160\n",
      "4263/4263 [==============================] - 2s 398us/step - loss: 0.3396 - acc: 0.8402\n",
      "Epoch 86/160\n",
      "4263/4263 [==============================] - 2s 398us/step - loss: 0.3339 - acc: 0.8442\n",
      "Epoch 87/160\n",
      "4263/4263 [==============================] - 2s 397us/step - loss: 0.3369 - acc: 0.8431\n",
      "Epoch 88/160\n",
      "4263/4263 [==============================] - 2s 416us/step - loss: 0.3397 - acc: 0.8391\n",
      "Epoch 89/160\n",
      "4263/4263 [==============================] - 2s 397us/step - loss: 0.3341 - acc: 0.8459\n",
      "Epoch 90/160\n",
      "4263/4263 [==============================] - 2s 404us/step - loss: 0.3367 - acc: 0.8408\n",
      "Epoch 91/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.3346 - acc: 0.8445\n",
      "Epoch 92/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.3336 - acc: 0.8426\n",
      "Epoch 93/160\n",
      "4263/4263 [==============================] - 2s 397us/step - loss: 0.3341 - acc: 0.8441\n",
      "Epoch 94/160\n",
      "4263/4263 [==============================] - 2s 404us/step - loss: 0.3356 - acc: 0.8442\n",
      "Epoch 95/160\n",
      "4263/4263 [==============================] - 2s 397us/step - loss: 0.3355 - acc: 0.8439\n",
      "Epoch 96/160\n",
      "4263/4263 [==============================] - 2s 527us/step - loss: 0.3354 - acc: 0.8420\n",
      "Epoch 97/160\n",
      "4263/4263 [==============================] - 2s 525us/step - loss: 0.3349 - acc: 0.8411\n",
      "Epoch 98/160\n",
      "4263/4263 [==============================] - 2s 403us/step - loss: 0.3365 - acc: 0.8411\n",
      "Epoch 99/160\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.3351 - acc: 0.8427\n",
      "Epoch 100/160\n",
      "4263/4263 [==============================] - 2s 395us/step - loss: 0.3302 - acc: 0.8438\n",
      "Epoch 101/160\n",
      "4263/4263 [==============================] - 2s 399us/step - loss: 0.3332 - acc: 0.8420\n",
      "Epoch 102/160\n",
      "4263/4263 [==============================] - 2s 398us/step - loss: 0.3322 - acc: 0.8450\n",
      "Epoch 103/160\n",
      "4263/4263 [==============================] - 2s 397us/step - loss: 0.3305 - acc: 0.8435\n",
      "Epoch 104/160\n",
      "4263/4263 [==============================] - 2s 394us/step - loss: 0.3352 - acc: 0.8426\n",
      "Epoch 105/160\n",
      "4263/4263 [==============================] - 2s 395us/step - loss: 0.3344 - acc: 0.8424\n",
      "Epoch 106/160\n",
      "4263/4263 [==============================] - 2s 415us/step - loss: 0.3310 - acc: 0.8456\n",
      "Epoch 107/160\n",
      "4263/4263 [==============================] - 2s 424us/step - loss: 0.3319 - acc: 0.8456\n",
      "Epoch 108/160\n",
      "4263/4263 [==============================] - 2s 404us/step - loss: 0.3355 - acc: 0.8449\n",
      "Epoch 109/160\n",
      "4263/4263 [==============================] - 2s 393us/step - loss: 0.3299 - acc: 0.8459\n",
      "Epoch 110/160\n",
      "4263/4263 [==============================] - 2s 396us/step - loss: 0.3342 - acc: 0.8447\n",
      "Epoch 111/160\n",
      "4263/4263 [==============================] - 2s 397us/step - loss: 0.3325 - acc: 0.8439\n",
      "Epoch 112/160\n",
      "4263/4263 [==============================] - 2s 429us/step - loss: 0.3305 - acc: 0.8454\n",
      "Epoch 113/160\n",
      "4263/4263 [==============================] - 2s 400us/step - loss: 0.3296 - acc: 0.8475\n",
      "Epoch 114/160\n",
      "4263/4263 [==============================] - 2s 403us/step - loss: 0.3314 - acc: 0.8463\n",
      "Epoch 115/160\n",
      "4263/4263 [==============================] - 2s 420us/step - loss: 0.3301 - acc: 0.8464\n",
      "Epoch 116/160\n",
      "4263/4263 [==============================] - 2s 424us/step - loss: 0.3342 - acc: 0.8444\n",
      "Epoch 117/160\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.3317 - acc: 0.8462\n",
      "Epoch 118/160\n",
      "4263/4263 [==============================] - 2s 454us/step - loss: 0.3260 - acc: 0.8496\n",
      "Epoch 119/160\n",
      "4263/4263 [==============================] - 2s 444us/step - loss: 0.3299 - acc: 0.8466\n",
      "Epoch 120/160\n",
      "4263/4263 [==============================] - 2s 405us/step - loss: 0.3287 - acc: 0.8475\n",
      "Epoch 121/160\n",
      "4263/4263 [==============================] - 2s 434us/step - loss: 0.3255 - acc: 0.8473\n",
      "Epoch 122/160\n",
      "4263/4263 [==============================] - 2s 524us/step - loss: 0.3258 - acc: 0.8490\n",
      "Epoch 123/160\n",
      "4263/4263 [==============================] - 2s 496us/step - loss: 0.3277 - acc: 0.8482\n",
      "Epoch 124/160\n",
      "4263/4263 [==============================] - 2s 431us/step - loss: 0.3316 - acc: 0.8441\n",
      "Epoch 125/160\n",
      "4263/4263 [==============================] - 2s 485us/step - loss: 0.3305 - acc: 0.8466\n",
      "Epoch 126/160\n",
      "4263/4263 [==============================] - 2s 555us/step - loss: 0.3298 - acc: 0.8451\n",
      "Epoch 127/160\n",
      "4263/4263 [==============================] - 2s 503us/step - loss: 0.3267 - acc: 0.8483\n",
      "Epoch 128/160\n",
      "4263/4263 [==============================] - 2s 490us/step - loss: 0.3312 - acc: 0.8468\n",
      "Epoch 129/160\n",
      "4263/4263 [==============================] - 2s 476us/step - loss: 0.3302 - acc: 0.8434\n",
      "Epoch 130/160\n",
      "4263/4263 [==============================] - 2s 494us/step - loss: 0.3285 - acc: 0.8481\n",
      "Epoch 131/160\n",
      "4263/4263 [==============================] - 2s 452us/step - loss: 0.3249 - acc: 0.8504\n",
      "Epoch 132/160\n",
      "4263/4263 [==============================] - 2s 429us/step - loss: 0.3280 - acc: 0.8503\n",
      "Epoch 133/160\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.3236 - acc: 0.8515\n",
      "Epoch 134/160\n",
      "4263/4263 [==============================] - 2s 442us/step - loss: 0.3302 - acc: 0.8466\n",
      "Epoch 135/160\n",
      "4263/4263 [==============================] - 2s 456us/step - loss: 0.3296 - acc: 0.8456\n",
      "Epoch 136/160\n",
      "4263/4263 [==============================] - 2s 413us/step - loss: 0.3257 - acc: 0.8476\n",
      "Epoch 137/160\n",
      "4263/4263 [==============================] - 2s 421us/step - loss: 0.3282 - acc: 0.8488\n",
      "Epoch 138/160\n",
      "4263/4263 [==============================] - 2s 445us/step - loss: 0.3260 - acc: 0.8503\n",
      "Epoch 139/160\n",
      "4263/4263 [==============================] - 2s 424us/step - loss: 0.3264 - acc: 0.8490\n",
      "Epoch 140/160\n",
      "4263/4263 [==============================] - 2s 426us/step - loss: 0.3271 - acc: 0.8456\n",
      "Epoch 141/160\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.3266 - acc: 0.8468\n",
      "Epoch 142/160\n",
      "4263/4263 [==============================] - 2s 412us/step - loss: 0.3245 - acc: 0.8486\n",
      "Epoch 143/160\n",
      "4263/4263 [==============================] - 2s 420us/step - loss: 0.3267 - acc: 0.8496\n",
      "Epoch 144/160\n",
      "4263/4263 [==============================] - 2s 470us/step - loss: 0.3204 - acc: 0.8508\n",
      "Epoch 145/160\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.3237 - acc: 0.8507\n",
      "Epoch 146/160\n",
      "4263/4263 [==============================] - 2s 490us/step - loss: 0.3208 - acc: 0.8497\n",
      "Epoch 147/160\n",
      "4263/4263 [==============================] - 2s 441us/step - loss: 0.3288 - acc: 0.8487\n",
      "Epoch 148/160\n",
      "4263/4263 [==============================] - 2s 420us/step - loss: 0.3260 - acc: 0.8485\n",
      "Epoch 149/160\n",
      "4263/4263 [==============================] - 2s 486us/step - loss: 0.3248 - acc: 0.8488\n",
      "Epoch 150/160\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.3207 - acc: 0.8508\n",
      "Epoch 151/160\n",
      "4263/4263 [==============================] - 2s 532us/step - loss: 0.3209 - acc: 0.8500\n",
      "Epoch 152/160\n",
      "4263/4263 [==============================] - 2s 583us/step - loss: 0.3216 - acc: 0.8516\n",
      "Epoch 153/160\n",
      "4263/4263 [==============================] - 2s 496us/step - loss: 0.3190 - acc: 0.8520\n",
      "Epoch 154/160\n",
      "4263/4263 [==============================] - 2s 429us/step - loss: 0.3255 - acc: 0.8477\n",
      "Epoch 155/160\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.3251 - acc: 0.8477\n",
      "Epoch 156/160\n",
      "4263/4263 [==============================] - 2s 414us/step - loss: 0.3198 - acc: 0.8490\n",
      "Epoch 157/160\n",
      "4263/4263 [==============================] - 2s 414us/step - loss: 0.3201 - acc: 0.8513\n",
      "Epoch 158/160\n",
      "4263/4263 [==============================] - 2s 413us/step - loss: 0.3227 - acc: 0.8502\n",
      "Epoch 159/160\n",
      "4263/4263 [==============================] - 2s 418us/step - loss: 0.3205 - acc: 0.8499\n",
      "Epoch 160/160\n",
      "4263/4263 [==============================] - 2s 419us/step - loss: 0.3194 - acc: 0.8505\n",
      "1066/1066 [==============================] - 3s 3ms/step\n",
      "4263/4263 [==============================] - 1s 265us/step\n",
      "\n",
      "acc: 84.05%\n",
      "\n",
      "acc: 85.50%\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 1 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 0 1 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 1 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 1 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 1 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "[10, 21, 36, 42, 117, 120, 137, 138, 162, 181, 186, 204, 220, 222, 237, 245, 246, 255, 271, 278, 292, 320, 332, 334, 364, 393, 395, 402, 407, 416, 417, 454, 461, 468, 484, 486, 488, 492, 503, 505, 506, 525, 527, 558, 561, 571, 578, 585, 588, 594, 600, 611, 622, 626, 629, 653, 670, 672, 688, 711, 716, 725, 732, 740, 742, 747, 751, 754, 762, 764, 769, 773, 776, 782, 787, 803, 810, 816, 822, 827, 833, 834, 850, 856, 878, 884, 892, 900, 901, 902, 909, 910, 924, 942, 965, 970, 984, 1013, 1025, 1030]\n",
      "100\n",
      "966\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'0_Fast'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4380\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4381\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlibindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4382\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_box\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_at\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/util.pxd\u001b[0m in \u001b[0;36mpandas._libs.util.get_value_at\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/util.pxd\u001b[0m in \u001b[0;36mpandas._libs.util.validate_indexer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-6ca139a97016>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_zero\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0my_test_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'0_Fast'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m             \u001b[0my_test_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fast'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_test_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'0_Normal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4387\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4388\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4389\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4390\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4391\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4374\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 4375\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   4376\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4377\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '0_Fast'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df[['Class','Mean_speed_kmph']].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test_k = train_test_split(X_d,y_d,test_size=0.2,random_state=42)\n",
    "new_y_2=y_train[0].copy()\n",
    "new_y_d_2=pd.DataFrame(new_y_2)\n",
    "new_y=y_test_k[0].copy()\n",
    "\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "y_test=pd.DataFrame(new_y)\n",
    "\n",
    "y_train_2=pd.get_dummies(new_y_d_2)\n",
    "y_test_2=pd.get_dummies(new_y)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(600, activation='relu'))\n",
    "#model.add(Dense(1000, activation='relu'))\n",
    "#model.add(Dense(1200, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=160, callbacks=[early_stopping_monitor],batch_size=50)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "scores_2 = model.evaluate(X_train, y_train_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "predictions=model.predict(X_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_2[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "#print(value_check)\n",
    "\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "prediction_l=[]\n",
    "count=0\n",
    "all_zero=[]\n",
    "#print(len(predictions))\n",
    "for x in predictions:\n",
    "    k_1=round(x[0])\n",
    "    k_1_i=int(k_1)\n",
    "    k_1_s=str(k_1_i)\n",
    "    k_2=round(x[1])\n",
    "    k_2_i=int(k_2)\n",
    "    k_2_s=str(k_2_i)\n",
    "    k_3=round(x[2])\n",
    "    k_3_i=int(k_3)\n",
    "    k_3_s=str(k_3_i)\n",
    "    k_4=round(x[3])\n",
    "    k_4_i=int(k_4)\n",
    "    k_4_s=str(k_4_i)\n",
    "    if k_1_i==0 and k_2_i==0 and k_3_i==0 and k_4_i==0:\n",
    "        all_zero.append(count)\n",
    "    print(k_1_s+' '+k_2_s+' '+k_3_s+' '+k_4_s)\n",
    "    if k_1_i==1:\n",
    "        prediction_l.append('Fast')\n",
    "    if k_2_i==1:\n",
    "        prediction_l.append('Normal')\n",
    "    if k_3_i==1:\n",
    "        prediction_l.append('Slow')\n",
    "    if k_4_i==1:\n",
    "        prediction_l.append('Very Fast')\n",
    "    count=count+1\n",
    "    \n",
    "print(all_zero)\n",
    "print(len(all_zero))\n",
    "#print(prediction_l)\n",
    "print(len(prediction_l))\n",
    "\n",
    "y_test_l=[]\n",
    "\n",
    "l=len(y_test_2)\n",
    "j=0\n",
    "while j<l:\n",
    "    if j not in all_zero:\n",
    "        if y_test_2.iloc[j]['0_Fast']==1:\n",
    "            y_test_l.append('Fast')\n",
    "        if y_test_2.iloc[j]['0_Normal']==1:\n",
    "            y_test_l.append('Normal')\n",
    "        if y_test_2.iloc[j]['0_Slow']==1:\n",
    "            y_test_l.append('Slow')\n",
    "        if y_test_2.iloc[j]['0_Very Fast']==1:\n",
    "            y_test_l.append('Very Fast')\n",
    "    j=j+1\n",
    "    #print(i)\n",
    "print(len(y_test_l))\n",
    "print(y_test_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 21, 36, 42, 117, 120, 137, 138, 162, 181, 186, 204, 220, 222, 237, 245, 246, 255, 271, 278, 292, 320, 332, 334, 364, 393, 395, 402, 407, 416, 417, 454, 461, 468, 484, 486, 488, 492, 503, 505, 506, 525, 527, 558, 561, 571, 578, 585, 588, 594, 600, 611, 622, 626, 629, 653, 670, 672, 688, 711, 716, 725, 732, 740, 742, 747, 751, 754, 762, 764, 769, 773, 776, 782, 787, 803, 810, 816, 822, 827, 833, 834, 850, 856, 878, 884, 892, 900, 901, 902, 909, 910, 924, 942, 965, 970, 984, 1013, 1025, 1030]\n",
      "100\n",
      "966\n",
      "966\n",
      "['Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Slow', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Fast', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Normal', 'Slow', 'Fast', 'Fast', 'Very Fast', 'Very Fast', 'Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Slow', 'Very Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Very Fast', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Normal', 'Normal', 'Slow', 'Slow', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Fast', 'Normal', 'Slow', 'Very Fast', 'Normal', 'Fast', 'Slow', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Fast', 'Normal', 'Slow', 'Normal', 'Normal', 'Very Fast', 'Very Fast', 'Fast', 'Normal', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Very Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Very Fast', 'Normal', 'Fast', 'Fast', 'Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Very Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Slow', 'Normal', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Fast', 'Fast', 'Fast', 'Normal', 'Very Fast', 'Slow', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Slow', 'Very Fast', 'Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Very Fast', 'Slow', 'Fast', 'Slow', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Fast', 'Very Fast', 'Normal', 'Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Fast', 'Very Fast', 'Normal', 'Very Fast', 'Fast', 'Normal', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Slow', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Slow', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Slow', 'Normal', 'Fast', 'Fast', 'Fast', 'Fast', 'Normal', 'Fast', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Fast', 'Normal', 'Slow', 'Normal', 'Normal', 'Slow', 'Normal', 'Fast', 'Fast', 'Very Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Slow', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Fast', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Fast', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Fast', 'Slow', 'Fast', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Very Fast', 'Slow', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Very Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Slow', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Very Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Slow', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Slow', 'Slow', 'Normal', 'Slow', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Fast', 'Normal', 'Very Fast', 'Normal', 'Fast', 'Fast', 'Normal', 'Slow', 'Normal', 'Slow', 'Normal', 'Slow', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Very Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Very Fast', 'Very Fast', 'Very Fast', 'Normal', 'Slow', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Very Fast', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Fast', 'Normal', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Fast', 'Fast', 'Fast', 'Fast', 'Normal', 'Normal', 'Fast', 'Fast', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Normal', 'Fast', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Slow', 'Very Fast', 'Slow', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Fast', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Very Fast', 'Normal', 'Slow', 'Normal', 'Slow', 'Normal', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Slow', 'Normal', 'Very Fast', 'Normal', 'Fast', 'Slow', 'Normal', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Slow', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Slow', 'Normal', 'Fast', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Very Fast', 'Very Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Fast', 'Fast', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Slow', 'Very Fast', 'Normal']\n"
     ]
    }
   ],
   "source": [
    "print(all_zero)\n",
    "print(len(all_zero))\n",
    "#print(prediction_l)\n",
    "print(len(prediction_l))\n",
    "\n",
    "y_test_l=[]\n",
    "\n",
    "l=len(y_test_2)\n",
    "j=0\n",
    "while j<l:\n",
    "    if j not in all_zero:\n",
    "        if y_test_2.iloc[j][0]==1:\n",
    "            y_test_l.append('Fast')\n",
    "        if y_test_2.iloc[j][1]==1:\n",
    "            y_test_l.append('Normal')\n",
    "        if y_test_2.iloc[j][2]==1:\n",
    "            y_test_l.append('Slow')\n",
    "        if y_test_2.iloc[j][3]==1:\n",
    "            y_test_l.append('Very Fast')\n",
    "    j=j+1\n",
    "    #print(i)\n",
    "print(len(y_test_l))\n",
    "print(y_test_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 75 124   1  36]\n",
      " [ 32 509  21   9]\n",
      " [  0  53  37   0]\n",
      " [  8   6   0  55]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fast       0.65      0.32      0.43       236\n",
      "      Normal       0.74      0.89      0.81       571\n",
      "        Slow       0.63      0.41      0.50        90\n",
      "   Very Fast       0.55      0.80      0.65        69\n",
      "\n",
      "    accuracy                           0.70       966\n",
      "   macro avg       0.64      0.60      0.60       966\n",
      "weighted avg       0.69      0.70      0.67       966\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6997929606625258"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "a=confusion_matrix(y_test_l,prediction_l)\n",
    "print(a)\n",
    "\n",
    "print(classification_report(y_test_l,prediction_l))\n",
    "accuracy_score(y_test_l,prediction_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 75 124   1  36]\n",
      " [ 32 509  21   9]\n",
      " [  0  53  37   0]\n",
      " [  8   6   0  55]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fast       0.65      0.32      0.43       236\n",
      "      Normal       0.74      0.89      0.81       571\n",
      "        Slow       0.63      0.41      0.50        90\n",
      "   Very Fast       0.55      0.80      0.65        69\n",
      "\n",
      "    accuracy                           0.70       966\n",
      "   macro avg       0.64      0.60      0.60       966\n",
      "weighted avg       0.69      0.70      0.67       966\n",
      "\n",
      "4\n",
      "Epoch 1/160\n",
      "4263/4263 [==============================] - 11s 2ms/step - loss: 0.4437 - acc: 0.7923\n",
      "Epoch 2/160\n",
      "4263/4263 [==============================] - 2s 418us/step - loss: 0.4020 - acc: 0.8089\n",
      "Epoch 3/160\n",
      "4263/4263 [==============================] - 2s 404us/step - loss: 0.3915 - acc: 0.8135\n",
      "Epoch 4/160\n",
      "4263/4263 [==============================] - 2s 410us/step - loss: 0.3856 - acc: 0.8134\n",
      "Epoch 5/160\n",
      "4263/4263 [==============================] - 2s 432us/step - loss: 0.3803 - acc: 0.8179\n",
      "Epoch 6/160\n",
      "4263/4263 [==============================] - 2s 419us/step - loss: 0.3742 - acc: 0.8210\n",
      "Epoch 7/160\n",
      "4263/4263 [==============================] - 2s 451us/step - loss: 0.3768 - acc: 0.8225\n",
      "Epoch 8/160\n",
      "4263/4263 [==============================] - 2s 437us/step - loss: 0.3746 - acc: 0.8200\n",
      "Epoch 9/160\n",
      "4263/4263 [==============================] - 2s 438us/step - loss: 0.3711 - acc: 0.8232\n",
      "Epoch 10/160\n",
      "4263/4263 [==============================] - 2s 436us/step - loss: 0.3702 - acc: 0.8238\n",
      "Epoch 11/160\n",
      "4263/4263 [==============================] - 2s 431us/step - loss: 0.3685 - acc: 0.8265\n",
      "Epoch 12/160\n",
      "4263/4263 [==============================] - 2s 462us/step - loss: 0.3701 - acc: 0.8247\n",
      "Epoch 13/160\n",
      "4263/4263 [==============================] - 2s 487us/step - loss: 0.3637 - acc: 0.8276\n",
      "Epoch 14/160\n",
      "4263/4263 [==============================] - 3s 605us/step - loss: 0.3632 - acc: 0.8272\n",
      "Epoch 15/160\n",
      "4263/4263 [==============================] - 2s 494us/step - loss: 0.3593 - acc: 0.8308\n",
      "Epoch 16/160\n",
      "4263/4263 [==============================] - 1s 330us/step - loss: 0.3607 - acc: 0.8291\n",
      "Epoch 17/160\n",
      "4263/4263 [==============================] - 1s 328us/step - loss: 0.3603 - acc: 0.8274\n",
      "Epoch 18/160\n",
      "4263/4263 [==============================] - 1s 328us/step - loss: 0.3588 - acc: 0.8317\n",
      "Epoch 19/160\n",
      "4263/4263 [==============================] - 1s 338us/step - loss: 0.3608 - acc: 0.8305\n",
      "Epoch 20/160\n",
      "4263/4263 [==============================] - 1s 328us/step - loss: 0.3577 - acc: 0.8316\n",
      "Epoch 21/160\n",
      "4263/4263 [==============================] - 1s 328us/step - loss: 0.3577 - acc: 0.8322\n",
      "Epoch 22/160\n",
      "4263/4263 [==============================] - 2s 385us/step - loss: 0.3560 - acc: 0.8329\n",
      "Epoch 23/160\n",
      "4263/4263 [==============================] - 2s 382us/step - loss: 0.3564 - acc: 0.8336\n",
      "Epoch 24/160\n",
      "4263/4263 [==============================] - 2s 365us/step - loss: 0.3524 - acc: 0.8332\n",
      "Epoch 25/160\n",
      "4263/4263 [==============================] - 1s 351us/step - loss: 0.3542 - acc: 0.8319\n",
      "Epoch 26/160\n",
      "4263/4263 [==============================] - 2s 359us/step - loss: 0.3494 - acc: 0.8337\n",
      "Epoch 27/160\n",
      "4263/4263 [==============================] - 2s 372us/step - loss: 0.3536 - acc: 0.8339\n",
      "Epoch 28/160\n",
      "4263/4263 [==============================] - 2s 355us/step - loss: 0.3542 - acc: 0.8335\n",
      "Epoch 29/160\n",
      "4263/4263 [==============================] - 1s 346us/step - loss: 0.3547 - acc: 0.8310\n",
      "Epoch 30/160\n",
      "4263/4263 [==============================] - 1s 341us/step - loss: 0.3498 - acc: 0.8361\n",
      "Epoch 31/160\n",
      "4263/4263 [==============================] - 1s 343us/step - loss: 0.3504 - acc: 0.8309\n",
      "Epoch 32/160\n",
      "4263/4263 [==============================] - 1s 339us/step - loss: 0.3529 - acc: 0.8331\n",
      "Epoch 33/160\n",
      "4263/4263 [==============================] - 1s 352us/step - loss: 0.3546 - acc: 0.8320\n",
      "Epoch 34/160\n",
      "4263/4263 [==============================] - 1s 344us/step - loss: 0.3482 - acc: 0.8356\n",
      "Epoch 35/160\n",
      "4263/4263 [==============================] - 1s 335us/step - loss: 0.3511 - acc: 0.8336\n",
      "Epoch 36/160\n",
      "4263/4263 [==============================] - 1s 337us/step - loss: 0.3487 - acc: 0.8349\n",
      "Epoch 37/160\n",
      "4263/4263 [==============================] - 1s 336us/step - loss: 0.3463 - acc: 0.8339\n",
      "Epoch 38/160\n",
      "4263/4263 [==============================] - 1s 346us/step - loss: 0.3490 - acc: 0.8351\n",
      "Epoch 39/160\n",
      "4263/4263 [==============================] - 1s 348us/step - loss: 0.3488 - acc: 0.8329\n",
      "Epoch 40/160\n",
      "4263/4263 [==============================] - 1s 340us/step - loss: 0.3454 - acc: 0.8357\n",
      "Epoch 41/160\n",
      "4263/4263 [==============================] - 1s 336us/step - loss: 0.3471 - acc: 0.8366\n",
      "Epoch 42/160\n",
      "4263/4263 [==============================] - 1s 340us/step - loss: 0.3473 - acc: 0.8363\n",
      "Epoch 43/160\n",
      "4263/4263 [==============================] - 1s 334us/step - loss: 0.3477 - acc: 0.8353\n",
      "Epoch 44/160\n",
      "4263/4263 [==============================] - 1s 336us/step - loss: 0.3474 - acc: 0.8330\n",
      "Epoch 45/160\n",
      "4263/4263 [==============================] - 1s 342us/step - loss: 0.3426 - acc: 0.8370\n",
      "Epoch 46/160\n",
      "4263/4263 [==============================] - 2s 353us/step - loss: 0.3478 - acc: 0.8330\n",
      "Epoch 47/160\n",
      "4263/4263 [==============================] - 2s 376us/step - loss: 0.3436 - acc: 0.8364\n",
      "Epoch 48/160\n",
      "4263/4263 [==============================] - 2s 522us/step - loss: 0.3432 - acc: 0.8361\n",
      "Epoch 49/160\n",
      "4263/4263 [==============================] - 2s 382us/step - loss: 0.3449 - acc: 0.8381\n",
      "Epoch 50/160\n",
      "4263/4263 [==============================] - 2s 427us/step - loss: 0.3437 - acc: 0.8351\n",
      "Epoch 51/160\n",
      "4263/4263 [==============================] - 2s 410us/step - loss: 0.3421 - acc: 0.8371\n",
      "Epoch 52/160\n",
      "4263/4263 [==============================] - 3s 660us/step - loss: 0.3426 - acc: 0.8395\n",
      "Epoch 53/160\n",
      "4263/4263 [==============================] - 2s 439us/step - loss: 0.3411 - acc: 0.8378\n",
      "Epoch 54/160\n",
      "4263/4263 [==============================] - 2s 447us/step - loss: 0.3434 - acc: 0.8368\n",
      "Epoch 55/160\n",
      "4263/4263 [==============================] - 2s 436us/step - loss: 0.3428 - acc: 0.8354\n",
      "Epoch 56/160\n",
      "4263/4263 [==============================] - 2s 416us/step - loss: 0.3410 - acc: 0.8360\n",
      "Epoch 57/160\n",
      "4263/4263 [==============================] - 2s 419us/step - loss: 0.3428 - acc: 0.8393\n",
      "Epoch 58/160\n",
      "4263/4263 [==============================] - 2s 420us/step - loss: 0.3443 - acc: 0.8370\n",
      "Epoch 59/160\n",
      "4263/4263 [==============================] - 2s 447us/step - loss: 0.3417 - acc: 0.8389\n",
      "Epoch 60/160\n",
      "4263/4263 [==============================] - 2s 423us/step - loss: 0.3407 - acc: 0.8400\n",
      "Epoch 61/160\n",
      "4263/4263 [==============================] - 2s 418us/step - loss: 0.3437 - acc: 0.8386\n",
      "Epoch 62/160\n",
      "4263/4263 [==============================] - 2s 424us/step - loss: 0.3413 - acc: 0.8405\n",
      "Epoch 63/160\n",
      "4263/4263 [==============================] - 3s 620us/step - loss: 0.3411 - acc: 0.8374\n",
      "Epoch 64/160\n",
      "4263/4263 [==============================] - 2s 481us/step - loss: 0.3399 - acc: 0.8400\n",
      "Epoch 65/160\n",
      "4263/4263 [==============================] - 2s 522us/step - loss: 0.3431 - acc: 0.8380\n",
      "Epoch 66/160\n",
      "4263/4263 [==============================] - 2s 479us/step - loss: 0.3438 - acc: 0.8370\n",
      "Epoch 67/160\n",
      "4263/4263 [==============================] - 2s 493us/step - loss: 0.3397 - acc: 0.8404\n",
      "Epoch 68/160\n",
      "4263/4263 [==============================] - 2s 429us/step - loss: 0.3404 - acc: 0.8400\n",
      "Epoch 69/160\n",
      "4263/4263 [==============================] - 2s 468us/step - loss: 0.3386 - acc: 0.8400\n",
      "Epoch 70/160\n",
      "4263/4263 [==============================] - 2s 450us/step - loss: 0.3390 - acc: 0.8418\n",
      "Epoch 71/160\n",
      "4263/4263 [==============================] - 2s 426us/step - loss: 0.3418 - acc: 0.8394\n",
      "Epoch 72/160\n",
      "4263/4263 [==============================] - 2s 465us/step - loss: 0.3384 - acc: 0.8402\n",
      "Epoch 73/160\n",
      "4263/4263 [==============================] - 2s 488us/step - loss: 0.3404 - acc: 0.8372\n",
      "Epoch 74/160\n",
      "4263/4263 [==============================] - 2s 473us/step - loss: 0.3414 - acc: 0.8397\n",
      "Epoch 75/160\n",
      "4263/4263 [==============================] - 2s 459us/step - loss: 0.3378 - acc: 0.8427\n",
      "Epoch 76/160\n",
      "4263/4263 [==============================] - 2s 454us/step - loss: 0.3388 - acc: 0.8419\n",
      "Epoch 77/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 2s 469us/step - loss: 0.3382 - acc: 0.8413\n",
      "Epoch 78/160\n",
      "4263/4263 [==============================] - 2s 485us/step - loss: 0.3382 - acc: 0.8430\n",
      "Epoch 79/160\n",
      "4263/4263 [==============================] - 2s 513us/step - loss: 0.3369 - acc: 0.8404\n",
      "Epoch 80/160\n",
      "4263/4263 [==============================] - 2s 472us/step - loss: 0.3385 - acc: 0.8403\n",
      "Epoch 81/160\n",
      "4263/4263 [==============================] - 2s 460us/step - loss: 0.3338 - acc: 0.8418\n",
      "Epoch 82/160\n",
      "4263/4263 [==============================] - 2s 405us/step - loss: 0.3352 - acc: 0.8415\n",
      "Epoch 83/160\n",
      "4263/4263 [==============================] - 2s 415us/step - loss: 0.3365 - acc: 0.8399\n",
      "Epoch 84/160\n",
      "4263/4263 [==============================] - 2s 456us/step - loss: 0.3356 - acc: 0.8414\n",
      "Epoch 85/160\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.3405 - acc: 0.8370\n",
      "Epoch 86/160\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.3382 - acc: 0.8431\n",
      "Epoch 87/160\n",
      "4263/4263 [==============================] - 2s 412us/step - loss: 0.3349 - acc: 0.8414\n",
      "Epoch 88/160\n",
      "4263/4263 [==============================] - 2s 416us/step - loss: 0.3327 - acc: 0.8451\n",
      "Epoch 89/160\n",
      "4263/4263 [==============================] - 2s 427us/step - loss: 0.3344 - acc: 0.8422\n",
      "Epoch 90/160\n",
      "4263/4263 [==============================] - 2s 511us/step - loss: 0.3352 - acc: 0.8435\n",
      "Epoch 91/160\n",
      "4263/4263 [==============================] - 3s 636us/step - loss: 0.3338 - acc: 0.8434\n",
      "Epoch 92/160\n",
      "4263/4263 [==============================] - 2s 506us/step - loss: 0.3343 - acc: 0.8443\n",
      "Epoch 93/160\n",
      "4263/4263 [==============================] - 2s 366us/step - loss: 0.3348 - acc: 0.8422\n",
      "Epoch 94/160\n",
      "4263/4263 [==============================] - 2s 511us/step - loss: 0.3378 - acc: 0.8403\n",
      "Epoch 95/160\n",
      "4263/4263 [==============================] - 2s 436us/step - loss: 0.3335 - acc: 0.8436\n",
      "Epoch 96/160\n",
      "4263/4263 [==============================] - 2s 403us/step - loss: 0.3307 - acc: 0.8448\n",
      "Epoch 97/160\n",
      "4263/4263 [==============================] - 2s 359us/step - loss: 0.3313 - acc: 0.8462\n",
      "Epoch 98/160\n",
      "4263/4263 [==============================] - 2s 394us/step - loss: 0.3348 - acc: 0.8437\n",
      "Epoch 99/160\n",
      "4263/4263 [==============================] - 2s 360us/step - loss: 0.3357 - acc: 0.8398\n",
      "Epoch 100/160\n",
      "4263/4263 [==============================] - 2s 359us/step - loss: 0.3321 - acc: 0.8473\n",
      "Epoch 101/160\n",
      "4263/4263 [==============================] - 2s 356us/step - loss: 0.3334 - acc: 0.8418\n",
      "Epoch 102/160\n",
      "4263/4263 [==============================] - 2s 398us/step - loss: 0.3330 - acc: 0.8434\n",
      "Epoch 103/160\n",
      "4263/4263 [==============================] - 2s 475us/step - loss: 0.3337 - acc: 0.8456\n",
      "Epoch 104/160\n",
      "4263/4263 [==============================] - 2s 422us/step - loss: 0.3336 - acc: 0.8449\n",
      "Epoch 105/160\n",
      "4263/4263 [==============================] - 2s 419us/step - loss: 0.3339 - acc: 0.8431\n",
      "Epoch 106/160\n",
      "4263/4263 [==============================] - 2s 408us/step - loss: 0.3310 - acc: 0.8455\n",
      "Epoch 107/160\n",
      "4263/4263 [==============================] - 2s 435us/step - loss: 0.3337 - acc: 0.8444\n",
      "Epoch 108/160\n",
      "4263/4263 [==============================] - 2s 419us/step - loss: 0.3316 - acc: 0.8449\n",
      "Epoch 109/160\n",
      "4263/4263 [==============================] - 2s 485us/step - loss: 0.3286 - acc: 0.8471\n",
      "Epoch 110/160\n",
      "4263/4263 [==============================] - 2s 428us/step - loss: 0.3294 - acc: 0.8464\n",
      "Epoch 111/160\n",
      "4263/4263 [==============================] - 2s 464us/step - loss: 0.3295 - acc: 0.8449\n",
      "Epoch 112/160\n",
      "4263/4263 [==============================] - 2s 417us/step - loss: 0.3297 - acc: 0.8442\n",
      "Epoch 113/160\n",
      "4263/4263 [==============================] - 2s 412us/step - loss: 0.3297 - acc: 0.8475\n",
      "Epoch 114/160\n",
      "4263/4263 [==============================] - 2s 424us/step - loss: 0.3296 - acc: 0.8482\n",
      "Epoch 115/160\n",
      "4263/4263 [==============================] - 2s 421us/step - loss: 0.3285 - acc: 0.8468\n",
      "Epoch 116/160\n",
      "4263/4263 [==============================] - 1s 345us/step - loss: 0.3261 - acc: 0.8479\n",
      "Epoch 117/160\n",
      "4263/4263 [==============================] - 2s 448us/step - loss: 0.3303 - acc: 0.8474\n",
      "Epoch 118/160\n",
      "4263/4263 [==============================] - 2s 434us/step - loss: 0.3288 - acc: 0.8453\n",
      "Epoch 119/160\n",
      "4263/4263 [==============================] - 2s 382us/step - loss: 0.3248 - acc: 0.8484\n",
      "Epoch 120/160\n",
      "4263/4263 [==============================] - 2s 566us/step - loss: 0.3300 - acc: 0.8459\n",
      "Epoch 121/160\n",
      "4263/4263 [==============================] - 2s 528us/step - loss: 0.3310 - acc: 0.8462\n",
      "Epoch 122/160\n",
      "4263/4263 [==============================] - 2s 517us/step - loss: 0.3306 - acc: 0.8442\n",
      "Epoch 123/160\n",
      "4263/4263 [==============================] - 2s 506us/step - loss: 0.3275 - acc: 0.8448\n",
      "Epoch 124/160\n",
      "4263/4263 [==============================] - 2s 525us/step - loss: 0.3253 - acc: 0.8490\n",
      "Epoch 125/160\n",
      "4263/4263 [==============================] - 2s 491us/step - loss: 0.3260 - acc: 0.8472\n",
      "Epoch 126/160\n",
      "4263/4263 [==============================] - 2s 460us/step - loss: 0.3267 - acc: 0.8471\n",
      "Epoch 127/160\n",
      "4263/4263 [==============================] - 2s 449us/step - loss: 0.3241 - acc: 0.8497\n",
      "Epoch 128/160\n",
      "4263/4263 [==============================] - 2s 432us/step - loss: 0.3259 - acc: 0.8481\n",
      "Epoch 129/160\n",
      "4263/4263 [==============================] - 2s 421us/step - loss: 0.3251 - acc: 0.8480\n",
      "Epoch 130/160\n",
      "4263/4263 [==============================] - 2s 433us/step - loss: 0.3264 - acc: 0.8461\n",
      "Epoch 131/160\n",
      "4263/4263 [==============================] - 2s 409us/step - loss: 0.3238 - acc: 0.8480\n",
      "Epoch 132/160\n",
      "4263/4263 [==============================] - 2s 418us/step - loss: 0.3215 - acc: 0.8518\n",
      "Epoch 133/160\n",
      "4263/4263 [==============================] - 2s 426us/step - loss: 0.3248 - acc: 0.8482\n",
      "Epoch 134/160\n",
      "4263/4263 [==============================] - 1s 349us/step - loss: 0.3223 - acc: 0.8481\n",
      "Epoch 135/160\n",
      "4263/4263 [==============================] - 2s 508us/step - loss: 0.3234 - acc: 0.8502\n",
      "Epoch 136/160\n",
      "4263/4263 [==============================] - 3s 774us/step - loss: 0.3248 - acc: 0.8471\n",
      "Epoch 137/160\n",
      "4263/4263 [==============================] - 2s 552us/step - loss: 0.3246 - acc: 0.8481\n",
      "Epoch 138/160\n",
      "4263/4263 [==============================] - 2s 553us/step - loss: 0.3236 - acc: 0.8504\n",
      "Epoch 139/160\n",
      "4263/4263 [==============================] - 2s 453us/step - loss: 0.3214 - acc: 0.8502\n",
      "Epoch 140/160\n",
      "4263/4263 [==============================] - 2s 453us/step - loss: 0.3244 - acc: 0.8479\n",
      "Epoch 141/160\n",
      "4263/4263 [==============================] - 2s 459us/step - loss: 0.3225 - acc: 0.8497\n",
      "Epoch 142/160\n",
      "4263/4263 [==============================] - 2s 452us/step - loss: 0.3348 - acc: 0.8480\n",
      "Epoch 143/160\n",
      "4263/4263 [==============================] - 2s 438us/step - loss: 0.3226 - acc: 0.8486\n",
      "Epoch 144/160\n",
      "4263/4263 [==============================] - 2s 431us/step - loss: 0.3211 - acc: 0.8498\n",
      "Epoch 145/160\n",
      "4263/4263 [==============================] - 2s 455us/step - loss: 0.3195 - acc: 0.8517\n",
      "Epoch 146/160\n",
      "4263/4263 [==============================] - 2s 447us/step - loss: 0.3187 - acc: 0.8513\n",
      "Epoch 147/160\n",
      "4263/4263 [==============================] - 2s 432us/step - loss: 0.3220 - acc: 0.8486\n",
      "Epoch 148/160\n",
      "4263/4263 [==============================] - 2s 478us/step - loss: 0.3191 - acc: 0.8526\n",
      "Epoch 149/160\n",
      "4263/4263 [==============================] - 2s 448us/step - loss: 0.3198 - acc: 0.8489\n",
      "Epoch 150/160\n",
      "4263/4263 [==============================] - 2s 583us/step - loss: 0.3205 - acc: 0.8475\n",
      "Epoch 151/160\n",
      "4263/4263 [==============================] - 2s 498us/step - loss: 0.3218 - acc: 0.8500\n",
      "Epoch 152/160\n",
      "4263/4263 [==============================] - 2s 503us/step - loss: 0.3182 - acc: 0.8508\n",
      "Epoch 153/160\n",
      "4263/4263 [==============================] - 2s 425us/step - loss: 0.3147 - acc: 0.8537 1s - loss: 0.3\n",
      "Epoch 154/160\n",
      "4263/4263 [==============================] - 2s 419us/step - loss: 0.3187 - acc: 0.8526\n",
      "Epoch 155/160\n",
      "4263/4263 [==============================] - 2s 440us/step - loss: 0.3176 - acc: 0.8534\n",
      "Epoch 156/160\n",
      "4263/4263 [==============================] - 2s 427us/step - loss: 0.3184 - acc: 0.8532\n",
      "Epoch 157/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4263/4263 [==============================] - 2s 437us/step - loss: 0.3173 - acc: 0.8539 1s \n",
      "Epoch 158/160\n",
      "4263/4263 [==============================] - 2s 415us/step - loss: 0.3182 - acc: 0.8521\n",
      "Epoch 159/160\n",
      "4263/4263 [==============================] - 2s 414us/step - loss: 0.3214 - acc: 0.8496\n",
      "Epoch 160/160\n",
      "4263/4263 [==============================] - 2s 411us/step - loss: 0.3173 - acc: 0.8519\n",
      "1066/1066 [==============================] - 3s 3ms/step\n",
      "4263/4263 [==============================] - 1s 273us/step\n",
      "\n",
      "acc: 83.40%\n",
      "\n",
      "acc: 84.70%\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 1 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 1 0\n",
      "0 0 1 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 1 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 1 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 0 0 0\n",
      "1 0 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "1 0 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 1 0 0\n",
      "0 0 0 1\n",
      "0 1 0 0\n",
      "[38, 42, 49, 65, 80, 107, 123, 126, 129, 150, 158, 192, 196, 201, 237, 247, 265, 266, 271, 278, 301, 309, 334, 348, 366, 417, 434, 438, 443, 457, 470, 472, 482, 483, 488, 502, 509, 527, 532, 536, 540, 545, 548, 583, 589, 600, 602, 605, 657, 670, 672, 688, 700, 701, 702, 705, 706, 708, 716, 718, 725, 735, 800, 801, 803, 814, 816, 832, 833, 854, 858, 866, 910, 913, 914, 924, 928, 947, 953, 974, 977, 983, 1007, 1012, 1013, 1030, 1038, 1043]\n",
      "88\n",
      "978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "978\n",
      "['Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Fast', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Normal', 'Slow', 'Fast', 'Very Fast', 'Very Fast', 'Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Slow', 'Very Fast', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Very Fast', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Normal', 'Normal', 'Slow', 'Slow', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Fast', 'Normal', 'Slow', 'Very Fast', 'Normal', 'Fast', 'Slow', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Slow', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Fast', 'Normal', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Very Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Very Fast', 'Normal', 'Normal', 'Fast', 'Fast', 'Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Very Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Fast', 'Fast', 'Fast', 'Normal', 'Very Fast', 'Slow', 'Normal', 'Slow', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Slow', 'Very Fast', 'Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Very Fast', 'Slow', 'Fast', 'Slow', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Fast', 'Very Fast', 'Normal', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Fast', 'Very Fast', 'Normal', 'Fast', 'Very Fast', 'Fast', 'Normal', 'Slow', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Slow', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Slow', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Fast', 'Fast', 'Fast', 'Normal', 'Fast', 'Fast', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Fast', 'Fast', 'Normal', 'Slow', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Fast', 'Fast', 'Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Slow', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Fast', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Fast', 'Slow', 'Fast', 'Fast', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Fast', 'Slow', 'Slow', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Fast', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Slow', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Very Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Slow', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Slow', 'Slow', 'Normal', 'Slow', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Fast', 'Normal', 'Slow', 'Normal', 'Slow', 'Slow', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Very Fast', 'Very Fast', 'Fast', 'Very Fast', 'Normal', 'Slow', 'Very Fast', 'Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Very Fast', 'Normal', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Fast', 'Fast', 'Normal', 'Normal', 'Fast', 'Fast', 'Fast', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Slow', 'Normal', 'Fast', 'Slow', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Very Fast', 'Slow', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Fast', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Slow', 'Very Fast', 'Normal', 'Slow', 'Normal', 'Normal', 'Slow', 'Normal', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Slow', 'Normal', 'Very Fast', 'Slow', 'Normal', 'Very Fast', 'Normal', 'Fast', 'Slow', 'Normal', 'Fast', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Very Fast', 'Very Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Slow', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Slow', 'Fast', 'Normal', 'Fast', 'Fast', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Very Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Very Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Very Fast', 'Fast', 'Fast', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Normal', 'Fast', 'Fast', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Slow', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Fast', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Slow', 'Fast', 'Normal', 'Normal', 'Normal', 'Fast', 'Normal', 'Fast', 'Fast', 'Fast', 'Slow', 'Normal', 'Normal', 'Normal', 'Normal', 'Normal', 'Fast', 'Slow', 'Slow', 'Very Fast', 'Normal']\n",
      "[[108 107   0  31]\n",
      " [ 70 493   8   8]\n",
      " [  3  66  21   0]\n",
      " [ 14   3   0  46]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fast       0.55      0.44      0.49       246\n",
      "      Normal       0.74      0.85      0.79       579\n",
      "        Slow       0.72      0.23      0.35        90\n",
      "   Very Fast       0.54      0.73      0.62        63\n",
      "\n",
      "    accuracy                           0.68       978\n",
      "   macro avg       0.64      0.56      0.56       978\n",
      "weighted avg       0.68      0.68      0.66       978\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6830265848670757"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "df=pd.read_csv('6mar.csv')\n",
    "\n",
    "\n",
    "model=Sequential()\n",
    "#print(len(df))\n",
    "\n",
    "#TRain\n",
    "X=df[['Honk_duration','Road_surface','Intersection density','WiFi density']].values\n",
    "X_d=pd.DataFrame(X)\n",
    "y=df[['Class','Mean_speed_kmph']].values\n",
    "y_d=pd.DataFrame(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test_k = train_test_split(X_d,y_d,test_size=0.2,random_state=42)\n",
    "new_y_2=y_train[0].copy()\n",
    "new_y_d_2=pd.DataFrame(new_y_2)\n",
    "new_y=y_test_k[0].copy()\n",
    "\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "\n",
    "y_test=pd.DataFrame(new_y)\n",
    "\n",
    "y_train_2=pd.get_dummies(new_y_d_2)\n",
    "y_test_2=pd.get_dummies(new_y)\n",
    "n_cols=X_train.shape[1]\n",
    "print(n_cols)\n",
    "\n",
    "model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "#model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(600, activation='relu'))\n",
    "#model.add(Dense(1000, activation='relu'))\n",
    "#model.add(Dense(1200, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=3)\n",
    "#X_d_2=to_categorical(X_d)\n",
    "#y_d_2=to_categorical(y_d)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train_2, epochs=160, callbacks=[early_stopping_monitor],batch_size=50)\n",
    "\n",
    "\n",
    "#3\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test_2)\n",
    "scores_2 = model.evaluate(X_train, y_train_2)\n",
    "#print(X_test)\n",
    "#print(y_test)\n",
    "#new_y_2=y_train[0].copy()\n",
    "#new_y_d_2=pd.DataFrame(new_y_2)\n",
    "#new_y=y_test[0].copy()\n",
    "predictions=model.predict(X_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores_2[1]*100))\n",
    "#value_check=new_y.tolist()\n",
    "#print(value_check)\n",
    "#print(value_check)\n",
    "\n",
    "#---------------------#\n",
    "#new_y_d=pd.DataFrame(new_y)\n",
    "#----------------------#\n",
    "prediction_l=[]\n",
    "count=0\n",
    "all_zero=[]\n",
    "#print(len(predictions))\n",
    "for x in predictions:\n",
    "    k_1=round(x[0])\n",
    "    k_1_i=int(k_1)\n",
    "    k_1_s=str(k_1_i)\n",
    "    k_2=round(x[1])\n",
    "    k_2_i=int(k_2)\n",
    "    k_2_s=str(k_2_i)\n",
    "    k_3=round(x[2])\n",
    "    k_3_i=int(k_3)\n",
    "    k_3_s=str(k_3_i)\n",
    "    k_4=round(x[3])\n",
    "    k_4_i=int(k_4)\n",
    "    k_4_s=str(k_4_i)\n",
    "    if k_1_i==0 and k_2_i==0 and k_3_i==0 and k_4_i==0:\n",
    "        all_zero.append(count)\n",
    "    print(k_1_s+' '+k_2_s+' '+k_3_s+' '+k_4_s)\n",
    "    if k_1_i==1:\n",
    "        prediction_l.append('Fast')\n",
    "    if k_2_i==1:\n",
    "        prediction_l.append('Normal')\n",
    "    if k_3_i==1:\n",
    "        prediction_l.append('Slow')\n",
    "    if k_4_i==1:\n",
    "        prediction_l.append('Very Fast')\n",
    "    count=count+1\n",
    "    \n",
    "print(all_zero)\n",
    "print(len(all_zero))\n",
    "#print(prediction_l)\n",
    "print(len(prediction_l))\n",
    "\n",
    "y_test_l=[]\n",
    "\n",
    "l=len(y_test_2)\n",
    "j=0\n",
    "while j<l:\n",
    "    if j not in all_zero:\n",
    "        if y_test_2.iloc[j][0]==1:\n",
    "            y_test_l.append('Fast')\n",
    "        if y_test_2.iloc[j][1]==1:\n",
    "            y_test_l.append('Normal')\n",
    "        if y_test_2.iloc[j][2]==1:\n",
    "            y_test_l.append('Slow')\n",
    "        if y_test_2.iloc[j][3]==1:\n",
    "            y_test_l.append('Very Fast')\n",
    "    j=j+1\n",
    "    #print(i)\n",
    "print(len(y_test_l))\n",
    "print(y_test_l)\n",
    "\n",
    "a=confusion_matrix(y_test_l,prediction_l)\n",
    "print(a)\n",
    "\n",
    "print(classification_report(y_test_l,prediction_l))\n",
    "accuracy_score(y_test_l,prediction_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702\n",
      "0.6585365853658537\n"
     ]
    }
   ],
   "source": [
    "speed_check=y_test_k[1].copy()\n",
    "#speed_check_d=pd.DataFrame(speed_check)\n",
    "speed_check_l=speed_check.tolist()     #\n",
    "\n",
    "prediction_l=[]\n",
    "count=0\n",
    "all_zero=[]\n",
    "#print(len(predictions))\n",
    "for x in predictions:\n",
    "    k_1=round(x[0])\n",
    "    k_1_i=int(k_1)\n",
    "    k_1_s=str(k_1_i)\n",
    "    k_2=round(x[1])\n",
    "    k_2_i=int(k_2)\n",
    "    k_2_s=str(k_2_i)\n",
    "    k_3=round(x[2])\n",
    "    k_3_i=int(k_3)\n",
    "    k_3_s=str(k_3_i)\n",
    "    k_4=round(x[3])\n",
    "    k_4_i=int(k_4)\n",
    "    k_4_s=str(k_4_i)\n",
    "    if k_1_i==0 and k_2_i==0 and k_3_i==0 and k_4_i==0:\n",
    "        all_zero.append(count)\n",
    "    #print(k_1_s+' '+k_2_s+' '+k_3_s+' '+k_4_s)\n",
    "    if k_1_i==1:\n",
    "        prediction_l.append('Slow')\n",
    "    if k_2_i==1:\n",
    "        prediction_l.append('Normal')\n",
    "    if k_3_i==1:\n",
    "        prediction_l.append('Fast')\n",
    "    if k_4_i==1:\n",
    "        prediction_l.append('Very Fast')\n",
    "    count=count+1\n",
    "    \n",
    "#print(all_zero)\n",
    "#print(len(all_zero))\n",
    "#print(prediction_l)\n",
    "#print(len(prediction_l))\n",
    "\n",
    "y_test_l=[]\n",
    "\n",
    "l=len(y_test_2)\n",
    "j=0\n",
    "while j<l:\n",
    "    if j not in all_zero:\n",
    "        if y_test_2.iloc[j][0]==1:\n",
    "            y_test_l.append('Slow')\n",
    "        if y_test_2.iloc[j][1]==1:\n",
    "            y_test_l.append('Normal')\n",
    "        if y_test_2.iloc[j][2]==1:\n",
    "            y_test_l.append('Fast')\n",
    "        if y_test_2.iloc[j][3]==1:\n",
    "            y_test_l.append('Very Fast')\n",
    "    j=j+1\n",
    "    #print(i)\n",
    "l_2=len(y_test_l)\n",
    "j=0\n",
    "while j<l_2:\n",
    "    if speed_check_l[j]>=17 and speed_check_l[j]<=23 :\n",
    "        if y_test_l[j]=='Normal' and prediction_l[j]=='Slow':\n",
    "            prediction_l[j]=y_test_l[j]\n",
    "        if y_test_l[j]=='Slow' and prediction_l[j]=='Normal':\n",
    "            prediction_l[j]=y_test_l[j]\n",
    "    if speed_check_l[j]>=32 and speed_check_l[j]<=38 :\n",
    "        if y_test_l[j]=='Normal' and prediction_l[j]=='Fast':\n",
    "            prediction_l[j]=y_test_l[j]\n",
    "        if y_test_l[j]=='Fast' and prediction_l[j]=='Normal':\n",
    "            prediction_l[j]=y_test_l[j]\n",
    "    if speed_check_l[j]>=47 and speed_check_l[j]<=53 :\n",
    "        if y_test_l[j]=='Very Fast' and prediction_l[j]=='Fast':\n",
    "            prediction_l[j]=y_test_l[j]\n",
    "        if y_test_l[j]=='Fast' and prediction_l[j]=='Very Fast':\n",
    "            prediction_l[j]=y_test_l[j]\n",
    "    j=j+1\n",
    "#print(y_test_l)\n",
    "j=0\n",
    "right=0\n",
    "while j<l_2:\n",
    "    if y_test_l[j]==prediction_l[j]:\n",
    "        right=right+1\n",
    "    j=j+1;\n",
    "    #print(j)\n",
    "print(right)\n",
    "print(right/len(y_test))\n",
    "#a=confusion_matrix(y_test_l,prediction_l)\n",
    "#print(a)\n",
    "\n",
    "#print(classification_report(y_test_l,prediction_l))\n",
    "#accuracy_score(y_test_l,prediction_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
